{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5259aa5",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#uberduck_ml_dev.exec.force_spectrogram\" data-toc-modified-id=\"uberduck_ml_dev.exec.force_spectrogram-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>uberduck_ml_dev.exec.force_spectrogram</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c70302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp exec.force_spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed1ab55",
   "metadata": {},
   "source": [
    "# uberduck_ml_dev.exec.force_spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2093cc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import argparse\n",
    "from collections import namedtuple\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from shutil import copyfile, copytree\n",
    "import sys\n",
    "from typing import List, Optional, Set\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "from nemo.collections.tts.models import TalkNetSpectModel\n",
    "from nemo.collections.asr.data.audio_to_text import AudioToCharWithDursF0Dataset\n",
    "\n",
    "\n",
    "def parse_args(args):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"-m\", \"--model\", default=\"Path to model state dict\", required=True\n",
    "    )\n",
    "    parser.add_argument(\"-f\", \"--filelist\", default=\"Path to filelist\", required=True)\n",
    "    parser.add_argument(\"-t\", \"--model-type\", help=\"model type\", default=\"talknet\")\n",
    "    parser.add_argument(\"--durations\")\n",
    "    parser.add_argument(\"--f0s\")\n",
    "    parser.add_argument(\"--rel-path\")\n",
    "    parser.add_argument(\"--cuda\", default=torch.cuda.is_available())\n",
    "    return parser.parse_args(args)\n",
    "\n",
    "\n",
    "# expected_tokens = torch.tensor([[34, 18,  0,  6, 36, 23,  0, 41,  0, 15, 31,  8, 35,  0, 41,  0,  8, 33,\n",
    "#           0, 23, 35,  0, 41,  0, 18, 39, 14, 28,  9,  0, 27, 12,  3,  0,  1, 34,\n",
    "#           6, 35,  0, 42,  0, 25, 12,  3, 15, 33,  0,  5, 15, 27, 11,  0, 29, 18,\n",
    "#           9, 26, 16, 18,  0, 41,  0,  8, 25,  3, 27,  0, 41,  0,  9, 22, 38, 15,\n",
    "#          14, 27, 18,  0, 41,  0, 12, 26, 16,  0, 27, 12,  3,  0,  4, 31, 12,  0,\n",
    "#          11, 35,  0, 42]])\n",
    "# expected_durs = torch.tensor([[10,  1,  0,  1, 11,  1,  1,  1,  0,  1,  0,  2, 19,  2,  0,  1,  0,  1,\n",
    "#           0,  1,  0,  1,  0,  1, 15,  1, 38,  1,  0,  1,  0,  2,  1,  1,  0,  1,\n",
    "#           0,  1,  0,  1, 14,  1, 14,  1,  0,  1,  0,  2,  0,  2,  0,  1, 20,  1,\n",
    "#           0,  1,  0,  2, 12,  3,  1,  1,  0,  1,  0,  1, 11,  3,  0,  1,  0,  1,\n",
    "#           0,  1, 10,  1, 10,  2,  0,  1,  0,  2,  1,  1,  0,  1, 10,  1,  0,  1,\n",
    "#           0,  1,  6,  3,  0,  1,  0,  1,  0,  1,  0,  1,  8,  2,  1,  1,  0,  1,\n",
    "#          20,  1,  0,  1,  0,  1,  0,  1, 19,  1,  0,  1,  0,  1,  0,  1,  0,  1,\n",
    "#           0,  1,  9,  1,  9,  1,  0,  1,  0,  1,  0,  1,  0,  1,  0,  1,  0,  1,\n",
    "#          10,  1,  0,  1,  0,  1, 23,  1,  0,  1,  0,  1,  0,  1,  0,  1,  0,  1,\n",
    "#          19,  3,  1,  1,  0,  1,  0,  1,  8,  3,  0,  1,  0,  1,  0,  1, 15,  2,\n",
    "#           0,  1,  0,  1, 17,  1,  0,  1,  0]])\n",
    "\n",
    "\n",
    "def run(args):\n",
    "    if args.model_type != \"talknet\":\n",
    "        raise Exception(\"Supported model types: talknet\")\n",
    "    model = TalkNetSpectModel.restore_from(args.model, map_location=\"cpu\")\n",
    "    model.eval()\n",
    "    durs = torch.load(args.durations)\n",
    "    f0s = torch.load(args.f0s)\n",
    "    rel_path = args.rel_path\n",
    "    with open(args.filelist, encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    for line in tqdm(lines):\n",
    "        if \"{\" in line or \"}\" in line:\n",
    "            print(\"arpabet is not supported, skipping\")\n",
    "            print(line)\n",
    "\n",
    "        path = line.split(\"|\")[0].strip()\n",
    "        if rel_path:\n",
    "            path = os.path.join(rel_path, path)\n",
    "        line_name, _ = os.path.splitext(os.path.basename(path))\n",
    "        text = line.split(\"|\")[1].strip()\n",
    "        line_tokens = model.parse(text=line.split(\"|\")[1].strip())\n",
    "        line_durs = (\n",
    "            torch.stack(\n",
    "                (\n",
    "                    durs[line_name][\"blanks\"],\n",
    "                    torch.cat((durs[line_name][\"tokens\"], torch.zeros(1).int())),\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "            .view(-1)[:-1]\n",
    "            .view(1, -1)\n",
    "        )\n",
    "        x_f0s = f0s[line_name].view(1, -1)\n",
    "        if args.cuda:\n",
    "            line_durs = line_durs.cuda()\n",
    "            x_f0s = x_f0s.cuda()\n",
    "\n",
    "        if model.blanking:\n",
    "            debug_tokens = [\n",
    "                AudioToCharWithDursF0Dataset.interleave(\n",
    "                    x=torch.empty(len(t) + 1, dtype=torch.long, device=t.device).fill_(\n",
    "                        model.vocab.blank\n",
    "                    ),\n",
    "                    y=t,\n",
    "                )\n",
    "                for t in line_tokens\n",
    "            ]\n",
    "            debug_tokens = AudioToCharWithDursF0Dataset.merge(\n",
    "                debug_tokens, value=model.vocab.pad, dtype=torch.long\n",
    "            )\n",
    "\n",
    "        text_len = torch.tensor(debug_tokens.shape[-1], dtype=torch.long).unsqueeze(0)\n",
    "        durs_len = torch.tensor(line_durs.shape[-1], dtype=torch.long).unsqueeze(0)\n",
    "        print(text_len, durs_len)\n",
    "        if text_len != durs_len:\n",
    "            print([model.vocab._id2label[x] for x in debug_tokens[0]])\n",
    "            import pdb\n",
    "\n",
    "            pdb.set_trace()\n",
    "        spect = model.force_spectrogram(tokens=line_tokens, durs=line_durs, f0=x_f0s)\n",
    "        out_path = path.replace(\".wav\", \".npy\")\n",
    "        np.save(out_path, spect.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "try:\n",
    "    from nbdev.imports import IN_NOTEBOOK\n",
    "except:\n",
    "    IN_NOTEBOOK = False\n",
    "\n",
    "if __name__ == \"__main__\" and not IN_NOTEBOOK:\n",
    "    args = parse_args(sys.argv[1:])\n",
    "    run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfd57a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-01-10 08:03:05 modelPT:138] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.asr.data.audio_to_text.AudioToCharWithDursF0Dataset\n",
      "      manifest_filepath: trainfiles.json\n",
      "      max_duration: null\n",
      "      min_duration: 0.1\n",
      "      int_values: false\n",
      "      load_audio: true\n",
      "      normalize: false\n",
      "      sample_rate: 22050\n",
      "      trim: false\n",
      "      durs_file: /content/drive/My Drive/talknet/zwf/durations.pt\n",
      "      f0_file: /content/drive/My Drive/talknet/zwf/f0s.pt\n",
      "      blanking: true\n",
      "      vocab:\n",
      "        notation: phonemes\n",
      "        punct: true\n",
      "        spaces: true\n",
      "        stresses: false\n",
      "        add_blank_at: last\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: true\n",
      "      batch_size: 32\n",
      "      num_workers: 4\n",
      "    \n",
      "[NeMo W 2022-01-10 08:03:05 modelPT:145] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.asr.data.audio_to_text.AudioToCharWithDursF0Dataset\n",
      "      manifest_filepath: valfiles.json\n",
      "      max_duration: null\n",
      "      min_duration: 0.1\n",
      "      int_values: false\n",
      "      load_audio: true\n",
      "      normalize: false\n",
      "      sample_rate: 22050\n",
      "      trim: false\n",
      "      durs_file: /content/drive/My Drive/talknet/zwf/durations.pt\n",
      "      f0_file: /content/drive/My Drive/talknet/zwf/f0s.pt\n",
      "      blanking: true\n",
      "      vocab:\n",
      "        notation: phonemes\n",
      "        punct: true\n",
      "        spaces: true\n",
      "        stresses: false\n",
      "        add_blank_at: last\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: false\n",
      "      batch_size: 32\n",
      "      num_workers: 1\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-01-10 08:03:05 features:252] PADDING: 1\n",
      "[NeMo I 2022-01-10 08:03:05 features:269] STFT using torch\n",
      "[NeMo I 2022-01-10 08:03:05 modelPT:439] Model TalkNetSpectModel was successfully restored from ../talknet/zwf/TalkNetSpect.nemo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▏                                                                                                                       | 4/392 [00:00<00:08, 45.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKENS:  tensor([[34, 18,  0,  6, 36, 23,  0, 41,  0, 15, 31,  8, 35,  0, 41,  0,  8, 33,\n",
      "          0, 23, 35,  0, 41,  0, 18, 39, 14, 28,  9,  0, 27, 12,  3,  0,  1, 34,\n",
      "          6, 35,  0, 42,  0, 25, 12,  3, 15, 33,  0,  5, 15, 27, 11,  0, 29, 18,\n",
      "          9, 26, 16, 18,  0, 41,  0,  8, 33,  3, 27,  0, 41,  0,  9, 27, 15, 27,\n",
      "         14, 18,  0, 41,  0, 12, 25, 23,  0, 27, 12,  3,  0,  4, 31, 12,  0, 11,\n",
      "         35,  0, 42]])\n",
      "DURS:  tensor([[10,  1,  0,  1, 11,  1,  1,  1,  0,  1,  0,  2, 19,  2,  0,  1,  0,  1,\n",
      "          0,  1,  0,  1,  0,  1, 15,  1, 38,  1,  0,  1,  0,  2,  1,  1,  0,  1,\n",
      "          0,  1,  0,  1, 14,  1, 14,  1,  0,  1,  0,  2,  0,  2,  0,  1, 20,  1,\n",
      "          0,  1,  0,  2, 12,  3,  1,  1,  0,  1,  0,  1, 11,  3,  0,  1,  0,  1,\n",
      "          0,  1, 10,  1, 10,  2,  0,  1,  0,  2,  1,  1,  0,  1, 10,  1,  0,  1,\n",
      "          0,  1,  6,  3,  0,  1,  0,  1,  0,  1,  0,  1,  8,  2,  1,  1,  0,  1,\n",
      "         20,  1,  0,  1,  0,  1,  0,  1, 19,  1,  0,  1,  0,  1,  0,  1,  0,  1,\n",
      "          0,  1,  9,  1,  9,  1,  0,  1,  0,  1,  0,  1,  0,  1,  0,  1,  0,  1,\n",
      "         10,  1,  0,  1,  0,  1, 23,  1,  0,  1,  0,  1,  0,  1,  0,  1,  0,  1,\n",
      "         19,  3,  1,  1,  0,  1,  0,  1,  8,  3,  0,  1,  0,  1,  0,  1, 15,  2,\n",
      "          0,  1,  0,  1, 17,  1,  0,  1,  0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (93) must match the size of tensor b (94) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9n/w8qrq4mx4cl_j036z8wg_9nh0000gp/T/ipykernel_14518/3228172413.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# ]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m )\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/9n/w8qrq4mx4cl_j036z8wg_9nh0000gp/T/ipykernel_14518/2337405901.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DURS: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_durs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mline_durs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mexpected_durs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mline_tokens\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mexpected_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mx_f0s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf0s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mline_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (93) must match the size of tensor b (94) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "args = parse_args(\n",
    "    [\n",
    "        \"-m\",\n",
    "        \"../talknet/zwf/TalkNetSpect.nemo\",\n",
    "        \"-f\",\n",
    "        # \"/Users/zwf/data/voice/dvc-managed/zwf/list.txt\",\n",
    "        \"/Users/zwf/Downloads/allfiles.txt\",\n",
    "        \"--durations\",\n",
    "        \"../talknet/zwf/durations2.pt\",\n",
    "        \"--f0s\",\n",
    "        \"../talknet/zwf/f0s2.pt\",\n",
    "        \"--rel-path\",\n",
    "        \"/Users/zwf/data/voice/dvc-managed/zwf\",\n",
    "    ]\n",
    "    # [\n",
    "    #     \"-m\",\n",
    "    #     \"../talknet/sam-lachow/TalkNetSpect.nemo\",\n",
    "    #     \"-f\",\n",
    "    #     \"/Users/zwf/data/voice/dvc-managed/uberduck-multispeaker/sam-lachow/list.txt\",\n",
    "    #     \"--durations\",\n",
    "    #     \"../talknet/sam-lachow/durations.pt\",\n",
    "    #     \"--f0s\",\n",
    "    #     \"../talknet/sam-lachow/f0s.pt\",\n",
    "    #     \"--rel-path\",\n",
    "    #     \"/Users/zwf/data/voice/dvc-managed/uberduck-multispeaker/sam-lachow\",\n",
    "    # ]\n",
    ")\n",
    "run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1831b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "with tarfile.open(\n",
    "    os.path.join(\"../talknet/sam-lachow/\", \"TalkNetSpect.nemo\"), \"r:gz\"\n",
    ") as tf:\n",
    "    tf.extractall(\"/tmp\")\n",
    "model_weights_path = \"/tmp/model_weights.ckpt\"\n",
    "state_dict = torch.load(\"/tmp/model_weights.ckpt\", map_location=\"cpu\")\n",
    "state_dict = {\n",
    "    k.replace(\"model.\", \"\"): v for k, v in state_dict.items() if k.startswith(\"model.\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b9657a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.force_spectrogram??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd00213",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-01-09 22:54:24 modelPT:138] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.asr.data.audio_to_text.AudioToCharWithDursF0Dataset\n",
      "      manifest_filepath: trainfiles.json\n",
      "      max_duration: null\n",
      "      min_duration: 0.1\n",
      "      int_values: false\n",
      "      load_audio: true\n",
      "      normalize: false\n",
      "      sample_rate: 22050\n",
      "      trim: false\n",
      "      durs_file: /content/drive/My Drive/talknet/sam-lachow/durations.pt\n",
      "      f0_file: /content/drive/My Drive/talknet/sam-lachow/f0s.pt\n",
      "      blanking: true\n",
      "      vocab:\n",
      "        notation: phonemes\n",
      "        punct: true\n",
      "        spaces: true\n",
      "        stresses: false\n",
      "        add_blank_at: last\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: true\n",
      "      batch_size: 32\n",
      "      num_workers: 4\n",
      "    \n",
      "[NeMo W 2022-01-09 22:54:24 modelPT:145] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.asr.data.audio_to_text.AudioToCharWithDursF0Dataset\n",
      "      manifest_filepath: valfiles.json\n",
      "      max_duration: null\n",
      "      min_duration: 0.1\n",
      "      int_values: false\n",
      "      load_audio: true\n",
      "      normalize: false\n",
      "      sample_rate: 22050\n",
      "      trim: false\n",
      "      durs_file: /content/drive/My Drive/talknet/sam-lachow/durations.pt\n",
      "      f0_file: /content/drive/My Drive/talknet/sam-lachow/f0s.pt\n",
      "      blanking: true\n",
      "      vocab:\n",
      "        notation: phonemes\n",
      "        punct: true\n",
      "        spaces: true\n",
      "        stresses: false\n",
      "        add_blank_at: last\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: false\n",
      "      batch_size: 32\n",
      "      num_workers: 1\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-01-09 22:54:24 features:252] PADDING: 1\n",
      "[NeMo I 2022-01-09 22:54:24 features:269] STFT using torch\n",
      "[NeMo I 2022-01-09 22:54:24 modelPT:439] Model TalkNetSpectModel was successfully restored from ../talknet/sam-lachow/TalkNetSpect.nemo.\n"
     ]
    }
   ],
   "source": [
    "from nemo.collections.tts.models import TalkNetSpectModel\n",
    "\n",
    "model = TalkNetSpectModel.restore_from(\n",
    "    \"../talknet/sam-lachow/TalkNetSpect.nemo\", map_location=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d807fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TalkNetSpectModel.force_spectrogram??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dec5be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'blanks': tensor([10,  0,  0, 16,  0,  0, 16,  0,  0,  0,  0,  0,  5,  1,  0,  7,  0,  0,\n",
      "         0,  7,  0,  0, 11,  0,  0,  0, 12,  0,  0,  0, 15,  0,  0, 15,  0,  0,\n",
      "         1,  0,  0,  0,  8,  0,  0, 11,  0,  0,  0, 17,  0,  0,  0,  0,  0, 28,\n",
      "         0,  0,  2,  0, 10,  0,  0, 17,  0,  0,  0,  0,  0, 11,  1,  0, 13,  0,\n",
      "         0,  0, 12,  0,  0,  1,  9,  0,  0, 19,  0,  0,  0, 14,  0,  0,  0,  0,\n",
      "        20,  0,  0,  0,  0, 12,  0,  0,  0,  0, 18,  0,  0,  0,  0, 13,  0,  0,\n",
      "         0, 11,  0,  0, 19,  0,  0]), 'tokens': tensor([1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 3, 1, 1, 1, 1, 1, 1, 3, 1,\n",
      "        1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 2, 1, 1, 1, 2, 3, 1, 1, 2, 1, 1, 1, 2,\n",
      "        2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1,\n",
      "        1, 1, 1, 1, 1, 1, 3, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2,\n",
      "        1, 2, 1, 1, 3, 1, 1, 1, 1, 3, 2, 1, 1, 2, 1, 1, 1, 1])}\n",
      "torch.Size([115])\n",
      "torch.Size([114])\n"
     ]
    }
   ],
   "source": [
    "durs = torch.load(\"../talknet/zwf/durations.pt\")[\"136\"]\n",
    "print(durs)\n",
    "print(durs[\"blanks\"].shape)\n",
    "print(durs[\"tokens\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7e828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load(\"../talknet/zwf/f0s.pt\")[\"136\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca14632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.blanking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf261311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " 'B',\n",
       " 'CH',\n",
       " 'D',\n",
       " 'DH',\n",
       " 'F',\n",
       " 'G',\n",
       " 'HH',\n",
       " 'JH',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'NG',\n",
       " 'P',\n",
       " 'R',\n",
       " 'S',\n",
       " 'SH',\n",
       " 'T',\n",
       " 'TH',\n",
       " 'V',\n",
       " 'W',\n",
       " 'Y',\n",
       " 'Z',\n",
       " 'ZH',\n",
       " 'AA',\n",
       " 'AE',\n",
       " 'AH',\n",
       " 'AO',\n",
       " 'AW',\n",
       " 'AY',\n",
       " 'EH',\n",
       " 'ER',\n",
       " 'EY',\n",
       " 'IH',\n",
       " 'IY',\n",
       " 'OW',\n",
       " 'OY',\n",
       " 'UH',\n",
       " 'UW',\n",
       " \"'\",\n",
       " ',',\n",
       " '.',\n",
       " '!',\n",
       " '?',\n",
       " '-',\n",
       " ':',\n",
       " ';',\n",
       " '/',\n",
       " '\"',\n",
       " '(',\n",
       " ')',\n",
       " '[',\n",
       " ']',\n",
       " '{',\n",
       " '}',\n",
       " '<pad>',\n",
       " '<oov>',\n",
       " '<blank>']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vocab._id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c52e997",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
