{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "090f30a3",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#New-heading\" data-toc-modified-id=\"New-heading-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>New heading</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4635f0",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#New-heading\" data-toc-modified-id=\"New-heading-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>New heading</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fada16",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#New-heading\" data-toc-modified-id=\"New-heading-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>New heading</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c70302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp exec.select_speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab03fbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Foo:\n",
    "    foo: str = None\n",
    "    bar: str = None\n",
    "    baz: str = None\n",
    "\n",
    "\n",
    "# Foo = namedtuple(\"Foo\", [\"bar\", \"baz\", \"blah\"], defaults=(None,) * 3)\n",
    "f = Foo(\"hi\")\n",
    "assert f.foo == \"hi\"\n",
    "assert f.bar is None\n",
    "assert f.baz is None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed1ab55",
   "metadata": {},
   "source": [
    "# New heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15997f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import argparse\n",
    "from collections import namedtuple\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from shutil import copyfile, copytree\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "from uberduck_ml_dev.utils.audio import convert_to_wav\n",
    "from uberduck_ml_dev.utils.utils import parse_vctk\n",
    "\n",
    "STANDARD_MULTISPEAKER = \"standard-multispeaker\"\n",
    "STANDARD_SINGLESPEAKER = \"standard-singlespeaker\"\n",
    "VCTK = \"vctk\"\n",
    "FORMATS = [\n",
    "    STANDARD_MULTISPEAKER,\n",
    "    STANDARD_SINGLESPEAKER,\n",
    "    VCTK,\n",
    "]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Dataset:\n",
    "    path: str\n",
    "    format: str = STANDARD_MULTISPEAKER\n",
    "    speakers: str = None\n",
    "\n",
    "\n",
    "def _convert_vctk(f, out_path: str, ds: Dataset, start_speaker_id: int):\n",
    "    assert ds.format == VCTK, \"VCTK is the only format supported by this function!\"\n",
    "    vctk_data = parse_vctk(ds.path)\n",
    "    if ds.speakers:\n",
    "        speakers = ds.speakers.split(\",\")\n",
    "    else:\n",
    "        speakers = list(vctk_data.keys())\n",
    "    speaker_id = start_speaker_id\n",
    "    for speaker_name, speaker_data in vctk_data.items():\n",
    "        if speaker_name not in speakers:\n",
    "            continue\n",
    "        speaker_out_path = Path(out_path) / speaker_name\n",
    "        if not speaker_out_path.exists():\n",
    "            os.makedirs(speaker_out_path)\n",
    "        for transcription, flac_path in speaker_data:\n",
    "            assert flac_path.endswith(\".flac\")\n",
    "            # convert flac to wav in proper location\n",
    "            basename = os.path.basename(flac_path).replace(\".flac\", \".wav\")\n",
    "            rel_path = Path(speaker_name) / basename\n",
    "            convert_to_wav(flac_path, str(speaker_out_path / basename))\n",
    "            line = f\"{rel_path}|{transcription}|{speaker_id}\\n\"\n",
    "            f.write(line)\n",
    "        speaker_id += 1\n",
    "    return speaker_id - start_speaker_id\n",
    "\n",
    "\n",
    "def _convert_standard_multispeaker(\n",
    "    f, out_path: str, ds: Dataset, start_speaker_id: int\n",
    "):\n",
    "    speaker_id = start_speaker_id\n",
    "    if ds.speakers:\n",
    "        speakers = ds.speakers.split(\",\")\n",
    "    else:\n",
    "        speakers = os.listdir(root)\n",
    "    for speaker in speakers:\n",
    "        path = Path(root) / Path(speaker)\n",
    "        files = os.listdir(path)\n",
    "        transcriptions, *_ = [f for f in files if f.endswith(\".txt\")]\n",
    "        with (Path(root) / speaker / transcriptions).open(\"r\") as txn_f:\n",
    "            transcriptions = txn_f.readlines()\n",
    "        for line in transcriptions:\n",
    "            line = line.strip(\"\\n\")\n",
    "            try:\n",
    "                line_path, line_txn, *_ = line.split(\"|\")\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(line)\n",
    "                raise\n",
    "            line = f\"{str(Path(speaker) / Path(line_path))}|{line_txn}\"\n",
    "            f.write(f\"{line}|{speaker_id}\\n\")\n",
    "        wavs_dvc = path / \"wavs.dvc\"\n",
    "        speaker_out_path = Path(out_path) / speaker\n",
    "        if not speaker_out_path.exists():\n",
    "            os.makedirs(speaker_out_path)\n",
    "        # if wavs_dvc.exists():\n",
    "        #     copyfile(wavs_dvc, speaker_out_path / \"wavs.dvc\")\n",
    "        wavs_dir = path / \"wavs\"\n",
    "        if wavs_dir.exists():\n",
    "            # copytree(wavs_dir, speaker_out_path / \"wavs\")\n",
    "            os.symlink(wavs_dir, speaker_out_path / \"wavs\", target_is_directory=True)\n",
    "        speaker_id += 1\n",
    "    return speaker_id - start_speaker_id\n",
    "\n",
    "\n",
    "def _convert_to_multispeaker(f, out_path: str, ds: Dataset, start_speaker_id: int):\n",
    "    assert ds.format in [\n",
    "        STANDARD_MULTISPEAKER,\n",
    "        VCTK,\n",
    "    ], f\"Supported formats: {STANDARD_MULTISPEAKER}, {VCTK}\"\n",
    "    root = ds.path\n",
    "\n",
    "    print(ds.format)\n",
    "    print(ds.path)\n",
    "    print(ds.speakers)\n",
    "    if ds.format == STANDARD_MULTISPEAKER:\n",
    "        return _convert_standard_multispeaker(f, out_path, ds, start_speaker_id)\n",
    "    elif ds.format == VCTK:\n",
    "        return _convert_vctk(f, out_path, ds, start_speaker_id)\n",
    "\n",
    "\n",
    "def select_speakers(datasets: List[Dataset], out_dir):\n",
    "    speaker_id = 0\n",
    "    out_path = Path(out_dir)\n",
    "    if not out_path.exists():\n",
    "        os.makedirs(out_path)\n",
    "    with (out_path / \"list.txt\").open(\"w\") as f:\n",
    "        for ds in datasets:\n",
    "            speaker_count = _convert_to_multispeaker(f, out_path, ds, speaker_id)\n",
    "            speaker_id += speaker_count\n",
    "\n",
    "\n",
    "def parse_args(args):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"-o\", \"--out\", help=\"Path to dataset out directory\", default=\"./dataset\"\n",
    "    )\n",
    "    parser.add_argument(\"--config\", help=\"path to JSON config\")\n",
    "    parser.add_argument(\"-d\", \"--dataset\", action=\"append\", nargs=\"*\")\n",
    "    return parser.parse_args(args)\n",
    "\n",
    "\n",
    "try:\n",
    "    from nbdev.imports import IN_NOTEBOOK\n",
    "except:\n",
    "    IN_NOTEBOOK = False\n",
    "\n",
    "if __name__ == \"__main__\" and not IN_NOTEBOOK:\n",
    "    #     args = parse_args(sys.argv[1:])\n",
    "    #     if args.config:\n",
    "    #         config = json.load(args.config)\n",
    "    #         dataset = config[\"dataset\"]\n",
    "    #     elif args.dataset:\n",
    "    #         dataset = args.dataset\n",
    "    #     else:\n",
    "    #         raise Exception(\"Dataset must be specified\")\n",
    "    #     dataset_collection = [Dataset(*d) for d in dataset]\n",
    "    #     select_speakers(dataset_collection, args.out)\n",
    "    args = parse_args(sys.argv[1:])\n",
    "    dataset = args.dataset\n",
    "    name = args.name\n",
    "    numpoints = int(args.numpoints)\n",
    "    outdir = args.outdir\n",
    "    cores = int(args.cores)\n",
    "    data = load_data(dataset, name, outdir, numpoints)\n",
    "    natoms = data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e7a1fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Dataset(path='test/one/two', format='three,four,five', speakers=None),\n",
       " Dataset(path='foo/bar/baz', format='standard-multispeaker', speakers=None)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = parse_args([\"-d\", \"test/one/two\", \"three,four,five\", \"-d\", \"foo/bar/baz\"])\n",
    "assert len(args.dataset) == 2\n",
    "assert args.dataset == [[\"test/one/two\", \"three,four,five\"], [\"foo/bar/baz\"]]\n",
    "args = parse_args([\"-d\", \"foo\"])\n",
    "assert len(args.dataset) == 1\n",
    "assert args.dataset == [[\"foo\"]]\n",
    "[Dataset(*d) for d in [[\"test/one/two\", \"three,four,five\"], [\"foo/bar/baz\"]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345e4b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_args(\n",
    "    [\"--dataset\", \"~/data/voice/dvc-managed\", \"standard-multispeaker\", \"brock-sampson\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9981e5a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Dataset(path='~/data/voice/dvc-managed', format='standard-multispeaker', speakers='brock-sampson')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[Dataset(*d) for d in args.dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b273ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sam stuff\n",
    "import sqlite3\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def parse_vctk2(folder):\n",
    "\n",
    "    wav_dir = folder + \"wav48_silence_trimmed\"\n",
    "    txt_dir = folder + \"txt\"\n",
    "    speaker_wavs = os.listdir(wav_dir)\n",
    "    speaker_txts = os.listdir(txt_dir)\n",
    "    speakers = np.intersect1d(speaker_wavs, speaker_txts)\n",
    "\n",
    "    output_dict = {}\n",
    "    # wav_dict = {}\n",
    "    # txt_dict = {}\n",
    "    # speaker_dict = {}\n",
    "    counter = 0\n",
    "    namelist = np.zeros((0, 2), dtype=object)\n",
    "    for speaker in speakers:\n",
    "\n",
    "        speaker_wav_dir = wav_dir + \"/\" + speaker\n",
    "        speaker_txt_dir = txt_dir + \"/\" + speaker\n",
    "        wav_files_speaker = np.asarray(os.listdir(speaker_wav_dir))\n",
    "        txt_files_speaker = np.asarray(os.listdir(speaker_txt_dir))\n",
    "        # data_dict[wav_dir] = pd.DataFrame()\n",
    "\n",
    "        wav_files = np.asarray([])\n",
    "        nwavfiles = len(wav_files_speaker)\n",
    "        list1 = np.asarray(\n",
    "            [txt_files_speaker[i][:8] for i in range(len(txt_files_speaker))]\n",
    "        )\n",
    "        list2 = np.asarray([wav_files_speaker[i][:8] for i in range(nwavfiles)])\n",
    "        mic = np.asarray([wav_files_speaker[i][12] for i in range(nwavfiles)])\n",
    "        mic1_ind = mic == \"1\"\n",
    "        wav_files_speaker = wav_files_speaker[mic1_ind]\n",
    "        list2 = list2[mic1_ind]\n",
    "        combined_files = np.intersect1d(list1, list2)\n",
    "        matching_inds1 = np.where(np.isin(list1, combined_files))[0]\n",
    "        matching_inds2 = np.where(np.isin(list2, combined_files))[0]\n",
    "        inds1 = matching_inds1[list1[matching_inds1].argsort()]\n",
    "        inds2 = matching_inds2[list2[matching_inds2].argsort()]\n",
    "        txt_files_speaker = txt_files_speaker[inds1]\n",
    "        wav_files_speaker = wav_files_speaker[inds2]\n",
    "        texts = list()\n",
    "        for g in range(len(txt_files_speaker)):\n",
    "            text_file = speaker_txt_dir + \"/\" + txt_files_speaker[g]\n",
    "            with open(text_file) as f:\n",
    "                contents = f.read().splitlines()\n",
    "            # print(contents)\n",
    "            texts = np.append(texts, contents)\n",
    "\n",
    "            wav_file = speaker_wav_dir + \"/\" + wav_files_speaker[g]\n",
    "            wav_files = np.append(wav_files, wav_file)\n",
    "\n",
    "        if wav_files.shape[0] > 0:\n",
    "            output_dict[speaker] = pd.DataFrame(\n",
    "                [wav_files, texts, np.repeat(counter, wav_files.shape[0])]\n",
    "            ).transpose()\n",
    "            namelist = np.vstack([namelist, np.asarray([[speaker, counter]])])\n",
    "            counter = counter + 1\n",
    "\n",
    "    output = pd.concat(list(output_dict.values()))\n",
    "    return (output, namelist)\n",
    "\n",
    "\n",
    "def load_filepaths_and_text(filename, split=\"|\"):\n",
    "    with open(filename, encoding=\"utf-8\") as f:\n",
    "        filepaths_and_text = [line.strip().split(split) for line in f]\n",
    "    return filepaths_and_text\n",
    "\n",
    "\n",
    "def synthesize_speakerids2(filelists, fix_indices_index=None):\n",
    "\n",
    "    data_dict = {}\n",
    "    data_dict_out = {}\n",
    "    for f in range(len(filelists)):\n",
    "        # data_dict[filelists[f]] = pd.read_csv(filelists[f], sep = \",\",index_col=0,error_bad_lines=False)\n",
    "        data = load_filepaths_and_text(filelists[f])\n",
    "        data_dict[filelists[f]] = pd.DataFrame(data)\n",
    "    # pd.read_csv(filelists[f], sep = \"|\",header=None, error_bad_lines=False)\n",
    "\n",
    "    source_files = list(data_dict.keys())\n",
    "\n",
    "    speaker_offset = {}\n",
    "    nfilelist = len(filelists)\n",
    "    reserved_speakers = np.unique(data_dict[filelists[fix_indices_index]].iloc[:, 2])\n",
    "\n",
    "    for s in range(nfilelist):\n",
    "        source_file = filelists[s]\n",
    "        data = data_dict[source_file]\n",
    "        if s != fix_indices_index:\n",
    "            speakers = np.unique(data.iloc[:, 2])\n",
    "            overlap = np.where(np.isin(speakers, reserved_speakers))[0]\n",
    "            reserved_speakers_temp = np.union1d(speakers, reserved_speakers)\n",
    "            newindices = np.setdiff1d(\n",
    "                list(range(len(reserved_speakers) + len(speakers))),\n",
    "                reserved_speakers_temp,\n",
    "            )[: len(overlap)]\n",
    "            for o in range(len(overlap)):\n",
    "                data.iloc[np.where(data.iloc[:, 2] == overlap[o])[0], 2] = newindices[o]\n",
    "\n",
    "            data_dict_out[source_file] = data\n",
    "            speakers = np.unique(data.iloc[:, 2])\n",
    "            reserved_speakers = np.union1d(speakers, reserved_speakers)\n",
    "            # print(speakers,reserved_speakers)\n",
    "        else:\n",
    "            data_dict_out[source_file] = data\n",
    "    return data_dict_out\n",
    "\n",
    "\n",
    "def get_filelist(database, speakerjson):\n",
    "    \"\"\"\n",
    "    Take a list of speakers and create a filelist\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def subset_speakers(database, seed, speakerlist=None, nspeakers=None):\n",
    "    \"\"\"\n",
    "    Takes a filelist and saves another filelist with either a random subset of speakers or speakers from the list\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740a3aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vctk_filelist2, namelist = parse_vctk2(vctk_folder)\n",
    "print(namelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5a7f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"test.db\")\n",
    "\n",
    "conn.execute(\n",
    "    \"\"\"CREATE TABLE DATAINFO\n",
    "         (ID INT PRIMARY KEY     NOT NULL,\n",
    "         NAME           TEXT     NOT NULL,\n",
    "         SOURCE         TEXT     NOT NULL,\n",
    "         FILELIST       TEXT,\n",
    "         SPEAKERID      INT);\"\"\"\n",
    ")\n",
    "\n",
    "conn.execute(\n",
    "    \"INSERT INTO DATAINFO (ID,NAME,SOURCE,FILELIST,SPEAKERID) \\\n",
    "      VALUES (1, 'eminem', 'uberduck', '/mnt/disks/uberduck-experiments-v0/uberduck-ml-dev/experiments/processed_metadata/eminem_all_processed.txt', 0)\"\n",
    ")\n",
    "\n",
    "# id doesnt need to be included necessarily but makes sense\n",
    "cursor = conn.execute(\"SELECT ID, NAME, SOURCE, FILELIST from DATAINFO\")\n",
    "for row in cursor:\n",
    "    print(\"ID = \", row[0])\n",
    "    print(\"NAME = \", row[1])\n",
    "    print(\"ADDRESS = \", row[2])\n",
    "    print(\"SALARY = \", row[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1b5146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['vctk' 'p302']\n",
      " ['uberduck' 'eminem']]\n"
     ]
    }
   ],
   "source": [
    "# this is an example of a speakerlist that would be loaded from a json\n",
    "# a json could also be generated by querying the database e.g.\n",
    "# subset_speakers(database, seed, speakerlist = None, nspeakers = None)\n",
    "speakerlist = np.asarray([[\"vctk\", \"p302\"], [\"uberduck\", \"eminem\"]])\n",
    "print(speakerlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0438cf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now call get_filelist(database, speakerjson)\n",
    "# this list (filelist locations) would be got from database\n",
    "metalist_dir = \"/mnt/disks/uberduck-experiments-v0/uberduck-ml-dev/experiments/metadata_collections\"\n",
    "metalist_files = os.listdir(metalist_dir)\n",
    "# print(metalist_files)\n",
    "train_ratios = np.ones(4) * 1.0\n",
    "print(np.load(metalist_dir + \"/\" + metalist_files[0]))\n",
    "print(np.load(metalist_dir + \"/libritts_processed_file.npy\"))\n",
    "print(np.load(metalist_dir + \"/uberduck_processed_files.npy\", allow_pickle=True))\n",
    "print(\n",
    "    np.load(\n",
    "        \"/mnt/disks/uberduck-experiments-v0/uberduck-ml-dev/experiments/metadata_collections/vctk_processed_file.npy\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a3eab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/mnt/disks/uberduck-experiments-v0/uberduck-ml-dev/experiments/processed_metadata/lj7all_processed.txt'\n",
      " '/mnt/disks/uberduck-experiments-v0/uberduck-ml-dev/experiments/processed_metadata/librittsall_processed.txt'\n",
      " '/mnt/disks/uberduck-experiments-v0/uberduck-ml-dev/experiments/processed_metadata/eminem_all_processed.txt'\n",
      " '/mnt/disks/uberduck-experiments-v0/uberduck-ml-dev/experiments/processed_metadata/ben-shapiro_all_processed.txt'\n",
      " '/mnt/disks/uberduck-experiments-v0/uberduck-ml-dev/experiments/processed_metadata/jay-z_all_processed.txt'\n",
      " '/mnt/disks/uberduck-experiments-v0/uberduck-ml-dev/experiments/processed_metadata/vctkall_processed.txt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/numpy/lib/arraysetops.py:565: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask &= (ar1 != a)\n"
     ]
    }
   ],
   "source": [
    "# now synthesize selected speakers from filelists in database and synthesize selected (e.g. [2,4,5])\n",
    "# something like the following, but with the ability to take only rows of multispeaker datasets corresponding to particular speakers\n",
    "filelists = np.asarray([])\n",
    "# files = np.asarray([])\n",
    "for r in range(4):\n",
    "    files = np.load(metalist_dir + \"/\" + metalist_files[r], allow_pickle=True)\n",
    "    filelist = np.asarray([])\n",
    "\n",
    "    if files.ndim > 0:\n",
    "        nfiles = files.shape[0]\n",
    "        for s in range(nfiles):\n",
    "            filelist = np.append(filelist, files[s])\n",
    "    else:\n",
    "        filelist = files\n",
    "    filelists = np.append(filelists, filelist)\n",
    "\n",
    "print(filelists)\n",
    "dd = synthesize_speakerids2(filelists, 1)\n",
    "ad = list(dd.values())\n",
    "ad2 = [ad[i] for i in [2, 4, 5]]\n",
    "alldata = pd.concat(ad2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba740bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                       0  \\\n",
      "0      /mnt/disks/uberduck-experiments-v0/data/uberdu...   \n",
      "1      /mnt/disks/uberduck-experiments-v0/data/uberdu...   \n",
      "2      /mnt/disks/uberduck-experiments-v0/data/uberdu...   \n",
      "3      /mnt/disks/uberduck-experiments-v0/data/uberdu...   \n",
      "4      /mnt/disks/uberduck-experiments-v0/data/uberdu...   \n",
      "...                                                  ...   \n",
      "43880  /mnt/disks/uberduck-experiments-v0/data/vctk/w...   \n",
      "43881  /mnt/disks/uberduck-experiments-v0/data/vctk/w...   \n",
      "43882  /mnt/disks/uberduck-experiments-v0/data/vctk/w...   \n",
      "43883  /mnt/disks/uberduck-experiments-v0/data/vctk/w...   \n",
      "43884  /mnt/disks/uberduck-experiments-v0/data/vctk/w...   \n",
      "\n",
      "                                                       1    2  \n",
      "0            It's like this and like that and like this.    0  \n",
      "1      I'm the illest rapper to hold the cordless, pa...    0  \n",
      "2      I'm meaner in action than Roscoe beatin' James...    0  \n",
      "3      I grew up in a wild hood, as a hazardous youth...    0  \n",
      "4      And ain't shit changed, I kept the same mind s...    0  \n",
      "...                                                  ...  ...  \n",
      "43880                      I can't stop people thinking.  107  \n",
      "43881                         That will be best for him.  107  \n",
      "43882                         That can happen so easily.  107  \n",
      "43883                I have not been so angry for years.  107  \n",
      "43884                                 Do you mean today?  107  \n",
      "\n",
      "[44782 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(alldata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
