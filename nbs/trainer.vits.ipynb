{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "224180cb",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Debugging\" data-toc-modified-id=\"Debugging-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Debugging</a></span></li><li><span><a href=\"#Trainer\" data-toc-modified-id=\"Trainer-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Trainer</a></span></li><li><span><a href=\"#Losses\" data-toc-modified-id=\"Losses-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Losses</a></span></li><li><span><a href=\"#VITS-Trainer\" data-toc-modified-id=\"VITS-Trainer-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>VITS Trainer</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df74d1c",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535085d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp trainer.vits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d789a053",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41357cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.util as librosa_util\n",
    "from librosa.util import normalize, pad_center, tiny\n",
    "from scipy.signal import get_window\n",
    "from scipy.io.wavfile import read\n",
    "from librosa.filters import mel as librosa_mel_fn\n",
    "\n",
    "MAX_WAV_VALUE = 32768.0\n",
    "\n",
    "\n",
    "def dynamic_range_compression_torch(x, C=1, clip_val=1e-5):\n",
    "    \"\"\"\n",
    "    PARAMS\n",
    "    ------\n",
    "    C: compression factor\n",
    "    \"\"\"\n",
    "    return torch.log(torch.clamp(x, min=clip_val) * C)\n",
    "\n",
    "\n",
    "def dynamic_range_decompression_torch(x, C=1):\n",
    "    \"\"\"\n",
    "    PARAMS\n",
    "    ------\n",
    "    C: compression factor used to compress\n",
    "    \"\"\"\n",
    "    return torch.exp(x) / C\n",
    "\n",
    "\n",
    "def spectral_normalize_torch(magnitudes):\n",
    "    output = dynamic_range_compression_torch(magnitudes)\n",
    "    return output\n",
    "\n",
    "\n",
    "def spectral_de_normalize_torch(magnitudes):\n",
    "    output = dynamic_range_decompression_torch(magnitudes)\n",
    "    return output\n",
    "\n",
    "\n",
    "mel_basis = {}\n",
    "hann_window = {}\n",
    "\n",
    "\n",
    "def spectrogram_torch(y, n_fft, sampling_rate, hop_size, win_size, center=False):\n",
    "    if torch.min(y) < -1.0:\n",
    "        print(\"min value is \", torch.min(y))\n",
    "    if torch.max(y) > 1.0:\n",
    "        print(\"max value is \", torch.max(y))\n",
    "\n",
    "    global hann_window\n",
    "    dtype_device = str(y.dtype) + \"_\" + str(y.device)\n",
    "    wnsize_dtype_device = str(win_size) + \"_\" + dtype_device\n",
    "    if wnsize_dtype_device not in hann_window:\n",
    "        hann_window[wnsize_dtype_device] = torch.hann_window(win_size).to(\n",
    "            dtype=y.dtype, device=y.device\n",
    "        )\n",
    "\n",
    "    y = torch.nn.functional.pad(\n",
    "        y.unsqueeze(1),\n",
    "        (int((n_fft - hop_size) / 2), int((n_fft - hop_size) / 2)),\n",
    "        mode=\"reflect\",\n",
    "    )\n",
    "    y = y.squeeze(1)\n",
    "\n",
    "    spec = torch.stft(\n",
    "        y,\n",
    "        n_fft,\n",
    "        hop_length=hop_size,\n",
    "        win_length=win_size,\n",
    "        window=hann_window[wnsize_dtype_device],\n",
    "        center=center,\n",
    "        pad_mode=\"reflect\",\n",
    "        normalized=False,\n",
    "        onesided=True,\n",
    "    )\n",
    "\n",
    "    spec = torch.sqrt(spec.pow(2).sum(-1) + 1e-6)\n",
    "    return spec\n",
    "\n",
    "\n",
    "def spec_to_mel_torch(spec, n_fft, num_mels, sampling_rate, fmin, fmax):\n",
    "    global mel_basis\n",
    "    dtype_device = str(spec.dtype) + \"_\" + str(spec.device)\n",
    "    fmax_dtype_device = str(fmax) + \"_\" + dtype_device\n",
    "    if fmax_dtype_device not in mel_basis:\n",
    "        mel = librosa_mel_fn(sampling_rate, n_fft, num_mels, fmin, fmax)\n",
    "        mel_basis[fmax_dtype_device] = torch.from_numpy(mel).to(\n",
    "            dtype=spec.dtype, device=spec.device\n",
    "        )\n",
    "    spec = torch.matmul(mel_basis[fmax_dtype_device], spec)\n",
    "    spec = spectral_normalize_torch(spec)\n",
    "    return spec\n",
    "\n",
    "\n",
    "def mel_spectrogram_torch(\n",
    "    y, n_fft, num_mels, sampling_rate, hop_size, win_size, fmin, fmax, center=False\n",
    "):\n",
    "    if torch.min(y) < -1.0:\n",
    "        print(\"min value is \", torch.min(y))\n",
    "    if torch.max(y) > 1.0:\n",
    "        print(\"max value is \", torch.max(y))\n",
    "\n",
    "    global mel_basis, hann_window\n",
    "    dtype_device = str(y.dtype) + \"_\" + str(y.device)\n",
    "    fmax_dtype_device = str(fmax) + \"_\" + dtype_device\n",
    "    wnsize_dtype_device = str(win_size) + \"_\" + dtype_device\n",
    "    if fmax_dtype_device not in mel_basis:\n",
    "        mel = librosa_mel_fn(sampling_rate, n_fft, num_mels, fmin, fmax)\n",
    "        mel_basis[fmax_dtype_device] = torch.from_numpy(mel).to(\n",
    "            dtype=y.dtype, device=y.device\n",
    "        )\n",
    "    if wnsize_dtype_device not in hann_window:\n",
    "        hann_window[wnsize_dtype_device] = torch.hann_window(win_size).to(\n",
    "            dtype=y.dtype, device=y.device\n",
    "        )\n",
    "\n",
    "    y = torch.nn.functional.pad(\n",
    "        y.unsqueeze(1),\n",
    "        (int((n_fft - hop_size) / 2), int((n_fft - hop_size) / 2)),\n",
    "        mode=\"reflect\",\n",
    "    )\n",
    "    y = y.squeeze(1)\n",
    "\n",
    "    spec = torch.stft(\n",
    "        y,\n",
    "        n_fft,\n",
    "        hop_length=hop_size,\n",
    "        win_length=win_size,\n",
    "        window=hann_window[wnsize_dtype_device],\n",
    "        center=center,\n",
    "        pad_mode=\"reflect\",\n",
    "        normalized=False,\n",
    "        onesided=True,\n",
    "    )\n",
    "\n",
    "    spec = torch.sqrt(spec.pow(2).sum(-1) + 1e-6)\n",
    "\n",
    "    spec = torch.matmul(mel_basis[fmax_dtype_device], spec)\n",
    "    spec = spectral_normalize_torch(spec)\n",
    "\n",
    "    return spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653d4f95",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47500540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.distributed as dist\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "\n",
    "from uberduck_ml_dev.models.common import MelSTFT\n",
    "from uberduck_ml_dev.utils.plot import (\n",
    "    plot_attention,\n",
    "    plot_gate_outputs,\n",
    "    plot_spectrogram,\n",
    ")\n",
    "from uberduck_ml_dev.text.util import text_to_sequence, random_utterance\n",
    "from uberduck_ml_dev.text.symbols import symbols_with_ipa\n",
    "from uberduck_ml_dev.trainer.base import TTSTrainer\n",
    "\n",
    "from uberduck_ml_dev.models.vits import (\n",
    "    DEFAULTS,\n",
    "    MultiPeriodDiscriminator,\n",
    "    SynthesizerTrn,\n",
    ")\n",
    "from uberduck_ml_dev.data_loader import (\n",
    "    TextAudioSpeakerLoader,\n",
    "    TextAudioSpeakerCollate,\n",
    "    DistributedBucketSampler,\n",
    ")\n",
    "from uberduck_ml_dev.vendor.tfcompat.hparam import HParams\n",
    "from uberduck_ml_dev.utils.plot import save_figure_to_numpy, plot_spectrogram\n",
    "from uberduck_ml_dev.utils.utils import slice_segments, clip_grad_value_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b54d4d6",
   "metadata": {},
   "source": [
    "# Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc21ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def feature_loss(fmap_r, fmap_g):\n",
    "    loss = 0\n",
    "    for dr, dg in zip(fmap_r, fmap_g):\n",
    "        for rl, gl in zip(dr, dg):\n",
    "            rl = rl.float().detach()\n",
    "            gl = gl.float()\n",
    "            loss += torch.mean(torch.abs(rl - gl))\n",
    "\n",
    "    return loss * 2\n",
    "\n",
    "\n",
    "def discriminator_loss(disc_real_outputs, disc_generated_outputs):\n",
    "    loss = 0\n",
    "    r_losses = []\n",
    "    g_losses = []\n",
    "    for dr, dg in zip(disc_real_outputs, disc_generated_outputs):\n",
    "        dr = dr.float()\n",
    "        dg = dg.float()\n",
    "        r_loss = torch.mean((1 - dr) ** 2)\n",
    "        g_loss = torch.mean(dg ** 2)\n",
    "        loss += r_loss + g_loss\n",
    "        r_losses.append(r_loss.item())\n",
    "        g_losses.append(g_loss.item())\n",
    "\n",
    "    return loss, r_losses, g_losses\n",
    "\n",
    "\n",
    "def generator_loss(disc_outputs):\n",
    "    loss = 0\n",
    "    gen_losses = []\n",
    "    for dg in disc_outputs:\n",
    "        dg = dg.float()\n",
    "        l = torch.mean((1 - dg) ** 2)\n",
    "        gen_losses.append(l)\n",
    "        loss += l\n",
    "\n",
    "    return loss, gen_losses\n",
    "\n",
    "\n",
    "def kl_loss(z_p, logs_q, m_p, logs_p, z_mask):\n",
    "    \"\"\"\n",
    "    z_p, logs_q: [b, h, t_t]\n",
    "    m_p, logs_p: [b, h, t_t]\n",
    "    \"\"\"\n",
    "    z_p = z_p.float()\n",
    "    logs_q = logs_q.float()\n",
    "    m_p = m_p.float()\n",
    "    logs_p = logs_p.float()\n",
    "    z_mask = z_mask.float()\n",
    "\n",
    "    kl = logs_p - logs_q - 0.5\n",
    "    kl += 0.5 * ((z_p - m_p) ** 2) * torch.exp(-2.0 * logs_p)\n",
    "    kl = torch.sum(kl * z_mask)\n",
    "    l = kl / torch.sum(z_mask)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b34fa71",
   "metadata": {},
   "source": [
    "# VITS Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e61d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class VITSTrainer(TTSTrainer):\n",
    "    REQUIRED_HPARAMS = [\n",
    "        \"betas\",\n",
    "        \"c_kl\",\n",
    "        \"c_mel\",\n",
    "        \"eps\",\n",
    "        \"lr_decay\",\n",
    "        \"segment_size\",\n",
    "        \"training_audiopaths_and_text\",\n",
    "        \"val_audiopaths_and_text\",\n",
    "        \"warm_start_name_g\",\n",
    "        \"warm_start_name_d\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.mel_stft = MelSTFT(device=self.device)\n",
    "        self.log_interval = 10\n",
    "        for param in self.REQUIRED_HPARAMS:\n",
    "            if not hasattr(self, param):\n",
    "                raise Exception(f\"VITSTrainer missing a required param: {param}\")\n",
    "\n",
    "    def _log_training(self, scalars, images):\n",
    "        print(\"log training placeholder...\")\n",
    "        if self.rank != 0 or self.global_step % self.log_interval != 0:\n",
    "            return\n",
    "        for k, v in scalars.items():\n",
    "            pieces = k.split(\"_\")\n",
    "            key = \"/\".join(pieces)\n",
    "            self.log(key, self.global_step, scalar=v)\n",
    "        for k, v in images.items():\n",
    "            pieces = k.split(\"_\")\n",
    "            key = \"/\".join(pieces)\n",
    "            self.log(key, self.global_step, image=v)\n",
    "\n",
    "    def _log_validation(self):\n",
    "        print(\"log validation...\")\n",
    "        pass\n",
    "\n",
    "    def save_checkpoint(self, checkpoint_name, model, optimizer, learning_rate, epoch):\n",
    "        if self.rank != 0:\n",
    "            return\n",
    "        if hasattr(model, \"module\"):\n",
    "            state_dict = model.module.state_dict()\n",
    "        else:\n",
    "            state_dict = model.state_dict()\n",
    "        os.makedirs(self.checkpoint_path, exist_ok=True)\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model\": model,\n",
    "                \"global_step\": self.global_step,\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"learning_rate\": learning_rate,\n",
    "                \"epoch\": epoch,\n",
    "            },\n",
    "            os.path.join(self.checkpoint_path, f\"{checkpoint_name}.pt\"),\n",
    "        )\n",
    "\n",
    "    def warm_start(self, net_g, net_d, optim_g, optim_d):\n",
    "        if not (self.warm_start_name_g and self.warm_start_name_d):\n",
    "            return net_g, net_d, optim_g, optim_d, 0\n",
    "        if self.warm_start_name_g:\n",
    "            checkpoint = torch.load(self.warm_start_name_g)\n",
    "            net_g.load_state_dict(checkpoint[\"model\"])\n",
    "            optim_g.load_state_dict(checkpoint[\"optimizer\"])\n",
    "        if self.warm_start_name_d:\n",
    "            checkpoint = torch.load(self.warm_start_name_d)\n",
    "            net_d.load_state_dict(checkpoint[\"model\"])\n",
    "            optim_d.load_state_dict(checkpoint[\"optimizer\"])\n",
    "        self.global_step = checkpoint[\"global_step\"]\n",
    "        self.learning_rate = checkpoint[\"learning_rate\"]\n",
    "        start_epoch = checkpoint[\"epoch\"]\n",
    "        return net_g, net_d, optim_g, optim_d, start_epoch\n",
    "\n",
    "    def _batch_to_device(self, *args):\n",
    "        ret = []\n",
    "        if self.device == \"cuda\":\n",
    "            for arg in args:\n",
    "                arg = arg.cuda(self.rank, non_blocking=True)\n",
    "                ret.append(arg)\n",
    "            return ret\n",
    "        else:\n",
    "            return args\n",
    "\n",
    "    def _evaluate(self, generator, val_loader):\n",
    "        print(\"Validation ...\")\n",
    "        generator.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(val_loader):\n",
    "                (\n",
    "                    x,\n",
    "                    x_lengths,\n",
    "                    spec,\n",
    "                    spec_lengths,\n",
    "                    y,\n",
    "                    y_lengths,\n",
    "                    speakers,\n",
    "                ) = self._batch_to_device(*batch)\n",
    "                x = x[:1]\n",
    "                x_lengths = x_lengths[:1]\n",
    "                spec = spec[:1]\n",
    "                spec_lengths[:1]\n",
    "                y = y[:1]\n",
    "                y_lengths = y_lengths[:1]\n",
    "                speakers = speakers[:1]\n",
    "                break\n",
    "            if self.distributed_run:\n",
    "                y_hat, attn, mask, *_ = generator.module.infer(\n",
    "                    x, x_lengths, speakers, max_len=1000\n",
    "                )\n",
    "            else:\n",
    "                y_hat, attn, mask, *_ = generator.infer(\n",
    "                    x, x_lengths, speakers, max_len=1000\n",
    "                )\n",
    "            y_hat_lengths = mask.sum([1, 2]).long() * self.hparams.hop_length\n",
    "            mel = self.mel_stft.spec_to_mel(spec)\n",
    "            print(\"y_hat: \", y_hat.shape)\n",
    "            y_hat_mel = self.mel_stft.mel_spectrogram(y_hat.squeeze(1).float())\n",
    "        self.log(\n",
    "            \"Val/mel_gen\",\n",
    "            self.global_step,\n",
    "            image=save_figure_to_numpy(plot_spectrogram(y_hat_mel[0].data.cpu())),\n",
    "        )\n",
    "        self.log(\n",
    "            \"Val/mel_gt\",\n",
    "            self.global_step,\n",
    "            image=save_figure_to_numpy(plot_spectrogram(mel[0].data.cpu())),\n",
    "        )\n",
    "        self.log(\n",
    "            \"Val/audio_gen\", self.global_step, audio=y_hat[0, :, : y_hat_lengths[0]]\n",
    "        )\n",
    "        self.log(\"Val/audio_gt\", self.global_step, audio=y[0, :, : y_lengths[0]])\n",
    "        generator.train()\n",
    "\n",
    "    def _train_and_evaluate(\n",
    "        self, epoch, nets, optims, schedulers, scaler: GradScaler, loaders\n",
    "    ):\n",
    "        net_g, net_d = nets\n",
    "        optim_g, optim_d = optims\n",
    "        scheduler_g, scheduler_d = schedulers\n",
    "        train_loader, val_loader = loaders\n",
    "        train_loader.batch_sampler.set_epoch(epoch)\n",
    "        net_g.train()\n",
    "        net_d.train()\n",
    "        # TODO (zach): remove when you want to.\n",
    "        # self._evaluate(net_g, val_loader)\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            print(f\"global step: {self.global_step}\")\n",
    "            print(f\"batch idx: {batch_idx}\")\n",
    "            (\n",
    "                x,\n",
    "                x_lengths,\n",
    "                spec,\n",
    "                spec_lengths,\n",
    "                y,\n",
    "                y_lengths,\n",
    "                speakers,\n",
    "            ) = self._batch_to_device(*batch)\n",
    "\n",
    "            with autocast(enabled=self.fp16_run):\n",
    "                (\n",
    "                    y_hat,\n",
    "                    l_length,\n",
    "                    attn,\n",
    "                    ids_slice,\n",
    "                    x_mask,\n",
    "                    z_mask,\n",
    "                    (z, z_p, m_p, logs_p, m_q, logs_q),\n",
    "                ) = net_g(x, x_lengths, spec, spec_lengths, speakers)\n",
    "                mel = self.mel_stft.spec_to_mel(spec)\n",
    "                # mel = spec_to_mel_torch(\n",
    "                #     spec,\n",
    "                #     self.filter_length,\n",
    "                #     self.n_mel_channels,\n",
    "                #     self.sampling_rate,\n",
    "                #     self.mel_fmin,\n",
    "                #     self.mel_fmax,\n",
    "                # )\n",
    "                # NOTE(zach): slight difference from the original VITS implementation due to padding differences in the spectrograms\n",
    "                y_mel = slice_segments(\n",
    "                    mel,\n",
    "                    ids_slice,\n",
    "                    self.segment_size // self.hop_length + 1\n",
    "                    # mel, ids_slice, self.segment_size // self.hop_length\n",
    "                )\n",
    "                y_hat_mel = self.mel_stft.mel_spectrogram(y_hat.squeeze(1))\n",
    "                # y_hat_mel = mel_spectrogram_torch(\n",
    "                #     y_hat.squeeze(1),\n",
    "                #     self.filter_length,\n",
    "                #     self.n_mel_channels,\n",
    "                #     self.sampling_rate,\n",
    "                #     self.hop_length,\n",
    "                #     self.win_length,\n",
    "                #     self.mel_fmin,\n",
    "                #     self.mel_fmax,\n",
    "                # )\n",
    "                y = slice_segments(y, ids_slice * self.hop_length, self.segment_size)\n",
    "\n",
    "                # Discriminator\n",
    "                y_d_hat_r, y_d_hat_g, _, _ = net_d(y, y_hat.detach())\n",
    "                with autocast(enabled=False):\n",
    "                    loss_disc, losses_disc_r, losses_disc_g = discriminator_loss(\n",
    "                        y_d_hat_r, y_d_hat_g\n",
    "                    )\n",
    "                    loss_disc_all = loss_disc\n",
    "            optim_d.zero_grad()\n",
    "            scaler.scale(loss_disc_all).backward()\n",
    "            scaler.unscale_(optim_d)\n",
    "            grad_norm_d = clip_grad_value_(net_d.parameters(), None)\n",
    "            scaler.step(optim_d)\n",
    "\n",
    "            with autocast(enabled=self.fp16_run):\n",
    "                # Generator\n",
    "                y_d_hat_r, y_d_hat_g, fmap_r, fmap_g = net_d(y, y_hat)\n",
    "                with autocast(enabled=False):\n",
    "                    loss_dur = torch.sum(l_length.float())\n",
    "                    loss_mel = F.l1_loss(y_mel, y_hat_mel) * self.c_mel\n",
    "                    loss_kl = kl_loss(z_p, logs_q, m_p, logs_p, z_mask) * self.c_kl\n",
    "\n",
    "                    loss_fm = feature_loss(fmap_r, fmap_g)\n",
    "                    loss_gen, losses_gen = generator_loss(y_d_hat_g)\n",
    "                    loss_gen_all = loss_gen + loss_fm + loss_mel + loss_dur + loss_kl\n",
    "            optim_g.zero_grad()\n",
    "            scaler.scale(loss_gen_all).backward()\n",
    "            scaler.unscale_(optim_g)\n",
    "            grad_norm_g = clip_grad_value_(net_g.parameters(), None)\n",
    "            scaler.step(optim_g)\n",
    "            scaler.update()\n",
    "\n",
    "            self._log_training(\n",
    "                scalars=dict(\n",
    "                    loss_g_total=loss_gen_all,\n",
    "                    loss_d_total=loss_disc_all,\n",
    "                    gradnorm_d=grad_norm_d,\n",
    "                    gradnorm_g=grad_norm_g,\n",
    "                    loss_g_fm=loss_fm,\n",
    "                    loss_g_dur=loss_dur,\n",
    "                    loss_g_mel=loss_mel,\n",
    "                    loss_g_kl=loss_kl,\n",
    "                ),\n",
    "                images=dict(\n",
    "                    slice_mel_org=save_figure_to_numpy(\n",
    "                        plot_spectrogram(y_mel[0].data.cpu())\n",
    "                    ),\n",
    "                    slice_mel_gen=save_figure_to_numpy(\n",
    "                        plot_spectrogram(y_hat_mel[0].data.cpu())\n",
    "                    ),\n",
    "                    all_mel=save_figure_to_numpy(plot_spectrogram(mel[0].data.cpu())),\n",
    "                    all_attn=save_figure_to_numpy(\n",
    "                        plot_attention(attn[0, 0].data.cpu())\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "            self.global_step += 1\n",
    "        self._evaluate(net_g, val_loader)\n",
    "\n",
    "    def train(self):\n",
    "        train_dataset = TextAudioSpeakerLoader(\n",
    "            self.training_audiopaths_and_text,\n",
    "            self.hparams,\n",
    "            debug=self.debug,\n",
    "            debug_dataset_size=self.debug_dataset_size,\n",
    "        )\n",
    "        train_sampler = DistributedBucketSampler(\n",
    "            train_dataset,\n",
    "            self.batch_size,\n",
    "            [32, 300, 400, 500, 600, 700, 800, 900, 1000],\n",
    "            num_replicas=self.world_size,\n",
    "            rank=self.rank,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        collate_fn = TextAudioSpeakerCollate()\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            num_workers=0,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            collate_fn=collate_fn,\n",
    "            batch_sampler=train_sampler,\n",
    "        )\n",
    "        if self.rank == 0:\n",
    "            val_dataset = TextAudioSpeakerLoader(\n",
    "                self.val_audiopaths_and_text,\n",
    "                self.hparams,\n",
    "                debug=self.debug,\n",
    "                debug_dataset_size=self.debug_dataset_size,\n",
    "            )\n",
    "            val_loader = DataLoader(\n",
    "                val_dataset,\n",
    "                num_workers=0,\n",
    "                shuffle=False,\n",
    "                batch_size=self.batch_size,\n",
    "                pin_memory=True,\n",
    "                drop_last=False,\n",
    "                collate_fn=collate_fn,\n",
    "            )\n",
    "\n",
    "        model_kwargs = {k: v for k, v in DEFAULTS.values().items() if hasattr(self, k)}\n",
    "        net_g = SynthesizerTrn(\n",
    "            len(symbols_with_ipa),\n",
    "            self.filter_length // 2 + 1,\n",
    "            self.segment_size // self.hop_length,\n",
    "            n_speakers=self.n_speakers,\n",
    "            **model_kwargs,\n",
    "        )\n",
    "        net_d = MultiPeriodDiscriminator(self.use_spectral_norm)\n",
    "        if self.device == \"cuda\":\n",
    "            net_g = net_g.cuda(self.rank)\n",
    "            net_d = net_d.cuda(self.rank)\n",
    "\n",
    "        optim_g = torch.optim.AdamW(\n",
    "            net_g.parameters(),\n",
    "            self.learning_rate,\n",
    "            betas=self.betas,\n",
    "            eps=self.eps,\n",
    "        )\n",
    "        optim_d = torch.optim.AdamW(\n",
    "            net_d.parameters(), self.learning_rate, betas=self.betas, eps=self.eps\n",
    "        )\n",
    "        if self.distributed_run:\n",
    "            net_g = DDP(net_g, device_ids=[self.rank])\n",
    "            net_d = DDP(net_d, device_ids=[self.rank])\n",
    "\n",
    "        start_epoch = 0\n",
    "        net_g, net_d, optim_g, optim_d, start_epoch = self.warm_start(\n",
    "            net_g,\n",
    "            net_d,\n",
    "            optim_g,\n",
    "            optim_d,\n",
    "        )\n",
    "\n",
    "        scheduler_g = ExponentialLR(\n",
    "            optim_g, gamma=self.lr_decay, last_epoch=start_epoch - 1\n",
    "        )\n",
    "        scheduler_d = ExponentialLR(\n",
    "            optim_d, gamma=self.lr_decay, last_epoch=start_epoch - 1\n",
    "        )\n",
    "        scaler = GradScaler(enabled=self.fp16_run)\n",
    "\n",
    "        for epoch in range(start_epoch, self.epochs):\n",
    "            self._train_and_evaluate(\n",
    "                epoch,\n",
    "                [net_g, net_d],\n",
    "                [optim_g, optim_d],\n",
    "                [scheduler_g, scheduler_d],\n",
    "                scaler,\n",
    "                [train_loader, val_loader],\n",
    "            )\n",
    "            if epoch % self.epochs_per_checkpoint == 0:\n",
    "                self.save_checkpoint(\n",
    "                    f\"vits_G_{self.global_step}\",\n",
    "                    net_g,\n",
    "                    optim_g,\n",
    "                    self.learning_rate,\n",
    "                    epoch,\n",
    "                )\n",
    "                self.save_checkpoint(\n",
    "                    f\"vits_D_{self.global_step}\",\n",
    "                    net_d,\n",
    "                    optim_d,\n",
    "                    self.learning_rate,\n",
    "                    epoch,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0134d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
