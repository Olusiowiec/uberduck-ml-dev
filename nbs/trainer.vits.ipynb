{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "224180cb",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Losses\" data-toc-modified-id=\"Losses-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Losses</a></span></li><li><span><a href=\"#VITS-Trainer\" data-toc-modified-id=\"VITS-Trainer-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>VITS Trainer</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df74d1c",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535085d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp trainer.vits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47500540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.distributed as dist\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "\n",
    "from uberduck_ml_dev.models.common import MelSTFT\n",
    "from uberduck_ml_dev.utils.plot import (\n",
    "    plot_attention,\n",
    "    plot_gate_outputs,\n",
    "    plot_spectrogram,\n",
    ")\n",
    "from uberduck_ml_dev.text.util import text_to_sequence, random_utterance\n",
    "from uberduck_ml_dev.text.symbols import symbols_with_ipa\n",
    "from uberduck_ml_dev.trainer.base import TTSTrainer\n",
    "\n",
    "from uberduck_ml_dev.models.vits import (\n",
    "    DEFAULTS,\n",
    "    MultiPeriodDiscriminator,\n",
    "    SynthesizerTrn,\n",
    ")\n",
    "from uberduck_ml_dev.data_loader import (\n",
    "    TextAudioSpeakerLoader,\n",
    "    TextAudioSpeakerCollate,\n",
    "    DistributedBucketSampler,\n",
    ")\n",
    "from uberduck_ml_dev.vendor.tfcompat.hparam import HParams\n",
    "from uberduck_ml_dev.utils.utils import slice_segments, clip_grad_value_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b54d4d6",
   "metadata": {},
   "source": [
    "# Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc21ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def feature_loss(fmap_r, fmap_g):\n",
    "    loss = 0\n",
    "    for dr, dg in zip(fmap_r, fmap_g):\n",
    "        for rl, gl in zip(dr, dg):\n",
    "            rl = rl.float().detach()\n",
    "            gl = gl.float()\n",
    "            loss += torch.mean(torch.abs(rl - gl))\n",
    "\n",
    "    return loss * 2\n",
    "\n",
    "\n",
    "def discriminator_loss(disc_real_outputs, disc_generated_outputs):\n",
    "    loss = 0\n",
    "    r_losses = []\n",
    "    g_losses = []\n",
    "    for dr, dg in zip(disc_real_outputs, disc_generated_outputs):\n",
    "        dr = dr.float()\n",
    "        dg = dg.float()\n",
    "        r_loss = torch.mean((1 - dr) ** 2)\n",
    "        g_loss = torch.mean(dg ** 2)\n",
    "        loss += r_loss + g_loss\n",
    "        r_losses.append(r_loss.item())\n",
    "        g_losses.append(g_loss.item())\n",
    "\n",
    "    return loss, r_losses, g_losses\n",
    "\n",
    "\n",
    "def generator_loss(disc_outputs):\n",
    "    loss = 0\n",
    "    gen_losses = []\n",
    "    for dg in disc_outputs:\n",
    "        dg = dg.float()\n",
    "        l = torch.mean((1 - dg) ** 2)\n",
    "        gen_losses.append(l)\n",
    "        loss += l\n",
    "\n",
    "    return loss, gen_losses\n",
    "\n",
    "\n",
    "def kl_loss(z_p, logs_q, m_p, logs_p, z_mask):\n",
    "    \"\"\"\n",
    "    z_p, logs_q: [b, h, t_t]\n",
    "    m_p, logs_p: [b, h, t_t]\n",
    "    \"\"\"\n",
    "    z_p = z_p.float()\n",
    "    logs_q = logs_q.float()\n",
    "    m_p = m_p.float()\n",
    "    logs_p = logs_p.float()\n",
    "    z_mask = z_mask.float()\n",
    "\n",
    "    kl = logs_p - logs_q - 0.5\n",
    "    kl += 0.5 * ((z_p - m_p) ** 2) * torch.exp(-2.0 * logs_p)\n",
    "    kl = torch.sum(kl * z_mask)\n",
    "    l = kl / torch.sum(z_mask)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b34fa71",
   "metadata": {},
   "source": [
    "# VITS Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e61d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class VITSTrainer(TTSTrainer):\n",
    "    REQUIRED_HPARAMS = [\n",
    "        \"betas\",\n",
    "        \"eps\",\n",
    "        \"training_audiopaths_and_text\",\n",
    "        \"val_audiopaths_and_text\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.mel_stft = MelSTFT()\n",
    "\n",
    "    def log_training(self):\n",
    "        print(\"log training placeholder...\")\n",
    "\n",
    "    def log_validation(self):\n",
    "        pass\n",
    "\n",
    "    def warm_start(self):\n",
    "        pass\n",
    "\n",
    "    def _evaluate(self, generator, val_loader):\n",
    "        pass\n",
    "\n",
    "    def _train_and_evaluate(\n",
    "        self, epoch, nets, optims, schedulers, scaler: GradScaler, loaders\n",
    "    ):\n",
    "        net_g, net_d = nets\n",
    "        optim_g, optim_d = optims\n",
    "        scheduler_g, scheduler_d = schedulers\n",
    "        train_loader, val_loader = loaders\n",
    "        train_loader.batch_sampler.set_epoch(epoch)\n",
    "        net_g.train()\n",
    "        net_d.train()\n",
    "        for batch_idx, (\n",
    "            x,\n",
    "            x_lengths,\n",
    "            spec,\n",
    "            spec_lengths,\n",
    "            y,\n",
    "            y_lengths,\n",
    "            speakers,\n",
    "        ) in enumerate(train_loader):\n",
    "            if self.device == \"cuda\":\n",
    "                x, x_lengths = x.cuda(self.rank, non_blocking=True), x_lengths.cuda(\n",
    "                    self.rank, non_blocking=True\n",
    "                )\n",
    "                y, y_lengths = y.cuda(self.rank, non_blocking=True), y_lengths.cuda(\n",
    "                    self.rank, non_blocking=True\n",
    "                )\n",
    "                spec, spec_lengths = spec.cuda(\n",
    "                    self.rank, non_blocking=True\n",
    "                ), spec_lengths.cuda(self.rank, non_blocking=True)\n",
    "                speakers = speakers.cuda(self.rank, non_blocking=True)\n",
    "            with autocast(enabled=self.fp16_run):\n",
    "                (\n",
    "                    y_hat,\n",
    "                    l_length,\n",
    "                    attn,\n",
    "                    ids_slice,\n",
    "                    x_mask,\n",
    "                    z_mask,\n",
    "                    (z, z_p, m_p, logs_p, m_q, logs_q),\n",
    "                ) = net_g(x, x_lengths, spec, spec_lengths, speakers)\n",
    "                mel = self.mel_stft.spec_to_mel(spec)\n",
    "                y_mel = slice_segments(\n",
    "                    mel, ids_slice, self.segment_size // self.hop_length\n",
    "                )\n",
    "                y_hat_mel = self.mel_stft.mel_spectrogram(y_hat.squeeze(1))\n",
    "                y = slice_segments(y, ids_slice * self.hop_length, self.segment_size)\n",
    "\n",
    "                # Discriminator\n",
    "                y_d_hat_r, y_d_hat_g, _, _ = net_d(y, y_hat.detach())\n",
    "                with autocase(enabled=False):\n",
    "                    loss_disc, losses_disc_r, losses_disc, g = discriminator_loss(\n",
    "                        y_d_hat_r, y_d_hat_g\n",
    "                    )\n",
    "                    loss_disc_all = loss_disc\n",
    "            optim_d.zero_grad()\n",
    "            scaler.scale(loss_disc_all).backward()\n",
    "            scaler.unscale_(optim_d)\n",
    "            grad_norm_d = clip_grad_value_(net_d.parameters(), None)\n",
    "            scaler.step(optim_d)\n",
    "\n",
    "            with autocast(enabled=self.fp16_run):\n",
    "                # Generator\n",
    "                y_d_hat_r, y_d_hat_g, fmap_r, fmap_g = net_d(y, y_hat)\n",
    "                with autocast(enabled=False):\n",
    "                    loss_dur = torch.sum(l_length.float())\n",
    "                    loss_mel = F.l1_loss(y_mel, y_hat_mel) * self.c_mel\n",
    "                    loss_kl = kl_loss(z_p, logs_q, m_p, logs_p, z_mask) * self.c_kl\n",
    "\n",
    "                    loss_fm = feature_loss(fmap_r, fmap_g)\n",
    "                    loss_gen, losses_gen = generator_loss(y_d_hat_g)\n",
    "                    loss_gen_all = loss_gen + loss_fm + loss_mel + loss_dur + loss_kl\n",
    "            optim_g.zero_grad()\n",
    "            scaler.scale(loss_gen_all).backward()\n",
    "            scaler.unscale_(optim_g)\n",
    "            grad_norm_g = clip_grad_value_(enet_g.parameters(), None)\n",
    "            scaler.step(optim_g)\n",
    "            scaler.update()\n",
    "\n",
    "            self.log_training()\n",
    "        self._evaluate(net_g, val_loader)\n",
    "\n",
    "    def train(self):\n",
    "        train_dataset = TextAudioSpeakerLoader(\n",
    "            self.training_audiopaths_and_text, self.hparams\n",
    "        )\n",
    "        train_sampler = DistributedBucketSampler(\n",
    "            train_dataset,\n",
    "            self.batch_size,\n",
    "            [32, 300, 400, 500, 600, 700, 800, 900, 1000],\n",
    "            num_replicas=self.world_size,\n",
    "            rank=self.rank,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        collate_fn = TextAudioSpeakerCollate()\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            num_workers=8,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            collate_fn=collate_fn,\n",
    "            batch_sampler=train_sampler,\n",
    "        )\n",
    "        if rank == 0:\n",
    "            val_dataset = TextAudioSpeakerLoader(\n",
    "                self.val_audiopaths_and_text, self.hparams\n",
    "            )\n",
    "            val_loader = DataLoader(\n",
    "                val_dataset,\n",
    "                num_workers=8,\n",
    "                shuffle=False,\n",
    "                batch_size=self.batch_size,\n",
    "                pin_memory=True,\n",
    "                drop_last=False,\n",
    "                collate_fn=collate_fn,\n",
    "            )\n",
    "\n",
    "        model_kwargs = {k: v for k, v in DEFAULTS.values().items() if hasattr(self, k)}\n",
    "        net_g = SynthesizerTrn(\n",
    "            len(symbols_with_ipa),\n",
    "            self.filter_length // 2 + 1,\n",
    "            self.segment_size // self.hop_length,\n",
    "            n_speakers=self.n_speakers,\n",
    "            **model_kwargs,\n",
    "        )\n",
    "        net_id = MultiPeriodDiscriminator(self.use_spectral_norm)\n",
    "        if self.device == \"cuda\":\n",
    "            net_g = net_g.cuda(self.rank)\n",
    "            net_d = net_d.cuda(self.rank)\n",
    "\n",
    "        optim_g = torch.optim.AdamW(\n",
    "            net_g.parameters(),\n",
    "            self.learning_rate,\n",
    "            betas=self.betas,\n",
    "            eps=self.eps,\n",
    "        )\n",
    "        optim_d = torch.optim.AdamW(\n",
    "            net_d.parameters(), self.learning_rate, betas=self.betas, eps=self.eps\n",
    "        )\n",
    "        if self.distributed_run:\n",
    "            net_g = DDP(net_g, device_ids=[self.rank])\n",
    "            net_d = DDP(net_d, device_ids=[self.rank])\n",
    "\n",
    "        start_epoch = 0\n",
    "        if self.warm_start_name:\n",
    "            # TODO\n",
    "            pass\n",
    "\n",
    "        scheduler_g = ExponentialLR(\n",
    "            optim_g, gamma=self.lr_decay, last_epoch=start_epoch - 1\n",
    "        )\n",
    "        scheduler_d = ExponentialLR(\n",
    "            optim_d, gamma=self.lr_decay, last_epoch=start_epoch - 1\n",
    "        )\n",
    "        scaler = GradScaler(enabled=self.fp16_run)\n",
    "\n",
    "        for epoch in range(start_epoch, self.epochs):\n",
    "            self._train_and_evaluate(\n",
    "                epoch,\n",
    "                [net_g, net_d],\n",
    "                [optim_g, optim_d],\n",
    "                [scheduler_g, scheduler_d],\n",
    "                scaler,\n",
    "                [train_loader, val_loader],\n",
    "            )\n",
    "            if epoch % self.epochs_per_checkpoint == 0:\n",
    "                self.save_checkpoint(f\"vits_{epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0134d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
