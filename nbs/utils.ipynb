{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6fcc155",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28b1c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16881815",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "    \n",
    "import sys\n",
    "import os\n",
    "import soundfile as sf \n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "\n",
    "\n",
    "def load_filepaths_and_text(dataset_path: str, filename: str, split: str =\"|\"):\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        filepaths_and_text = [line.strip().split(split) for line in f]\n",
    "    return filepaths_and_text\n",
    "\n",
    "\n",
    "def synthesize_speakerids2(filelists, fix_indices_index = None):\n",
    "\n",
    "    data_dict = {}\n",
    "    data_dict_out = {}\n",
    "    for f in range(len(filelists)):\n",
    "            data = load_filepaths_and_text(filelists[f])\n",
    "            data_dict[filelists[f]] = pd.DataFrame(data)    \n",
    "        \n",
    "    source_files = list(data_dict.keys())\n",
    "    \n",
    "    speaker_offset = {}\n",
    "    nfilelist = len(filelists)\n",
    "    reserved_speakers = np.unique(data_dict[filelists[fix_indices_index]].iloc[:,2])\n",
    "    \n",
    "    for s in range(nfilelist):\n",
    "        source_file = filelists[s]\n",
    "        data = data_dict[source_file]\n",
    "        if s != fix_indices_index:\n",
    "            speakers = np.unique(data.iloc[:,2])\n",
    "            overlap = np.where(np.isin(speakers, reserved_speakers))[0]\n",
    "            reserved_speakers_temp = np.union1d(speakers, reserved_speakers)\n",
    "            newindices = np.setdiff1d(list(range(len(reserved_speakers) + len(speakers))), reserved_speakers_temp)[:len(overlap)]\n",
    "            for o in range(len(overlap)):\n",
    "                data.iloc[np.where(data.iloc[:,2] == overlap[o])[0] ,2] = newindices[o]\n",
    "\n",
    "            data_dict_out[source_file] = data\n",
    "            speakers = np.unique(data.iloc[:,2])\n",
    "            reserved_speakers = np.union1d(speakers, reserved_speakers)\n",
    "        else:\n",
    "            data_dict_out[source_file] = data\n",
    "    return(data_dict_out)  \n",
    "\n",
    "\n",
    "def parse_vctk(folder):\n",
    "    wav_dir = folder + 'wav48_silence_trimmed'\n",
    "    txt_dir = folder + 'txt'\n",
    "    speaker_wavs = os.listdir(wav_dir)\n",
    "    speaker_txts = os.listdir(txt_dir)\n",
    "    speakers = np.intersect1d(speaker_wavs, speaker_txts)\n",
    "    \n",
    "    output_dict = {}\n",
    "    #wav_dict = {}\n",
    "    #txt_dict = {}\n",
    "    #speaker_dict = {}\n",
    "    counter = 0\n",
    "    for speaker in speakers:\n",
    "        \n",
    "        speaker_wav_dir = wav_dir + '/' + speaker\n",
    "        speaker_txt_dir = txt_dir + '/' + speaker\n",
    "        wav_files_speaker = np.asarray(os.listdir(speaker_wav_dir))\n",
    "        txt_files_speaker = np.asarray(os.listdir(speaker_txt_dir))\n",
    "        #data_dict[wav_dir] = pd.DataFrame()\n",
    "\n",
    "        wav_files = np.asarray([])\n",
    "        nwavfiles= len(wav_files_speaker)\n",
    "        list1 = np.asarray([txt_files_speaker[i][:8] for i in range(len(txt_files_speaker))])\n",
    "        list2 = np.asarray([wav_files_speaker[i][:8] for i in range(nwavfiles)])\n",
    "        mic = np.asarray([wav_files_speaker[i][12]  for i in range(nwavfiles)])\n",
    "        mic1_ind = mic == '1'\n",
    "        wav_files_speaker = wav_files_speaker[mic1_ind]\n",
    "        list2 = list2[mic1_ind]\n",
    "        combined_files = np.intersect1d(list1, list2)\n",
    "        matching_inds1 = np.where(np.isin(list1 , combined_files))[0]\n",
    "        matching_inds2 = np.where(np.isin(list2 , combined_files))[0]\n",
    "        inds1 = matching_inds1[list1[matching_inds1].argsort()]\n",
    "        inds2 = matching_inds2[list2[matching_inds2].argsort()]\n",
    "        txt_files_speaker = txt_files_speaker[inds1]\n",
    "        wav_files_speaker = wav_files_speaker[inds2]\n",
    "        texts = list()\n",
    "        for g in range(len(txt_files_speaker)):\n",
    "            text_file = speaker_txt_dir + '/' + txt_files_speaker[g]\n",
    "            with open(text_file) as f:\n",
    "                contents = f.read().splitlines() \n",
    "            #print(contents)\n",
    "            texts = np.append(texts, contents)\n",
    "\n",
    "            wav_file = speaker_wav_dir + '/' + wav_files_speaker[g]\n",
    "            wav_files = np.append(wav_files, wav_file)\n",
    "\n",
    "        if wav_files.shape[0]>0:\n",
    "            output_dict[speaker] = pd.DataFrame([wav_files, texts,np.repeat(counter, wav_files.shape[0])]).transpose()\n",
    "            counter = counter +1\n",
    "            \n",
    "    output = pd.concat(list(output_dict.values()))   \n",
    "    return(output)\n",
    "\n",
    "def parse_libritts_mellotron(source_folder, mellotron_filelist):\n",
    "\n",
    "    data = pd.read_csv(mellotron_filelist, sep = \"|\",header=None, error_bad_lines=False)\n",
    "    \n",
    "    data[0] = data[0].str[17:]\n",
    "    \n",
    "    data[0] = source_folder + data[0].astype(str)\n",
    "    return(data)\n",
    "\n",
    "def load_filepaths_and_text(filename, split=\"|\"):\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        filepaths_and_text = [line.strip().split(split) for line in f]\n",
    "    return filepaths_and_text\n",
    "\n",
    "\n",
    "def flac_to_wav(input_file):\n",
    "    \n",
    "    nsamp = input_file.shape[0]\n",
    "    output_file = input_file.copy()\n",
    "    for i in range(nsamp):\n",
    "        filename = input_file.iloc[i,0]\n",
    "        print(i,filename)\n",
    "        filestart = filename[:-5]\n",
    "        print(i,filestart)\n",
    "        audio, sr = librosa.load(filename)#sf.read(filename)\n",
    "        newfile = filestart + '.wav'\n",
    "        print(i,newfile)\n",
    "        sf.write(newfile,audio,sr)\n",
    "        output_file.iloc[i,0] = newfile\n",
    "        \n",
    "    return(output_file)\n",
    "\n",
    "def add_speakerid(data, speaker_key = 0):\n",
    "\n",
    "    if data.shape[1] == 3:\n",
    "        if type(data[2]) == int:\n",
    "            pass\n",
    "        else:\n",
    "            speaker_ids = np.asarray(np.ones(data.shape[0], dtype = int) * speaker_key, dtype = int)\n",
    "            data[2] = speaker_ids            \n",
    "    if data.shape[1] == 2:\n",
    "        speaker_ids = np.asarray(np.ones(data.shape[0], dtype = int) * speaker_key, dtype = int)\n",
    "        data[2] = speaker_ids\n",
    "\n",
    "    return(data)\n",
    "\n",
    "\n",
    "def parse_libritts_mellotron(source_folder, mellotron_filelist):\n",
    "\n",
    "    data = load_filepaths_and_text(mellotron_filelist)\n",
    "    data = pd.DataFrame(data)    \n",
    "    data[0] = data[0].str[17:]\n",
    "    \n",
    "    data[0] = source_folder + data[0].astype(str)\n",
    "    return(data)\n",
    "\n",
    "    \n",
    "def parse_uberduck(source_folder):\n",
    "    \n",
    "    source_file = source_folder + '/all.txt'\n",
    "    data = load_filepaths_and_text(source_file)\n",
    "    data = pd.DataFrame(data)  \n",
    "    \n",
    "    nsamp = data.shape[0]\n",
    "    data[0] =  source_folder + '/'+data[0].astype(str)\n",
    "    output = add_speakerid(data, speaker_key = 0)\n",
    "    \n",
    "    for i in range(output.shape[0]):\n",
    "        loaded = librosa.load(output.iloc[i,0])\n",
    "        sf.write(output.iloc[i,0],loaded[0],loaded[1])\n",
    "        \n",
    "    return(output)\n",
    "\n",
    "def parse_lj7(source_folder):\n",
    "    \n",
    "    source_file = source_folder + '/metadata.csv'\n",
    "    data = load_filepaths_and_text(source_file)\n",
    "    data = pd.DataFrame(data)  \n",
    "    nsamp = data.shape[0]\n",
    "    \n",
    "    data[0] = source_folder + '/wavs/' + data[0].astype(str)\n",
    "    output = add_speakerid(data, speaker_key = 0)\n",
    "    for i in range(output.shape[0]):\n",
    "        output.iloc[i,0] = output.iloc[i,0] + '.wav'\n",
    "        \n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f470161",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.signal import get_window\n",
    "import librosa.util as librosa_util\n",
    "\n",
    "\n",
    "def window_sumsquare(window, n_frames, hop_length=200, win_length=800,\n",
    "                     n_fft=800, dtype=np.float32, norm=None):\n",
    "    \"\"\"\n",
    "    # from librosa 0.6\n",
    "    Compute the sum-square envelope of a window function at a given hop length.\n",
    "\n",
    "    This is used to estimate modulation effects induced by windowing\n",
    "    observations in short-time fourier transforms.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    window : string, tuple, number, callable, or list-like\n",
    "        Window specification, as in `get_window`\n",
    "\n",
    "    n_frames : int > 0\n",
    "        The number of analysis frames\n",
    "\n",
    "    hop_length : int > 0\n",
    "        The number of samples to advance between frames\n",
    "\n",
    "    win_length : [optional]\n",
    "        The length of the window function.  By default, this matches `n_fft`.\n",
    "\n",
    "    n_fft : int > 0\n",
    "        The length of each analysis frame.\n",
    "\n",
    "    dtype : np.dtype\n",
    "        The data type of the output\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    wss : np.ndarray, shape=`(n_fft + hop_length * (n_frames - 1))`\n",
    "        The sum-squared envelope of the window function\n",
    "    \"\"\"\n",
    "    if win_length is None:\n",
    "        win_length = n_fft\n",
    "\n",
    "    n = n_fft + hop_length * (n_frames - 1)\n",
    "    x = np.zeros(n, dtype=dtype)\n",
    "\n",
    "    # Compute the squared window at the desired length\n",
    "    win_sq = get_window(window, win_length, fftbins=True)\n",
    "    win_sq = librosa_util.normalize(win_sq, norm=norm)**2\n",
    "    win_sq = librosa_util.pad_center(win_sq, n_fft)\n",
    "\n",
    "    # Fill the envelope\n",
    "    for i in range(n_frames):\n",
    "        sample = i * hop_length\n",
    "        x[sample:min(n, sample + n_fft)] += win_sq[:max(0, min(n_fft, n - sample))]\n",
    "    return x\n",
    "\n",
    "\n",
    "def griffin_lim(magnitudes, stft_fn, n_iters=30):\n",
    "    \"\"\"\n",
    "    PARAMS\n",
    "    ------\n",
    "    magnitudes: spectrogram magnitudes\n",
    "    stft_fn: STFT class with transform (STFT) and inverse (ISTFT) methods\n",
    "    \"\"\"\n",
    "\n",
    "    angles = np.angle(np.exp(2j * np.pi * np.random.rand(*magnitudes.size())))\n",
    "    angles = angles.astype(np.float32)\n",
    "    angles = torch.autograd.Variable(torch.from_numpy(angles))\n",
    "    signal = stft_fn.inverse(magnitudes, angles).squeeze(1)\n",
    "\n",
    "    for i in range(n_iters):\n",
    "        _, angles = stft_fn.transform(signal)\n",
    "        signal = stft_fn.inverse(magnitudes, angles).squeeze(1)\n",
    "    return signal\n",
    "\n",
    "\n",
    "def dynamic_range_compression(x, C=1, clip_val=1e-5):\n",
    "    \"\"\"\n",
    "    PARAMS\n",
    "    ------\n",
    "    C: compression factor\n",
    "    \"\"\"\n",
    "    return torch.log(torch.clamp(x, min=clip_val) * C)\n",
    "\n",
    "\n",
    "def dynamic_range_decompression(x, C=1):\n",
    "    \"\"\"\n",
    "    PARAMS\n",
    "    ------\n",
    "    C: compression factor used to compress\n",
    "    \"\"\"\n",
    "    return torch.exp(x) / C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1744f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def to_gpu(x):\n",
    "    x = x.contiguous()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda(non_blocking=True)\n",
    "    return torch.autograd.Variable(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afec67d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2539,  0.9459,  1.0853,  0.4250, -0.9583, -0.2783, -0.5885, -0.0074,\n",
       "          1.4092,  0.6315],\n",
       "        [ 1.3164, -0.8408, -0.7321, -0.1941,  0.6928, -0.6122, -0.1003, -1.5531,\n",
       "          1.0428,  0.2837],\n",
       "        [-0.3450,  0.6284,  0.7693, -1.0759,  2.3381,  1.6420, -0.8584,  1.3954,\n",
       "         -0.3278,  0.8979],\n",
       "        [ 0.2296, -0.2814, -0.7780,  0.3238, -0.5444,  0.2637,  1.2909, -0.0611,\n",
       "          0.0076, -0.1130],\n",
       "        [-0.1073,  0.3603,  0.0726, -0.0629, -1.0747,  1.7711, -0.4577,  0.5372,\n",
       "         -0.2569, -0.2937],\n",
       "        [-1.3738, -0.5428,  0.2975,  0.0234, -1.7203,  0.8453,  1.4796,  0.4519,\n",
       "          0.8104,  1.4779],\n",
       "        [-1.8680,  1.1598, -0.8081,  0.8324, -0.3575, -0.6670, -1.3276,  0.4503,\n",
       "         -0.1513, -1.3132],\n",
       "        [-0.3910,  1.1334, -0.6742,  0.7681, -0.2728, -1.8644,  0.4343,  1.2008,\n",
       "         -0.8278,  1.4906],\n",
       "        [-0.0293, -0.6808, -0.3770, -0.1554, -0.3942, -0.0421,  0.4135, -0.8548,\n",
       "          0.2775,  0.6327],\n",
       "        [ 1.6418, -0.1465,  2.2453, -0.4917, -0.4610, -0.4835,  0.4940, -0.1196,\n",
       "         -2.0548,  0.2415]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_gpu(torch.randn(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccf96db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def get_mask_from_lengths(lengths):\n",
    "    \"\"\"Return a mask matrix. Unmasked entires are true.\"\"\"\n",
    "    max_len = torch.max(lengths).item()\n",
    "    tensor_cls = torch.cuda.LongTensor if torch.cuda.is_available() else torch.LongTensor\n",
    "    ids = torch.arange(0, max_len, out=tensor_cls(max_len))\n",
    "    mask = (ids < lengths.unsqueeze(1)).bool()\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af55f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2])\n",
      "tensor([1, 3, 2, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False],\n",
       "        [ True,  True,  True],\n",
       "        [ True,  True, False],\n",
       "        [ True, False, False]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mask_from_lengths(torch.LongTensor([1, 3, 2, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b11fc09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
