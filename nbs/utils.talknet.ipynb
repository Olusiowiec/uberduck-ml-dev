{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd51773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp utils.talknet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eed61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import os\n",
    "import crepe\n",
    "\n",
    "\n",
    "def preprocess_tokens(tokens, blank):\n",
    "    new_tokens = [blank]\n",
    "    for c in tokens:\n",
    "        new_tokens.extend([c, blank])\n",
    "    tokens = new_tokens\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def forward_extractor(tokens, log_probs, blank):\n",
    "    \"\"\"Computes states f and p.\"\"\"\n",
    "    n, m = len(tokens), log_probs.shape[0]\n",
    "    # `f[s, t]` -- max sum of log probs for `s` first codes\n",
    "    # with `t` first timesteps with ending in `tokens[s]`.\n",
    "    f = np.empty((n + 1, m + 1), dtype=float)\n",
    "    f.fill(-(10 ** 9))\n",
    "    p = np.empty((n + 1, m + 1), dtype=int)\n",
    "    f[0, 0] = 0.0  # Start\n",
    "    for s in range(1, n + 1):\n",
    "        c = tokens[s - 1]\n",
    "        for t in range((s + 1) // 2, m + 1):\n",
    "            f[s, t] = log_probs[t - 1, c]\n",
    "            # Option #1: prev char is equal to current one.\n",
    "            if s == 1 or c == blank or c == tokens[s - 3]:\n",
    "                options = f[s : (s - 2 if s > 1 else None) : -1, t - 1]\n",
    "            else:  # Is not equal to current one.\n",
    "                options = f[s : (s - 3 if s > 2 else None) : -1, t - 1]\n",
    "            f[s, t] += np.max(options)\n",
    "            p[s, t] = np.argmax(options)\n",
    "    return f, p\n",
    "\n",
    "\n",
    "def backward_extractor(f, p):\n",
    "    \"\"\"Computes durs from f and p.\"\"\"\n",
    "    n, m = f.shape\n",
    "    n -= 1\n",
    "    m -= 1\n",
    "    durs = np.zeros(n, dtype=int)\n",
    "    if f[-1, -1] >= f[-2, -1]:\n",
    "        s, t = n, m\n",
    "    else:\n",
    "        s, t = n - 1, m\n",
    "    while s > 0:\n",
    "        durs[s - 1] += 1\n",
    "        s -= p[s, t]\n",
    "        t -= 1\n",
    "    assert durs.shape[0] == n\n",
    "    assert np.sum(durs) == m\n",
    "    assert np.all(durs[1::2] > 0)\n",
    "    return durs\n",
    "\n",
    "\n",
    "def crepe_f0(audio_file, hop_length=256):\n",
    "    sr, audio = wavfile.read(audio_file)\n",
    "    audio_x = np.arange(0, len(audio)) / 22050.0\n",
    "    time, frequency, confidence, activation = crepe.predict(audio, sr, viterbi=True)\n",
    "\n",
    "    x = np.arange(0, len(audio), hop_length) / 22050.0\n",
    "    freq_interp = np.interp(x, time, frequency)\n",
    "    conf_interp = np.interp(x, time, confidence)\n",
    "    audio_interp = np.interp(x, audio_x, np.absolute(audio)) / 32768.0\n",
    "    weights = [0.5, 0.25, 0.25]\n",
    "    audio_smooth = np.convolve(audio_interp, np.array(weights)[::-1], \"same\")\n",
    "\n",
    "    conf_threshold = 0.25\n",
    "    audio_threshold = 0.0005\n",
    "    for i in range(len(freq_interp)):\n",
    "        if conf_interp[i] < conf_threshold:\n",
    "            freq_interp[i] = 0.0\n",
    "        if audio_smooth[i] < audio_threshold:\n",
    "            freq_interp[i] = 0.0\n",
    "\n",
    "    # Hack to make f0 and mel lengths equal\n",
    "    if len(audio) % hop_length == 0:\n",
    "        freq_interp = np.pad(freq_interp, pad_width=[0, 1])\n",
    "    return torch.from_numpy(freq_interp.astype(np.float32))\n",
    "\n",
    "\n",
    "def get_durs(dl, asr_model, output_dir):\n",
    "    blank_id = asr_model.decoder.num_classes_with_blank - 1\n",
    "    dur_data = {}\n",
    "    for sample_idx, test_sample in tqdm(enumerate(dl), total=len(dl)):\n",
    "        # print('sample_idx', sample_idx)\n",
    "        # pdb.set_trace()\n",
    "        # print(sample_idx)\n",
    "        log_probs, _, greedy_predictions = asr_model(\n",
    "            input_signal=test_sample[0], input_signal_length=test_sample[1]\n",
    "        )\n",
    "\n",
    "        log_probs = log_probs[0].cpu().detach().numpy()\n",
    "        seq_ids = test_sample[2][0].cpu().detach().numpy()\n",
    "\n",
    "        target_tokens = preprocess_tokens(seq_ids, blank_id)\n",
    "\n",
    "        f, p = forward_extractor(target_tokens, log_probs, blank_id)\n",
    "        durs = backward_extractor(f, p)\n",
    "\n",
    "        dur_key = Path(dl.dataset.collection[sample_idx].audio_file).stem\n",
    "        dur_data[dur_key] = {\n",
    "            \"blanks\": torch.tensor(durs[::2], dtype=torch.long).cpu().detach(),\n",
    "            \"tokens\": torch.tensor(durs[1::2], dtype=torch.long).cpu().detach(),\n",
    "        }\n",
    "\n",
    "        del test_sample\n",
    "\n",
    "    torch.save(dur_data, os.path.join(output_dir, \"durations.pt\"))\n",
    "\n",
    "\n",
    "# Extract F0 (pitch)\n",
    "def get_f0(data_config):\n",
    "    f0_data = {}\n",
    "    with open(str(data_config[\"manifest_filepath\"])) as f:\n",
    "        for i, l in enumerate(f.readlines()):\n",
    "            # print(str(i))\n",
    "            audio_path = json.loads(l)[\"audio_filepath\"]\n",
    "            f0_data[Path(audio_path).stem] = crepe_f0(audio_path)\n",
    "    return f0_data\n",
    "\n",
    "\n",
    "def talknet_predict_mel(model, texts, train_ids, f0s, durs):\n",
    "\n",
    "    for i in tqdm(range(len(texts))):\n",
    "        x_name = os.path.splitext(os.path.basename(str(train_ids[i]).strip()))[0]\n",
    "        x_tokens = model.parse(text=texts[i])\n",
    "        x_durs = (\n",
    "            torch.stack(\n",
    "                (\n",
    "                    durs[x_name][\"blanks\"],\n",
    "                    torch.cat((durs[x_name][\"tokens\"], torch.zeros(1).int())),\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "            .view(-1)[:-1]\n",
    "            .view(1, -1)\n",
    "            .to(\"cuda:0\")\n",
    "        )\n",
    "        x_f0s = f0s[x_name].view(1, -1).to(\"cuda:0\")\n",
    "        x_spect = model.force_spectrogram(tokens=x_tokens, durs=x_durs, f0=x_f0s)\n",
    "        # print(x_name + \".npy\")\n",
    "        np.save(str(train_ids[i])[:-4] + \".npy\", x_spect.detach().cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
