{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1503813",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#TextMelDataset\" data-toc-modified-id=\"TextMelDataset-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>TextMelDataset</a></span></li><li><span><a href=\"#TextMelCollate\" data-toc-modified-id=\"TextMelCollate-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>TextMelCollate</a></span></li><li><span><a href=\"#TextAudioLoader\" data-toc-modified-id=\"TextAudioLoader-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>TextAudioLoader</a></span></li><li><span><a href=\"#TextAudioCollate\" data-toc-modified-id=\"TextAudioCollate-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>TextAudioCollate</a></span></li><li><span><a href=\"#DistributedBucketSampler\" data-toc-modified-id=\"DistributedBucketSampler-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>DistributedBucketSampler</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5161412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db625c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import read\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from uberduck_ml_dev.models.common import STFT, MelSTFT\n",
    "from uberduck_ml_dev.text.symbols import DEFAULT_SYMBOLS, IPA_SYMBOLS\n",
    "from uberduck_ml_dev.text.util import cleaned_text_to_sequence, text_to_sequence\n",
    "from uberduck_ml_dev.utils.audio import compute_yin, load_wav_to_torch\n",
    "from uberduck_ml_dev.utils.utils import load_filepaths_and_text, intersperse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a9d9e8",
   "metadata": {},
   "source": [
    "# TextMelDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0bcafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def _orig_to_dense_speaker_id(speaker_ids):\n",
    "    speaker_ids = sorted(list(set(speaker_ids)))\n",
    "    return {orig: idx for orig, idx in zip(speaker_ids, range(len(speaker_ids)))}\n",
    "\n",
    "\n",
    "class TextMelDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        audiopaths_and_text: str,\n",
    "        text_cleaners: List[str],\n",
    "        p_arpabet: float,\n",
    "        n_mel_channels: int,\n",
    "        sample_rate: int,\n",
    "        mel_fmin: float,\n",
    "        mel_fmax: float,\n",
    "        filter_length: int,\n",
    "        hop_length: int,\n",
    "        win_length: int,\n",
    "        max_wav_value: float = 32768.0,\n",
    "        include_f0: bool = False,\n",
    "        pos_weight: float = 10,\n",
    "        f0_min: int = 80,\n",
    "        f0_max: int = 880,\n",
    "        harmonic_thresh=0.25,\n",
    "        debug: bool = False,\n",
    "        debug_dataset_size: int = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        path = audiopaths_and_text\n",
    "        self.audiopaths_and_text = load_filepaths_and_text(path)\n",
    "        self.text_cleaners = text_cleaners\n",
    "        self.p_arpabet = p_arpabet\n",
    "        self.stft = MelSTFT(\n",
    "            filter_length=filter_length,\n",
    "            hop_length=hop_length,\n",
    "            win_length=win_length,\n",
    "            n_mel_channels=n_mel_channels,\n",
    "            sampling_rate=sample_rate,\n",
    "            mel_fmin=mel_fmin,\n",
    "            mel_fmax=mel_fmax,\n",
    "        )\n",
    "        self.max_wav_value = max_wav_value\n",
    "        self.sample_rate = sample_rate\n",
    "        self.filter_length = filter_length\n",
    "        self.hop_length = hop_length\n",
    "        self.mel_fmin = mel_fmin\n",
    "        self.mel_fmax = mel_fmax\n",
    "        self.include_f0 = include_f0\n",
    "        self.f0_min = f0_min\n",
    "        self.f0_max = f0_max\n",
    "        self.harmonic_threshold = harmonic_thresh\n",
    "        # speaker id lookup table\n",
    "        speaker_ids = [i[2] for i in self.audiopaths_and_text]\n",
    "        self._speaker_id_map = _orig_to_dense_speaker_id(speaker_ids)\n",
    "        self.debug = debug\n",
    "        self.debug_dataset_size = debug_dataset_size\n",
    "\n",
    "    def _get_f0(self, audio):\n",
    "        f0, harmonic_rates, argmins, times = compute_yin(\n",
    "            audio,\n",
    "            self.sample_rate,\n",
    "            self.filter_length,\n",
    "            self.hop_length,\n",
    "            self.f0_min,\n",
    "            self.f0_max,\n",
    "            self.harmonic_threshold,\n",
    "        )\n",
    "        pad = int((self.filter_length / self.hop_length) / 2)\n",
    "        f0 = [0.0] * pad + f0 + [0.0] * pad\n",
    "        f0 = np.array(f0, dtype=np.float32)\n",
    "        return f0\n",
    "\n",
    "    def _get_data(self, audiopath_and_text):\n",
    "        path, transcription, speaker_id = audiopath_and_text\n",
    "        speaker_id = self._speaker_id_map[speaker_id]\n",
    "        sample_rate, wav_data = read(path)\n",
    "        text_sequence = torch.LongTensor(\n",
    "            text_to_sequence(\n",
    "                transcription, self.text_cleaners, p_arpabet=self.p_arpabet\n",
    "            )\n",
    "        )\n",
    "        audio = torch.FloatTensor(wav_data)\n",
    "        audio_norm = audio / self.max_wav_value\n",
    "        audio_norm = audio_norm.unsqueeze(0)\n",
    "        melspec = self.stft.mel_spectrogram(audio_norm)\n",
    "        melspec = torch.squeeze(melspec, 0)\n",
    "        if not self.include_f0:\n",
    "            return (text_sequence, melspec, speaker_id)\n",
    "        f0 = self._get_f0(audio.data.cpu().numpy())\n",
    "        f0 = torch.from_numpy(f0)[None]\n",
    "        f0 = f0[:, : melspec.size(1)]\n",
    "\n",
    "        return (text_sequence, melspec, speaker_id, f0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return data for a single audio file + transcription.\"\"\"\n",
    "        try:\n",
    "            data = self._get_data(self.audiopaths_and_text[idx])\n",
    "        except Exception:\n",
    "            print(f\"Error while getting data: {self.audiopaths_and_text[idx]}\")\n",
    "            raise\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.debug and self.debug_dataset_size:\n",
    "            return min(self.debug_dataset_size, len(self.audiopaths_and_text))\n",
    "        return len(self.audiopaths_and_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6cde58",
   "metadata": {},
   "source": [
    "# TextMelCollate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4c8a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class TextMelCollate:\n",
    "    def __init__(self, n_frames_per_step: int = 1, include_f0: bool = False):\n",
    "        self.n_frames_per_step = n_frames_per_step\n",
    "        self.include_f0 = include_f0\n",
    "\n",
    "    def set_frames_per_step(self, n_frames_per_step):\n",
    "        \"\"\"Set n_frames_step.\n",
    "\n",
    "        This is used to train with gradual training, where we start with a large\n",
    "        n_frames_per_step in order to learn attention quickly and decrease it\n",
    "        over the course of training in order to increase accuracy. Gradual training\n",
    "        reference:\n",
    "        https://erogol.com/gradual-training-with-tacotron-for-faster-convergence/\n",
    "        \"\"\"\n",
    "        self.n_frames_per_step = n_frames_per_step\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        \"\"\"Collate's training batch from normalized text and mel-spectrogram\n",
    "        PARAMS\n",
    "        ------\n",
    "        batch: [text_normalized, mel_normalized, speaker_id]\n",
    "        \"\"\"\n",
    "        # Right zero-pad all one-hot text sequences to max input length\n",
    "        input_lengths, ids_sorted_decreasing = torch.sort(\n",
    "            torch.LongTensor([len(x[0]) for x in batch]), dim=0, descending=True\n",
    "        )\n",
    "        max_input_len = input_lengths[0]\n",
    "\n",
    "        text_padded = torch.LongTensor(len(batch), max_input_len)\n",
    "        text_padded.zero_()\n",
    "        for i in range(len(ids_sorted_decreasing)):\n",
    "            text = batch[ids_sorted_decreasing[i]][0]\n",
    "            text_padded[i, : text.size(0)] = text\n",
    "\n",
    "        # Right zero-pad mel-spec\n",
    "        num_mels = batch[0][1].size(0)\n",
    "        max_target_len = max([x[1].size(1) for x in batch])\n",
    "        if max_target_len % self.n_frames_per_step != 0:\n",
    "            max_target_len += (\n",
    "                self.n_frames_per_step - max_target_len % self.n_frames_per_step\n",
    "            )\n",
    "            assert max_target_len % self.n_frames_per_step == 0\n",
    "\n",
    "        # include mel padded, gate padded and speaker ids\n",
    "        mel_padded = torch.FloatTensor(len(batch), num_mels, max_target_len)\n",
    "        mel_padded.zero_()\n",
    "        gate_padded = torch.FloatTensor(len(batch), max_target_len)\n",
    "        gate_padded.zero_()\n",
    "        output_lengths = torch.LongTensor(len(batch))\n",
    "        speaker_ids = torch.LongTensor(len(batch))\n",
    "        if self.include_f0:\n",
    "            f0_padded = torch.FloatTensor(len(batch), 1, max_target_len)\n",
    "            f0_padded.zero_()\n",
    "\n",
    "        for i in range(len(ids_sorted_decreasing)):\n",
    "            mel = batch[ids_sorted_decreasing[i]][1]\n",
    "            mel_padded[i, :, : mel.size(1)] = mel\n",
    "            gate_padded[i, mel.size(1) - 1 :] = 1\n",
    "            output_lengths[i] = mel.size(1)\n",
    "            speaker_ids[i] = batch[ids_sorted_decreasing[i]][2]\n",
    "            if self.include_f0:\n",
    "                f0 = batch[ids_sorted_decreasing[i]][3]\n",
    "                f0_padded[i, :, : f0.size(1)] = f0\n",
    "\n",
    "        # NOTE(zach): would model_inputs be better as a namedtuple or dataclass?\n",
    "        if self.include_f0:\n",
    "            model_inputs = (\n",
    "                text_padded,\n",
    "                input_lengths,\n",
    "                mel_padded,\n",
    "                gate_padded,\n",
    "                output_lengths,\n",
    "                speaker_ids,\n",
    "                f0_padded,\n",
    "            )\n",
    "        else:\n",
    "            model_inputs = (\n",
    "                text_padded,\n",
    "                input_lengths,\n",
    "                mel_padded,\n",
    "                gate_padded,\n",
    "                output_lengths,\n",
    "                speaker_ids,\n",
    "            )\n",
    "\n",
    "        return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed89f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 0, 3: 1, 4: 2, 9: 3}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_orig_to_dense_speaker_id([4, 2, 9, 3, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b89cc08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = TextMelDataset(\n",
    "    \"test/fixtures/val.txt\",\n",
    "    [\"english_cleaners\"],\n",
    "    0.0,\n",
    "    80,\n",
    "    22050,\n",
    "    0,\n",
    "    8000,\n",
    "    1024,\n",
    "    256,\n",
    "    1024,\n",
    "    debug=True,\n",
    "    debug_dataset_size=12,\n",
    ")\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16563fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zwf/miniconda3/envs/uberduck/lib/python3.6/site-packages/ipykernel_launcher.py:85: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:143.)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "collate_fn = TextMelCollate()\n",
    "dl = DataLoader(ds, 12, collate_fn=collate_fn)\n",
    "for i, batch in enumerate(dl):\n",
    "    assert len(batch) == 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1d74b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = TextMelDataset(\n",
    "    \"test/fixtures/val.txt\",\n",
    "    [\"english_cleaners\"],\n",
    "    0.0,\n",
    "    80,\n",
    "    22050,\n",
    "    0,\n",
    "    8000,\n",
    "    1024,\n",
    "    256,\n",
    "    1024,\n",
    "    debug=True,\n",
    "    debug_dataset_size=12,\n",
    "    include_f0=True,\n",
    ")\n",
    "assert len(ds) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad10c68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = TextMelCollate(include_f0=True)\n",
    "dl = DataLoader(ds, 12, collate_fn=collate_fn)\n",
    "for i, batch in enumerate(dl):\n",
    "    # text_padded,\n",
    "    # input_lengths,\n",
    "    # mel_padded,\n",
    "    # gate_padded,\n",
    "    # output_lengths,\n",
    "    # speaker_ids,\n",
    "    (\n",
    "        text_padded,\n",
    "        input_lengths,\n",
    "        mel_padded,\n",
    "        gate_padded,\n",
    "        output_lengths,\n",
    "        speaker_ids,\n",
    "        *_,\n",
    "    ) = batch\n",
    "    assert output_lengths.item() == 566, print(\"output lengths: \", output_lengths)\n",
    "    assert gate_padded.size(1) == 566\n",
    "    assert mel_padded.size(2) == 566\n",
    "    assert len(batch) == 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae303fcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 80, 566])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3cfd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing n_frames_per_step > 1\n",
    "ds = TextMelDataset(\n",
    "    \"test/fixtures/val.txt\",\n",
    "    [\"english_cleaners\"],\n",
    "    0.0,\n",
    "    80,\n",
    "    22050,\n",
    "    0,\n",
    "    8000,\n",
    "    1024,\n",
    "    256,\n",
    "    1024,\n",
    "    debug=True,\n",
    "    debug_dataset_size=12,\n",
    "    include_f0=True,\n",
    ")\n",
    "assert len(ds) == 1\n",
    "collate_fn = TextMelCollate(n_frames_per_step=5, include_f0=True)\n",
    "dl = DataLoader(ds, 12, collate_fn=collate_fn)\n",
    "# text_padded,\n",
    "# input_lengths,\n",
    "# mel_padded,\n",
    "# gate_padded,\n",
    "# output_lengths,\n",
    "# speaker_ids,\n",
    "for i, batch in enumerate(dl):\n",
    "    (\n",
    "        text_padded,\n",
    "        input_lengths,\n",
    "        mel_padded,\n",
    "        gate_padded,\n",
    "        output_lengths,\n",
    "        speaker_ids,\n",
    "        *_,\n",
    "    ) = batch\n",
    "    assert output_lengths.item() == 566, output_lengths.item()\n",
    "    assert mel_padded.size(2) == 570, print(\"actual shape: \", mel_padded.shape)\n",
    "    assert gate_padded.size(1) == 570, print(\"actual shape: \", gate_padded.shape)\n",
    "    assert len(batch) == 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bfd167",
   "metadata": {},
   "source": [
    "# TextAudioLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991f08ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class TextAudioSpeakerLoader(Dataset):\n",
    "    \"\"\"\n",
    "    1) loads audio, speaker_id, text pairs\n",
    "    2) normalizes text and converts them to sequences of integers\n",
    "    3) computes spectrograms from audio files.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, audiopaths_sid_text, hparams, debug=False, debug_dataset_size=None\n",
    "    ):\n",
    "        self.audiopaths_sid_text = load_filepaths_and_text(audiopaths_sid_text)\n",
    "        self.text_cleaners = hparams.text_cleaners\n",
    "        self.max_wav_value = hparams.max_wav_value\n",
    "        self.sampling_rate = hparams.sampling_rate\n",
    "        self.filter_length = hparams.filter_length\n",
    "        self.hop_length = hparams.hop_length\n",
    "        self.win_length = hparams.win_length\n",
    "        self.sampling_rate = hparams.sampling_rate\n",
    "\n",
    "        self.debug = debug\n",
    "        self.debug_dataset_size = debug_dataset_size\n",
    "\n",
    "        self.stft = MelSTFT(\n",
    "            filter_length=self.filter_length,\n",
    "            hop_length=self.hop_length,\n",
    "            win_length=self.win_length,\n",
    "            n_mel_channels=hparams.n_mel_channels,\n",
    "            sampling_rate=hparams.sampling_rate,\n",
    "            mel_fmin=hparams.mel_fmin,\n",
    "            mel_fmax=hparams.mel_fmax,\n",
    "        )\n",
    "\n",
    "        self.cleaned_text = getattr(hparams, \"cleaned_text\", False)\n",
    "        # NOTE(zach): Parametrize this later if desired.\n",
    "        self.symbol_set = IPA_SYMBOLS\n",
    "\n",
    "        self.add_blank = hparams.add_blank\n",
    "        self.min_text_len = getattr(hparams, \"min_text_len\", 1)\n",
    "        self.max_text_len = getattr(hparams, \"max_text_len\", 190)\n",
    "\n",
    "        random.seed(1234)\n",
    "        random.shuffle(self.audiopaths_sid_text)\n",
    "        self._filter()\n",
    "\n",
    "    def _filter(self):\n",
    "        \"\"\"\n",
    "        Filter text & store spec lengths\n",
    "        \"\"\"\n",
    "        # Store spectrogram lengths for Bucketing\n",
    "        # wav_length ~= file_size / (wav_channels * Bytes per dim) = file_size / (1 * 2)\n",
    "        # spec_length = wav_length // hop_length\n",
    "\n",
    "        audiopaths_sid_text_new = []\n",
    "        lengths = []\n",
    "        for audiopath, sid, text in self.audiopaths_sid_text:\n",
    "            if self.min_text_len <= len(text) and len(text) <= self.max_text_len:\n",
    "                audiopaths_sid_text_new.append([audiopath, sid, text])\n",
    "                lengths.append(os.path.getsize(audiopath) // (2 * self.hop_length))\n",
    "        self.audiopaths_sid_text = audiopaths_sid_text_new\n",
    "        self.lengths = lengths\n",
    "\n",
    "    def get_audio_text_speaker_pair(self, audiopath_sid_text):\n",
    "        # separate filename, speaker_id and text\n",
    "        audiopath, text, sid = (\n",
    "            audiopath_sid_text[0],\n",
    "            audiopath_sid_text[1],\n",
    "            audiopath_sid_text[2],\n",
    "        )\n",
    "        text = self.get_text(text)\n",
    "        spec, wav = self.get_audio(audiopath)\n",
    "        sid = self.get_sid(sid)\n",
    "        return (text, spec, wav, sid)\n",
    "\n",
    "    def get_audio(self, filename):\n",
    "        audio, sampling_rate = load_wav_to_torch(filename)\n",
    "        if sampling_rate != self.sampling_rate:\n",
    "            raise ValueError(\n",
    "                \"{} {} SR doesn't match target {} SR\".format(\n",
    "                    sampling_rate, self.sampling_rate\n",
    "                )\n",
    "            )\n",
    "\n",
    "        audio_norm = audio / self.max_wav_value\n",
    "        audio_norm = audio_norm.unsqueeze(0)\n",
    "        spec_filename = filename.replace(\".wav\", \".uberduck.spec.pt\")\n",
    "        if os.path.exists(spec_filename):\n",
    "            spec = torch.load(spec_filename)\n",
    "        else:\n",
    "            spec = self.stft.spectrogram(audio_norm)\n",
    "            spec = torch.squeeze(spec, 0)\n",
    "            torch.save(spec, spec_filename)\n",
    "        return spec, audio_norm\n",
    "\n",
    "    def get_text(self, text):\n",
    "        if self.cleaned_text:\n",
    "            text_norm = cleaned_text_to_sequence(text, symbol_set=self.symbol_set)\n",
    "        else:\n",
    "            text_norm = text_to_sequence(\n",
    "                text, self.text_cleaners, symbol_set=self.symbol_set\n",
    "            )\n",
    "        if self.add_blank:\n",
    "            text_norm = intersperse(text_norm, 0)\n",
    "        text_norm = torch.LongTensor(text_norm)\n",
    "        return text_norm\n",
    "\n",
    "    def get_sid(self, sid):\n",
    "        sid = torch.LongTensor([int(sid)])\n",
    "        return sid\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.get_audio_text_speaker_pair(self.audiopaths_sid_text[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.debug and self.debug_dataset_size:\n",
    "            return min(self.debug_dataset_size, len(self.audiopaths_sid_text))\n",
    "        else:\n",
    "            return len(self.audiopaths_sid_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94ac590",
   "metadata": {},
   "source": [
    "# TextAudioCollate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3f8eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class TextAudioSpeakerCollate:\n",
    "    \"\"\"Zero-pads model inputs and targets\"\"\"\n",
    "\n",
    "    def __init__(self, return_ids=False):\n",
    "        self.return_ids = return_ids\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        \"\"\"Collate's training batch from normalized text, audio and speaker identities\n",
    "        PARAMS\n",
    "        ------\n",
    "        batch: [text_normalized, spec_normalized, wav_normalized, sid]\n",
    "        \"\"\"\n",
    "        # Right zero-pad all one-hot text sequences to max input length\n",
    "        _, ids_sorted_decreasing = torch.sort(\n",
    "            torch.LongTensor([x[1].size(1) for x in batch]), dim=0, descending=True\n",
    "        )\n",
    "\n",
    "        max_text_len = max([len(x[0]) for x in batch])\n",
    "        max_spec_len = max([x[1].size(1) for x in batch])\n",
    "        max_wav_len = max([x[2].size(1) for x in batch])\n",
    "\n",
    "        text_lengths = torch.LongTensor(len(batch))\n",
    "        spec_lengths = torch.LongTensor(len(batch))\n",
    "        wav_lengths = torch.LongTensor(len(batch))\n",
    "        sid = torch.LongTensor(len(batch))\n",
    "\n",
    "        text_padded = torch.LongTensor(len(batch), max_text_len)\n",
    "        spec_padded = torch.FloatTensor(len(batch), batch[0][1].size(0), max_spec_len)\n",
    "        wav_padded = torch.FloatTensor(len(batch), 1, max_wav_len)\n",
    "        text_padded.zero_()\n",
    "        spec_padded.zero_()\n",
    "        wav_padded.zero_()\n",
    "        for i in range(len(ids_sorted_decreasing)):\n",
    "            row = batch[ids_sorted_decreasing[i]]\n",
    "\n",
    "            text = row[0]\n",
    "            text_padded[i, : text.size(0)] = text\n",
    "            text_lengths[i] = text.size(0)\n",
    "\n",
    "            spec = row[1]\n",
    "            spec_padded[i, :, : spec.size(1)] = spec\n",
    "            spec_lengths[i] = spec.size(1)\n",
    "\n",
    "            wav = row[2]\n",
    "            wav_padded[i, :, : wav.size(1)] = wav\n",
    "            wav_lengths[i] = wav.size(1)\n",
    "\n",
    "            sid[i] = row[3]\n",
    "\n",
    "        if self.return_ids:\n",
    "            return (\n",
    "                text_padded,\n",
    "                text_lengths,\n",
    "                spec_padded,\n",
    "                spec_lengths,\n",
    "                wav_padded,\n",
    "                wav_lengths,\n",
    "                sid,\n",
    "                ids_sorted_decreasing,\n",
    "            )\n",
    "        return (\n",
    "            text_padded,\n",
    "            text_lengths,\n",
    "            spec_padded,\n",
    "            spec_lengths,\n",
    "            wav_padded,\n",
    "            wav_lengths,\n",
    "            sid,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ad232f",
   "metadata": {},
   "source": [
    "# DistributedBucketSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53b7ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class DistributedBucketSampler(DistributedSampler):\n",
    "    \"\"\"\n",
    "    Maintain similar input lengths in a batch.\n",
    "    Length groups are specified by boundaries.\n",
    "    Ex) boundaries = [b1, b2, b3] -> any batch is included either {x | b1 < length(x) <=b2} or {x | b2 < length(x) <= b3}.\n",
    "\n",
    "    It removes samples which are not included in the boundaries.\n",
    "    Ex) boundaries = [b1, b2, b3] -> any x s.t. length(x) <= b1 or length(x) > b3 are discarded.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size,\n",
    "        boundaries,\n",
    "        num_replicas=None,\n",
    "        rank=None,\n",
    "        shuffle=True,\n",
    "    ):\n",
    "        super().__init__(dataset, num_replicas=num_replicas, rank=rank, shuffle=shuffle)\n",
    "        self.lengths = dataset.lengths\n",
    "        self.batch_size = batch_size\n",
    "        self.boundaries = boundaries\n",
    "\n",
    "        self.buckets, self.num_samples_per_bucket = self._create_buckets()\n",
    "        self.total_size = sum(self.num_samples_per_bucket)\n",
    "        self.num_samples = self.total_size // self.num_replicas\n",
    "\n",
    "    def _create_buckets(self):\n",
    "        buckets = [[] for _ in range(len(self.boundaries) - 1)]\n",
    "        for i in range(len(self.lengths)):\n",
    "            length = self.lengths[i]\n",
    "            idx_bucket = self._bisect(length)\n",
    "            if idx_bucket != -1:\n",
    "                buckets[idx_bucket].append(i)\n",
    "\n",
    "        for i in range(len(buckets) - 1, 0, -1):\n",
    "            if len(buckets[i]) == 0:\n",
    "                buckets.pop(i)\n",
    "                self.boundaries.pop(i + 1)\n",
    "\n",
    "        num_samples_per_bucket = []\n",
    "        for i in range(len(buckets)):\n",
    "            len_bucket = len(buckets[i])\n",
    "            total_batch_size = self.num_replicas * self.batch_size\n",
    "            rem = (\n",
    "                total_batch_size - (len_bucket % total_batch_size)\n",
    "            ) % total_batch_size\n",
    "            num_samples_per_bucket.append(len_bucket + rem)\n",
    "        return buckets, num_samples_per_bucket\n",
    "\n",
    "    def __iter__(self):\n",
    "        # deterministically shuffle based on epoch\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(self.epoch)\n",
    "        print(\"SFJKSJFLJDSKFJ\")\n",
    "\n",
    "        indices = []\n",
    "        if self.shuffle:\n",
    "            for bucket in self.buckets:\n",
    "                indices.append(torch.randperm(len(bucket), generator=g).tolist())\n",
    "        else:\n",
    "            for bucket in self.buckets:\n",
    "                indices.append(list(range(len(bucket))))\n",
    "\n",
    "        batches = []\n",
    "        for i in range(len(self.buckets)):\n",
    "            bucket = self.buckets[i]\n",
    "            len_bucket = len(bucket)\n",
    "            ids_bucket = indices[i]\n",
    "            num_samples_bucket = self.num_samples_per_bucket[i]\n",
    "\n",
    "            # add extra samples to make it evenly divisible\n",
    "            rem = num_samples_bucket - len_bucket\n",
    "            ids_bucket = (\n",
    "                ids_bucket\n",
    "                + ids_bucket * (rem // len_bucket)\n",
    "                + ids_bucket[: (rem % len_bucket)]\n",
    "            )\n",
    "\n",
    "            # subsample\n",
    "            ids_bucket = ids_bucket[self.rank :: self.num_replicas]\n",
    "\n",
    "            # batching\n",
    "            for j in range(len(ids_bucket) // self.batch_size):\n",
    "                batch = [\n",
    "                    bucket[idx]\n",
    "                    for idx in ids_bucket[\n",
    "                        j * self.batch_size : (j + 1) * self.batch_size\n",
    "                    ]\n",
    "                ]\n",
    "                batches.append(batch)\n",
    "\n",
    "        if self.shuffle:\n",
    "            batch_ids = torch.randperm(len(batches), generator=g).tolist()\n",
    "            batches = [batches[i] for i in batch_ids]\n",
    "        self.batches = batches\n",
    "\n",
    "        assert len(self.batches) * self.batch_size == self.num_samples\n",
    "        return iter(self.batches)\n",
    "\n",
    "    def _bisect(self, x, lo=0, hi=None):\n",
    "        if hi is None:\n",
    "            hi = len(self.boundaries) - 1\n",
    "\n",
    "        if hi > lo:\n",
    "            mid = (hi + lo) // 2\n",
    "            if self.boundaries[mid] < x and x <= self.boundaries[mid + 1]:\n",
    "                return mid\n",
    "            elif x <= self.boundaries[mid]:\n",
    "                return self._bisect(x, lo, mid)\n",
    "            else:\n",
    "                return self._bisect(x, mid + 1, hi)\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples // self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e600a61d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
