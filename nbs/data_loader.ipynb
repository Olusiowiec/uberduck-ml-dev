{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a5e8ece",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534dac87",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5031681",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5161412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db625c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import read\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from uberduck_ml_dev.models.common import STFT, MelSTFT\n",
    "from uberduck_ml_dev.text.util import text_to_sequence\n",
    "from uberduck_ml_dev.utils.audio import compute_yin\n",
    "from uberduck_ml_dev.utils.utils import load_filepaths_and_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0bcafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def _orig_to_dense_speaker_id(speaker_ids):\n",
    "    speaker_ids = sorted(list(set(speaker_ids)))\n",
    "    return {orig: idx for orig, idx in zip(speaker_ids, range(len(speaker_ids)))}\n",
    "\n",
    "\n",
    "class TextMelDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        audiopaths_and_text: str,\n",
    "        text_cleaners: List[str],\n",
    "        p_arpabet: float,\n",
    "        n_mel_channels: int,\n",
    "        sample_rate: int,\n",
    "        mel_fmin: float,\n",
    "        mel_fmax: float,\n",
    "        filter_length: int,\n",
    "        hop_length: int,\n",
    "        win_length: int,\n",
    "        max_wav_value: float = 32768.0,\n",
    "        include_f0: bool = False,\n",
    "        pos_weight: float = 10,\n",
    "        f0_min: int = 80,\n",
    "        f0_max: int = 880,\n",
    "        harmonic_thresh=0.25,\n",
    "        debug: bool = False,\n",
    "        debug_dataset_size: int = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        path = audiopaths_and_text\n",
    "        self.audiopaths_and_text = load_filepaths_and_text(path)\n",
    "        self.text_cleaners = text_cleaners\n",
    "        self.p_arpabet = p_arpabet\n",
    "        self.stft = MelSTFT(\n",
    "            filter_length=filter_length,\n",
    "            hop_length=hop_length,\n",
    "            win_length=win_length,\n",
    "            n_mel_channels=n_mel_channels,\n",
    "            sampling_rate=sample_rate,\n",
    "            mel_fmin=mel_fmin,\n",
    "            mel_fmax=mel_fmax,\n",
    "        )\n",
    "        self.max_wav_value = max_wav_value\n",
    "        self.sample_rate = sample_rate\n",
    "        self.filter_length = filter_length\n",
    "        self.hop_length = hop_length\n",
    "        self.mel_fmin = mel_fmin\n",
    "        self.mel_fmax = mel_fmax\n",
    "        self.include_f0 = include_f0\n",
    "        self.f0_min = f0_min\n",
    "        self.f0_max = f0_max\n",
    "        self.harmonic_threshold = harmonic_thresh\n",
    "        # speaker id lookup table\n",
    "        speaker_ids = [i[2] for i in self.audiopaths_and_text]\n",
    "        self._speaker_id_map = _orig_to_dense_speaker_id(speaker_ids)\n",
    "        self.debug = debug\n",
    "        self.debug_dataset_size = debug_dataset_size\n",
    "\n",
    "    def _get_f0(self, audio):\n",
    "        f0, harmonic_rates, argmins, times = compute_yin(\n",
    "            audio,\n",
    "            self.sample_rate,\n",
    "            self.filter_length,\n",
    "            self.hop_length,\n",
    "            self.f0_min,\n",
    "            self.f0_max,\n",
    "            self.harmonic_threshold,\n",
    "        )\n",
    "        pad = int((self.filter_length / self.hop_length) / 2)\n",
    "        f0 = [0.0] * pad + f0 + [0.0] * pad\n",
    "        f0 = np.array(f0, dtype=np.float32)\n",
    "        return f0\n",
    "\n",
    "    def _get_data(self, audiopath_and_text):\n",
    "        path, transcription, speaker_id = audiopath_and_text\n",
    "        speaker_id = self._speaker_id_map[speaker_id]\n",
    "        sample_rate, wav_data = read(path)\n",
    "        text_sequence = torch.LongTensor(\n",
    "            text_to_sequence(\n",
    "                transcription, self.text_cleaners, p_arpabet=self.p_arpabet\n",
    "            )\n",
    "        )\n",
    "        audio = torch.FloatTensor(wav_data)\n",
    "        audio_norm = audio / self.max_wav_value\n",
    "        audio_norm = audio_norm.unsqueeze(0)\n",
    "        melspec = self.stft.mel_spectrogram(audio_norm)\n",
    "        melspec = torch.squeeze(melspec, 0)\n",
    "        if not self.include_f0:\n",
    "            return (text_sequence, melspec, speaker_id)\n",
    "        f0 = self._get_f0(audio.data.cpu().numpy())\n",
    "        f0 = torch.from_numpy(f0)[None]\n",
    "        f0 = f0[:, : melspec.size(1)]\n",
    "\n",
    "        return (text_sequence, melspec, speaker_id, f0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return data for a single audio file + transcription.\"\"\"\n",
    "        try:\n",
    "            data = self._get_data(self.audiopaths_and_text[idx])\n",
    "        except Exception:\n",
    "            print(f\"Error while getting data: {self.audiopaths_and_text[idx]}\")\n",
    "            raise\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.debug and self.debug_dataset_size:\n",
    "            return min(self.debug_dataset_size, len(self.audiopaths_and_text))\n",
    "        return len(self.audiopaths_and_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4c8a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class TextMelCollate:\n",
    "    def __init__(self, n_frames_per_step: int = 1, include_f0: bool = False):\n",
    "        self.n_frames_per_step = n_frames_per_step\n",
    "        self.include_f0 = include_f0\n",
    "\n",
    "    def set_frames_per_step(self, n_frames_per_step):\n",
    "        \"\"\"Set n_frames_step.\n",
    "\n",
    "        This is used to train with gradual training, where we start with a large\n",
    "        n_frames_per_step in order to learn attention quickly and decrease it\n",
    "        over the course of training in order to increase accuracy. Gradual training\n",
    "        reference:\n",
    "        https://erogol.com/gradual-training-with-tacotron-for-faster-convergence/\n",
    "        \"\"\"\n",
    "        self.n_frames_per_step = n_frames_per_step\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        \"\"\"Collate's training batch from normalized text and mel-spectrogram\n",
    "        PARAMS\n",
    "        ------\n",
    "        batch: [text_normalized, mel_normalized, speaker_id]\n",
    "        \"\"\"\n",
    "        # Right zero-pad all one-hot text sequences to max input length\n",
    "        input_lengths, ids_sorted_decreasing = torch.sort(\n",
    "            torch.LongTensor([len(x[0]) for x in batch]), dim=0, descending=True\n",
    "        )\n",
    "        max_input_len = input_lengths[0]\n",
    "\n",
    "        text_padded = torch.LongTensor(len(batch), max_input_len)\n",
    "        text_padded.zero_()\n",
    "        for i in range(len(ids_sorted_decreasing)):\n",
    "            text = batch[ids_sorted_decreasing[i]][0]\n",
    "            text_padded[i, : text.size(0)] = text\n",
    "\n",
    "        # Right zero-pad mel-spec\n",
    "        num_mels = batch[0][1].size(0)\n",
    "        max_target_len = max([x[1].size(1) for x in batch])\n",
    "        if max_target_len % self.n_frames_per_step != 0:\n",
    "            max_target_len += (\n",
    "                self.n_frames_per_step - max_target_len % self.n_frames_per_step\n",
    "            )\n",
    "            assert max_target_len % self.n_frames_per_step == 0\n",
    "\n",
    "        # include mel padded, gate padded and speaker ids\n",
    "        mel_padded = torch.FloatTensor(len(batch), num_mels, max_target_len)\n",
    "        mel_padded.zero_()\n",
    "        gate_padded = torch.FloatTensor(len(batch), max_target_len)\n",
    "        gate_padded.zero_()\n",
    "        output_lengths = torch.LongTensor(len(batch))\n",
    "        speaker_ids = torch.LongTensor(len(batch))\n",
    "        if self.include_f0:\n",
    "            f0_padded = torch.FloatTensor(len(batch), 1, max_target_len)\n",
    "            f0_padded.zero_()\n",
    "\n",
    "        for i in range(len(ids_sorted_decreasing)):\n",
    "            mel = batch[ids_sorted_decreasing[i]][1]\n",
    "            mel_padded[i, :, : mel.size(1)] = mel\n",
    "            gate_padded[i, mel.size(1) - 1 :] = 1\n",
    "            output_lengths[i] = mel.size(1)\n",
    "            speaker_ids[i] = batch[ids_sorted_decreasing[i]][2]\n",
    "            if self.include_f0:\n",
    "                f0 = batch[ids_sorted_decreasing[i]][3]\n",
    "                f0_padded[i, :, : f0.size(1)] = f0\n",
    "\n",
    "        # NOTE(zach): would model_inputs be better as a namedtuple or dataclass?\n",
    "        if self.include_f0:\n",
    "            model_inputs = (\n",
    "                text_padded,\n",
    "                input_lengths,\n",
    "                mel_padded,\n",
    "                gate_padded,\n",
    "                output_lengths,\n",
    "                speaker_ids,\n",
    "                f0_padded,\n",
    "            )\n",
    "        else:\n",
    "            model_inputs = (\n",
    "                text_padded,\n",
    "                input_lengths,\n",
    "                mel_padded,\n",
    "                gate_padded,\n",
    "                output_lengths,\n",
    "                speaker_ids,\n",
    "            )\n",
    "\n",
    "        return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed89f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 0, 3: 1, 4: 2, 9: 3}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_orig_to_dense_speaker_id([4, 2, 9, 3, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b89cc08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = TextMelDataset(\n",
    "    \"test/fixtures/val.txt\",\n",
    "    [\"english_cleaners\"],\n",
    "    0.0,\n",
    "    80,\n",
    "    22050,\n",
    "    0,\n",
    "    8000,\n",
    "    1024,\n",
    "    256,\n",
    "    1024,\n",
    "    debug=True,\n",
    "    debug_dataset_size=12,\n",
    ")\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16563fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zwf/miniconda3/envs/uberduck/lib/python3.6/site-packages/ipykernel_launcher.py:85: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:143.)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "collate_fn = TextMelCollate()\n",
    "dl = DataLoader(ds, 12, collate_fn=collate_fn)\n",
    "for i, batch in enumerate(dl):\n",
    "    assert len(batch) == 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1d74b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = TextMelDataset(\n",
    "    \"test/fixtures/val.txt\",\n",
    "    [\"english_cleaners\"],\n",
    "    0.0,\n",
    "    80,\n",
    "    22050,\n",
    "    0,\n",
    "    8000,\n",
    "    1024,\n",
    "    256,\n",
    "    1024,\n",
    "    debug=True,\n",
    "    debug_dataset_size=12,\n",
    "    include_f0=True,\n",
    ")\n",
    "assert len(ds) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad10c68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = TextMelCollate(include_f0=True)\n",
    "dl = DataLoader(ds, 12, collate_fn=collate_fn)\n",
    "for i, batch in enumerate(dl):\n",
    "    # text_padded,\n",
    "    # input_lengths,\n",
    "    # mel_padded,\n",
    "    # gate_padded,\n",
    "    # output_lengths,\n",
    "    # speaker_ids,\n",
    "    (\n",
    "        text_padded,\n",
    "        input_lengths,\n",
    "        mel_padded,\n",
    "        gate_padded,\n",
    "        output_lengths,\n",
    "        speaker_ids,\n",
    "        *_,\n",
    "    ) = batch\n",
    "    assert output_lengths.item() == 566, print(\"output lengths: \", output_lengths)\n",
    "    assert gate_padded.size(1) == 566\n",
    "    assert mel_padded.size(2) == 566\n",
    "    assert len(batch) == 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae303fcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 80, 566])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3cfd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing n_frames_per_step > 1\n",
    "ds = TextMelDataset(\n",
    "    \"test/fixtures/val.txt\",\n",
    "    [\"english_cleaners\"],\n",
    "    0.0,\n",
    "    80,\n",
    "    22050,\n",
    "    0,\n",
    "    8000,\n",
    "    1024,\n",
    "    256,\n",
    "    1024,\n",
    "    debug=True,\n",
    "    debug_dataset_size=12,\n",
    "    include_f0=True,\n",
    ")\n",
    "assert len(ds) == 1\n",
    "collate_fn = TextMelCollate(n_frames_per_step=5, include_f0=True)\n",
    "dl = DataLoader(ds, 12, collate_fn=collate_fn)\n",
    "# text_padded,\n",
    "# input_lengths,\n",
    "# mel_padded,\n",
    "# gate_padded,\n",
    "# output_lengths,\n",
    "# speaker_ids,\n",
    "for i, batch in enumerate(dl):\n",
    "    (\n",
    "        text_padded,\n",
    "        input_lengths,\n",
    "        mel_padded,\n",
    "        gate_padded,\n",
    "        output_lengths,\n",
    "        speaker_ids,\n",
    "        *_,\n",
    "    ) = batch\n",
    "    assert output_lengths.item() == 566, output_lengths.item()\n",
    "    assert mel_padded.size(2) == 570, print(\"actual shape: \", mel_padded.shape)\n",
    "    assert gate_padded.size(1) == 570, print(\"actual shape: \", gate_padded.shape)\n",
    "    assert len(batch) == 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117a6aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
