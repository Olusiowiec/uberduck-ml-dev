{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c43cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp exec.dataset_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1b5836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from typing import List, Any, Dict\n",
    "import json\n",
    "\n",
    "from g2p_en import G2p\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from mdutils.mdutils import MdUtils\n",
    "import numpy as np\n",
    "from pydub import AudioSegment, silence\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "from uberduck_ml_dev.text.util import clean_text, text_to_sequence\n",
    "from uberduck_ml_dev.data.statistics import (\n",
    "    AbsoluteMetrics,\n",
    "    count_frequency,\n",
    "    create_wordcloud,\n",
    "    get_sample_format,\n",
    "    pace_character,\n",
    "    pace_phoneme,\n",
    "    word_frequencies,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483a20bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def get_summary_statistics(arr):\n",
    "    if len(arr) == 0:\n",
    "        return {}\n",
    "    arr_np = np.array(arr)\n",
    "    return {\n",
    "        \"p10\": float(np.percentile(arr_np, 10)),\n",
    "        \"p25\": float(np.percentile(arr_np, 25)),\n",
    "        \"p50\": float(np.percentile(arr_np, 50)),\n",
    "        \"p75\": float(np.percentile(arr_np, 75)),\n",
    "        \"p90\": float(np.percentile(arr_np, 90)),\n",
    "        \"max\": float(np.max(arr_np)),\n",
    "        \"min\": float(np.min(arr_np)),\n",
    "    }\n",
    "\n",
    "\n",
    "def calculate_statistics(\n",
    "    dataset_path, input_file, output_folder, delimiter, metrics=True, wordcloud=True\n",
    "):\n",
    "    n_clips = 0\n",
    "    sample_rates = {}\n",
    "    channels = {\"mono\": 0, \"stereo\": 0}\n",
    "    extensions = {}\n",
    "    sample_formats = {}\n",
    "    total_lengths = []\n",
    "    leading_silence_lengths = []\n",
    "    trailing_silence_lengths = []\n",
    "    paces_characters = []  # number of characters / seconds in audio clip\n",
    "    paces_phonemes = []  # number of phonemes / seconds in audio clip\n",
    "    lookup_results = {\n",
    "        \"RNN\": [],\n",
    "        \"CMU\": [],\n",
    "        \"non-alphanumeric\": [],\n",
    "        \"homograph\": [],\n",
    "    }  # keep track of how arpabet sequences were generated\n",
    "    mosnet_scores = []\n",
    "    srmr_scores = []\n",
    "    word_freqs = []\n",
    "    all_words = []\n",
    "    g2p = G2p()\n",
    "    files_with_error = []\n",
    "    if metrics:\n",
    "        abs_metrics = AbsoluteMetrics()\n",
    "\n",
    "    with open(os.path.join(dataset_path, input_file)) as transcripts:\n",
    "        for line in tqdm(transcripts):\n",
    "            try:\n",
    "                line = line.strip()  # remove trailing newline character\n",
    "                file, transcription = line.lower().split(delimiter)\n",
    "                transcription_cleaned = clean_text(transcription, [\"english_cleaners\"])\n",
    "\n",
    "                _, file_extension = os.path.splitext(file)\n",
    "                path_to_file = os.path.join(dataset_path, file)\n",
    "                file_pydub = AudioSegment.from_wav(path_to_file)\n",
    "\n",
    "                # Format Metadata\n",
    "                sr = file_pydub.frame_rate\n",
    "                if sr in sample_rates.keys():\n",
    "                    sample_rates[sr] += 1\n",
    "                else:\n",
    "                    sample_rates[sr] = 1\n",
    "\n",
    "                if file_pydub.channels == 1:\n",
    "                    channels[\"mono\"] += 1\n",
    "                else:\n",
    "                    channels[\"stereo\"] += 1\n",
    "\n",
    "                if file_extension in extensions.keys():\n",
    "                    extensions[file_extension] += 1\n",
    "                else:\n",
    "                    extensions[file_extension] = 1\n",
    "\n",
    "                fmt = get_sample_format(path_to_file)\n",
    "                if fmt in sample_formats.keys():\n",
    "                    sample_formats[fmt] += 1\n",
    "                else:\n",
    "                    sample_formats[fmt] = 1\n",
    "\n",
    "                # lengths\n",
    "                total_lengths.append(file_pydub.duration_seconds)\n",
    "                leading_silence_lengths.append(\n",
    "                    silence.detect_leading_silence(file_pydub)\n",
    "                )\n",
    "                trailing_silence_lengths.append(\n",
    "                    silence.detect_leading_silence(file_pydub.reverse())\n",
    "                )\n",
    "\n",
    "                # Paces\n",
    "                paces_phonemes.append(\n",
    "                    pace_phoneme(text=transcription_cleaned, audio=path_to_file)\n",
    "                )\n",
    "                paces_characters.append(\n",
    "                    pace_character(text=transcription_cleaned, audio=path_to_file)\n",
    "                )\n",
    "\n",
    "                # Quality\n",
    "                if metrics:\n",
    "                    scores = abs_metrics(path_to_file)\n",
    "                    mosnet_scores.append(scores[\"mosnet\"][0][0])\n",
    "                    srmr_scores.append(scores[\"srmr\"])\n",
    "\n",
    "                # Transcription\n",
    "                word_freqs.extend(word_frequencies(transcription_cleaned))\n",
    "                transcription_lookups = g2p.check_lookup(transcription_cleaned)\n",
    "                for k in transcription_lookups:\n",
    "                    lookup_results[k].extend(transcription_lookups[k])\n",
    "\n",
    "                all_words.append(transcription_cleaned)\n",
    "\n",
    "                n_clips += 1\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                files_with_error.append(file)\n",
    "\n",
    "    if n_clips == 0:\n",
    "        return None\n",
    "\n",
    "    if wordcloud:\n",
    "        create_wordcloud(\n",
    "            \" \".join(all_words),\n",
    "            os.path.join(dataset_path, output_folder, \"wordcloud.png\"),\n",
    "        )\n",
    "\n",
    "    # Length graph\n",
    "    plt.clf()\n",
    "    sns.histplot(total_lengths)\n",
    "    plt.title(\"Audio length distribution\")\n",
    "    plt.xlabel(\"Audio length (s)\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.savefig(os.path.join(dataset_path, output_folder, \"lengths.png\"))\n",
    "\n",
    "    # Word Frequencies graph\n",
    "    plt.clf()\n",
    "    sns.histplot(word_freqs, bins=10)\n",
    "    plt.title(\"Word frequency distribution [0-1]\")\n",
    "    plt.xlabel(\"Word frequency\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.savefig(os.path.join(dataset_path, output_folder, \"word_frequencies.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Silences graph\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(leading_silence_lengths)\n",
    "    plt.title(\"Leading silence distribution\")\n",
    "    plt.xlabel(\"Leading silence (ms)\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(trailing_silence_lengths)\n",
    "    plt.title(\"Traling silence distribution\")\n",
    "    plt.xlabel(\"Trailing silence (ms)\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.savefig(os.path.join(dataset_path, output_folder, \"silences.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Metrics graph\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(mosnet_scores)\n",
    "    plt.title(\"Mosnet score distribution\")\n",
    "    plt.xlabel(\"Mosnet score\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(srmr_scores)\n",
    "    plt.title(\"SRMR score distribution\")\n",
    "    plt.xlabel(\"SRMR score\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.savefig(os.path.join(dataset_path, output_folder, \"metrics.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Paces graph\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(paces_characters)\n",
    "    plt.title(\"Pace (chars/s)\")\n",
    "    plt.xlabel(\"Characters / second\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(paces_phonemes)\n",
    "    plt.title(\"Pace (phonemes/s)\")\n",
    "    plt.xlabel(\"Phonemes / second\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.savefig(os.path.join(dataset_path, output_folder, \"paces.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    return {\n",
    "        \"n_clips\": n_clips,\n",
    "        \"total_lengths_summary\": get_summary_statistics(total_lengths),\n",
    "        \"paces_phonemes_summary\": get_summary_statistics(paces_phonemes),\n",
    "        \"paces_characters_summary\": get_summary_statistics(paces_characters),\n",
    "        \"mosnet_scores_summary\": get_summary_statistics(mosnet_scores),\n",
    "        \"srmr_scores_summary\": get_summary_statistics(srmr_scores),\n",
    "        \"total_lengths\": total_lengths,\n",
    "        \"paces_phonemes\": paces_phonemes,\n",
    "        \"paces_characters\": paces_characters,\n",
    "        \"mosnet_scores\": mosnet_scores,\n",
    "        \"srmr_scores\": srmr_scores,\n",
    "        \"sample_rates\": sample_rates,\n",
    "        \"channels\": channels,\n",
    "        \"extensions\": extensions,\n",
    "        \"sample_formats\": sample_formats,\n",
    "        \"lookup_results\": lookup_results,\n",
    "        \"files_with_error\": files_with_error,\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_markdown(output_file, dataset_path, output_folder, data):\n",
    "    mdFile = MdUtils(\n",
    "        file_name=os.path.join(dataset_path, output_file), title=f\"Dataset statistics\"\n",
    "    )\n",
    "\n",
    "    total_length_mins = sum(data[\"total_lengths\"]) / 60.0\n",
    "    mdFile.new_header(level=1, title=\"Overview\")\n",
    "    mdFile.new_line(f\"**Number of clips:** {data['n_clips']}\")\n",
    "    mdFile.new_line(\n",
    "        f\"**Total data:** {math.floor(total_length_mins)} minutes {math.ceil(total_length_mins % 1 * 60.0)} seconds\"\n",
    "    )\n",
    "    mdFile.new_line(\n",
    "        f\"**Mean clip length:** {sum(data['total_lengths'])/data['n_clips']:.2f} seconds\"\n",
    "    )\n",
    "    mdFile.new_line(\n",
    "        f\"**Mean pace:** {sum(data['paces_phonemes'])/len(data['paces_phonemes']):.2f} \\\n",
    "            phonemes/sec {sum(data['paces_characters'])/len(data['paces_characters']):.2f} chars/sec\"\n",
    "    )\n",
    "    if len(data[\"mosnet_scores\"]) > 0:\n",
    "        mdFile.new_line(\n",
    "            f\"**Mean MOSNet:** {sum(data['mosnet_scores'])/len(data['mosnet_scores']):.2f}\"\n",
    "        )\n",
    "        mdFile.new_line(\n",
    "            f\"**Mean SRMR:** {sum(data['srmr_scores'])/len(data['srmr_scores']):.2f}\"\n",
    "        )\n",
    "\n",
    "    if len(data[\"files_with_error\"]) > 0:\n",
    "        mdFile.new_line(f\"**Errored Files:** {', '.join(data['files_with_error'])}\")\n",
    "\n",
    "    list_of_strings = [\"Sample Rate (Hz)\", \"Count\"]\n",
    "    for k in data[\"sample_rates\"].keys():\n",
    "        list_of_strings.extend([str(k), str(data[\"sample_rates\"][k])])\n",
    "    mdFile.new_table(\n",
    "        columns=2,\n",
    "        rows=len(data[\"sample_rates\"].keys()) + 1,\n",
    "        text=list_of_strings,\n",
    "        text_align=\"center\",\n",
    "    )\n",
    "\n",
    "    list_of_strings = [\"Audio Type\", \"Count\"]\n",
    "    n_rows = 1\n",
    "    for k in data[\"channels\"].keys():\n",
    "        if data[\"channels\"][k] > 0:\n",
    "            n_rows += 1\n",
    "            list_of_strings.extend([str(k), str(data[\"channels\"][k])])\n",
    "    mdFile.new_table(columns=2, rows=n_rows, text=list_of_strings, text_align=\"center\")\n",
    "\n",
    "    list_of_strings = [\"Audio Format\", \"Count\"]\n",
    "    for k in data[\"extensions\"].keys():\n",
    "        list_of_strings.extend([str(k), str(data[\"extensions\"][k])])\n",
    "    mdFile.new_table(\n",
    "        columns=2,\n",
    "        rows=len(data[\"extensions\"].keys()) + 1,\n",
    "        text=list_of_strings,\n",
    "        text_align=\"center\",\n",
    "    )\n",
    "\n",
    "    list_of_strings = [\"Sample Format\", \"Count\"]\n",
    "    for k in data[\"sample_formats\"].keys():\n",
    "        list_of_strings.extend([str(k), str(data[\"sample_formats\"][k])])\n",
    "    mdFile.new_table(\n",
    "        columns=2,\n",
    "        rows=len(data[\"sample_formats\"].keys()) + 1,\n",
    "        text=list_of_strings,\n",
    "        text_align=\"center\",\n",
    "    )\n",
    "\n",
    "    list_of_strings = [\"Arpabet Lookup Type\", \"Count\"]\n",
    "    for k in data[\"lookup_results\"].keys():\n",
    "        list_of_strings.extend([str(k), str(len(data[\"lookup_results\"][k]))])\n",
    "    mdFile.new_table(\n",
    "        columns=2,\n",
    "        rows=len(data[\"lookup_results\"].keys()) + 1,\n",
    "        text=list_of_strings,\n",
    "        text_align=\"center\",\n",
    "    )\n",
    "    mdFile.new_line(\n",
    "        mdFile.new_inline_image(\n",
    "            text=\"Wordcloud\", path=os.path.join(output_folder, \"wordcloud.png\")\n",
    "        )\n",
    "    )\n",
    "    mdFile.new_line(\n",
    "        mdFile.new_inline_image(\n",
    "            text=\"Audio Lengths\", path=os.path.join(output_folder, \"lengths.png\")\n",
    "        )\n",
    "    )\n",
    "    mdFile.new_line(\n",
    "        mdFile.new_inline_image(\n",
    "            text=\"Paces\", path=os.path.join(output_folder, \"paces.png\")\n",
    "        )\n",
    "    )\n",
    "    mdFile.new_line(\n",
    "        mdFile.new_inline_image(\n",
    "            text=\"Silences\", path=os.path.join(output_folder, \"silences.png\")\n",
    "        )\n",
    "    )\n",
    "    if len(data[\"mosnet_scores\"]) > 0:\n",
    "        mdFile.new_line(\n",
    "            mdFile.new_inline_image(\n",
    "                text=\"Metrics\", path=os.path.join(output_folder, \"metrics.png\")\n",
    "            )\n",
    "        )\n",
    "    mdFile.new_line(\n",
    "        mdFile.new_inline_image(\n",
    "            text=\"Word Frequencies\",\n",
    "            path=os.path.join(output_folder, \"word_frequencies.png\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    rnn_frequency_counts = count_frequency(data[\"lookup_results\"][\"RNN\"])\n",
    "\n",
    "    list_of_strings = [\"Frequently Missed Words\", \"Count\"]\n",
    "    n_rows = 0\n",
    "    for k in rnn_frequency_counts.keys():\n",
    "        if rnn_frequency_counts[k] > 1:\n",
    "            n_rows += 1\n",
    "            list_of_strings.extend([str(k), str(rnn_frequency_counts[k])])\n",
    "    mdFile.new_table(\n",
    "        columns=2,\n",
    "        rows=n_rows + 1,\n",
    "        text=list_of_strings,\n",
    "        text_align=\"center\",\n",
    "    )\n",
    "\n",
    "    mdFile.new_line(\n",
    "        f'**Words not found in CMU:** {\", \".join(data[\"lookup_results\"][\"RNN\"])}'\n",
    "    )\n",
    "    mdFile.create_md_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5eb1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def parse_args(args):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"-d\", \"--dataset_path\", help=\"Path to the dataset.\", type=str, required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-i\",\n",
    "        \"--input_file\",\n",
    "        help=\"Path to the transcription file.\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-o\",\n",
    "        \"--output_file\",\n",
    "        help=\"Markdown file to write statistics to.\",\n",
    "        type=str,\n",
    "        default=\"README\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_folder\",\n",
    "        help=\"Folder to save plots and images.\",\n",
    "        type=str,\n",
    "        default=\"stats\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--delimiter\", help=\"Transcription file delimiter.\", type=str, default=\"|\"\n",
    "    )\n",
    "    parser.add_argument(\"--metrics\", dest=\"metrics\", action=\"store_true\")\n",
    "    parser.add_argument(\"--no-metrics\", dest=\"metrics\", action=\"store_false\")\n",
    "    parser.add_argument(\"--wordcloud\", dest=\"wordcloud\", action=\"store_true\")\n",
    "    parser.add_argument(\"--no-wordcloud\", dest=\"wordcloud\", action=\"store_false\")\n",
    "    parser.set_defaults(metrics=True, wordcloud=True)\n",
    "    return parser.parse_args(args)\n",
    "\n",
    "\n",
    "def run(\n",
    "    dataset_path, input_file, output_file, output_folder, delimiter, metrics, wordcloud\n",
    "):\n",
    "    if not os.path.exists(os.path.join(dataset_path, input_file)):\n",
    "        raise Exception(\n",
    "            f\"Transcription file {os.path.join(dataset_path,input_file)} does not exist\"\n",
    "        )\n",
    "\n",
    "    os.makedirs(os.path.join(dataset_path, output_folder), exist_ok=True)\n",
    "    data = calculate_statistics(\n",
    "        dataset_path, input_file, output_folder, delimiter, metrics, wordcloud\n",
    "    )\n",
    "    if data:\n",
    "        generate_markdown(output_file, dataset_path, output_folder, data)\n",
    "        with open(os.path.join(dataset_path, \"stats.json\"), \"w\") as outfile:\n",
    "            keys = [\n",
    "                \"n_clips\",\n",
    "                \"total_lengths_summary\",\n",
    "                \"paces_phonemes_summary\",\n",
    "                \"paces_characters_summary\",\n",
    "                \"mosnet_scores_summary\",\n",
    "                \"srmr_scores_summary\",\n",
    "                \"sample_rates\",\n",
    "                \"channels\",\n",
    "                \"extensions\",\n",
    "                \"sample_formats\",\n",
    "            ]\n",
    "            json_data = {k: data[k] for k in keys}\n",
    "            json_data[\"arpabet_rnn\"] = data[\"lookup_results\"][\"RNN\"]\n",
    "            json.dump(json_data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ef8ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded  speechmetrics.absolute.mosnet\n",
      "Loaded  speechmetrics.absolute.srmr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:01, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_478591/290434998.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"|\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mwordcloud\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     )\n",
      "\u001b[0;32m/tmp/ipykernel_478591/4186226992.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(dataset_path, input_file, output_file, output_folder, delimiter, metrics, wordcloud)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     data = calculate_statistics(\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordcloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_478591/2148983653.py\u001b[0m in \u001b[0;36mcalculate_statistics\u001b[0;34m(dataset_path, input_file, output_folder, delimiter, metrics, wordcloud)\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;31m# Quality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m                     \u001b[0mmosnet_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mosnet\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                     \u001b[0msrmr_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"srmr\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/uberduck-ml-dev/uberduck_ml_dev/data/statistics.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wav_file)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mfile_extension\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\".wav\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".wav file must be supplied\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/tts/lib/python3.7/site-packages/speechmetrics/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, rate, *files)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0mresult_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult_metric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult_metric\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tts/lib/python3.7/site-packages/speechmetrics/__init__.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, array_rate, *test_files)\u001b[0m\n\u001b[1;32m     93\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudios\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tts/lib/python3.7/site-packages/speechmetrics/absolute/mosnet/model.py\u001b[0m in \u001b[0;36mtest_window\u001b[0;34m(self, audios, rate)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m# now call the actual MOSnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         return {'mosnet':\n\u001b[0;32m--> 107\u001b[0;31m                 self.model.predict(mag[None, ...], verbose=0, batch_size=1)[0]}\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/tts/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1627\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tts/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tts/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       return self._concrete_stateful_fn._call_flat(\n\u001b[0;32m--> 895\u001b[0;31m           filtered_flat_args, self._concrete_stateful_fn.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfn_with_cond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_kwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_filtered_flat_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tts/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tts/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/miniconda3/envs/tts/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "folders = glob.glob(\"/home/ubuntu/data/uberduck-multispeaker/*/*.txt\")\n",
    "\n",
    "for dataset in folders:\n",
    "    split = dataset.split(\"/\")\n",
    "    file = split[-1]\n",
    "    dataset_path = \"/\".join(split[:-1])\n",
    "\n",
    "    run(\n",
    "        dataset_path=dataset_path,\n",
    "        input_file=file,\n",
    "        output_file=\"README.md\",\n",
    "        output_folder=\"imgs\",\n",
    "        delimiter=\"|\",\n",
    "        metrics=True,\n",
    "        wordcloud=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2832141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded  speechmetrics.absolute.mosnet\n",
      "Loaded  speechmetrics.absolute.srmr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:10,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_clips': 11, 'total_lengths_summary': {'p10': 1.1946031746031747, 'p25': 1.5814965986394558, 'p50': 1.9547845804988662, 'p75': 2.6675056689342407, 'p90': 2.7964625850340137, 'max': 5.5114739229024945, 'min': 0.8824036281179138}, 'paces_phonemes_summary': {'p10': 7.9328776275890425, 'p25': 10.055462362442928, 'p50': 13.627935723114957, 'p75': 14.492604373956365, 'p90': 15.267175572519083, 'max': 17.373695987897506, 'min': 7.540444561357359}, 'paces_characters_summary': {'p10': 9.280547152439826, 'p25': 12.272987166143496, 'p50': 14.835394288100597, 'p75': 16.979217655670745, 'p90': 17.579059261227744, 'max': 18.763591666929308, 'min': 7.9328776275890425}, 'mosnet_scores_summary': {'p10': 2.447880506515503, 'p25': 2.7411155700683594, 'p50': 2.8930106163024902, 'p75': 3.2346423864364624, 'p90': 3.3316733837127686, 'max': 3.370675802230835, 'min': 2.437530040740967}, 'srmr_scores_summary': {'p10': 6.936754256941414, 'p25': 7.513167463233813, 'p50': 8.022657907234512, 'p75': 9.548200979493597, 'p90': 10.50844546077112, 'max': 12.381816951132445, 'min': 6.582824733141039}, 'sample_rates': {22050: 11}, 'channels': {'mono': 11, 'stereo': 0}, 'extensions': {'.wav': 11}, 'sample_formats': {'s16': 11}, 'arpabet_rnn': []}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run(\n",
    "    dataset_path=\"/home/ubuntu/data/uberduck-multispeaker/bullwinkle\",\n",
    "    input_file=\"list.txt\",\n",
    "    output_file=\"STATISTICS.md\",\n",
    "    output_folder=\"imgs\",\n",
    "    delimiter=\"|\",\n",
    "    metrics=True,\n",
    "    wordcloud=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e79961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "try:\n",
    "    from nbdev.imports import IN_NOTEBOOK\n",
    "except:\n",
    "    IN_NOTEBOOK = False\n",
    "\n",
    "if __name__ == \"__main__\" and not IN_NOTEBOOK:\n",
    "    args = parse_args(sys.argv[1:])\n",
    "\n",
    "    if os.path.exists(\n",
    "        os.path.join(args.dataset_path, args.output_file)\n",
    "    ) or os.path.exists(os.path.join(args.dataset_path, args.output_file + \".md\")):\n",
    "        inp = input(\n",
    "            f\"This script will overwite everything in the {args.output_file} file with dataset statistics. Would you like to continue? (y/n) \"\n",
    "        ).lower()\n",
    "        if inp != \"y\":\n",
    "            print(\"Not calculating statistics...\")\n",
    "            print(\"HINT: Use -o/--output-file to specify a new markdown file name\")\n",
    "            sys.exit()\n",
    "    print(\"Calculating statistics...\")\n",
    "    run(\n",
    "        args.dataset_path,\n",
    "        args.input_file,\n",
    "        args.output_file,\n",
    "        args.output_folder,\n",
    "        args.delimiter,\n",
    "        args.metrics,\n",
    "        args.wordcloud,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5884719e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
