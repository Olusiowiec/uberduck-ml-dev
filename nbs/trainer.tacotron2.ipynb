{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42d43fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp trainer.tacotron2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4295c4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "from random import choice, randint\n",
    "from uberduck_ml_dev.models.tacotron2.tacotron2 import Tacotron2\n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from uberduck_ml_dev.data_loader import TextMelDataset, TextMelCollate\n",
    "\n",
    "from uberduck_ml_dev.utils.plot import save_figure_to_numpy\n",
    "from uberduck_ml_dev.utils.utils import reduce_tensor\n",
    "from uberduck_ml_dev.monitoring.statistics import get_alignment_metrics\n",
    "import nemo\n",
    "from typing import NamedTuple\n",
    "from uberduck_ml_dev.losses.tacotron2 import Tacotron2Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16004eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from random import choice\n",
    "\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from tensorboardX import SummaryWriter\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "from uberduck_ml_dev.models.common import MelSTFT\n",
    "from uberduck_ml_dev.models.torchmoji import TorchMojiInterface\n",
    "from uberduck_ml_dev.utils.plot import (\n",
    "    plot_attention,\n",
    "    plot_gate_outputs,\n",
    "    plot_spectrogram,\n",
    ")\n",
    "from uberduck_ml_dev.text.util import text_to_sequence, random_utterance\n",
    "from uberduck_ml_dev.trainer.base import TTSTrainer\n",
    "from uberduck_ml_dev.data_loader import TextMelDataset, TextMelCollate\n",
    "from uberduck_ml_dev.data.batch import Batch\n",
    "\n",
    "class Tacotron2Trainer(TTSTrainer):\n",
    "\n",
    "    REQUIRED_HPARAMS = [\n",
    "        \"audiopaths_and_text\",\n",
    "        \"checkpoint_path\",\n",
    "        \"epochs\",\n",
    "        \"mel_fmax\",\n",
    "        \"mel_fmin\",\n",
    "        \"n_mel_channels\",\n",
    "        \"text_cleaners\",\n",
    "        \"pos_weight\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.include_durations = self.hparams.include_durations\n",
    "        self.sampling_rate = self.hparams.sampling_rate\n",
    "        self.hop_length = self.hparams.hop_length\n",
    "\n",
    "        if self.hparams.get(\"gst_type\") == \"torchmoji\":\n",
    "            assert self.hparams.get(\n",
    "                \"torchmoji_vocabulary_file\"\n",
    "            ), \"torchmoji_vocabulary_file must be set\"\n",
    "            assert self.hparams.get(\n",
    "                \"torchmoji_model_file\"\n",
    "            ), \"torchmoji_model_file must be set\"\n",
    "            assert self.hparams.get(\"gst_dim\"), \"gst_dim must be set\"\n",
    "\n",
    "            self.torchmoji = TorchMojiInterface(\n",
    "                self.hparams.get(\"torchmoji_vocabulary_file\"),\n",
    "                self.hparams.get(\"torchmoji_model_file\"),\n",
    "            )\n",
    "            self.compute_gst = lambda texts: self.torchmoji.encode_texts(texts)\n",
    "        else:\n",
    "            self.compute_gst = None\n",
    "\n",
    "        if not self.sample_inference_speaker_ids:\n",
    "            self.sample_inference_speaker_ids = list(range(self.n_speakers))\n",
    "\n",
    "        # pass\n",
    "\n",
    "    def log_training(\n",
    "        self,\n",
    "        model,\n",
    "        X,\n",
    "        y_pred,\n",
    "        y,\n",
    "        loss,\n",
    "        mel_loss,\n",
    "        gate_loss,\n",
    "        mel_loss_batch,\n",
    "        gate_loss_batch,\n",
    "        grad_norm,\n",
    "        step_duration_seconds,\n",
    "    ):\n",
    "        self.log(\"Loss/train\", self.global_step, scalar=loss)\n",
    "        self.log(\"MelLoss/train\", self.global_step, scalar=mel_loss)\n",
    "        self.log(\"GateLoss/train\", self.global_step, scalar=gate_loss)\n",
    "        self.log(\"GradNorm\", self.global_step, scalar=grad_norm.item())\n",
    "        self.log(\"LearningRate\", self.global_step, scalar=self.learning_rate)\n",
    "        self.log(\n",
    "            \"StepDurationSeconds\", self.global_step, scalar=step_duration_seconds,\n",
    "        )\n",
    "\n",
    "        batch_levels = X.speaker_ids\n",
    "        batch_levels_unique = torch.unique(batch_levels)\n",
    "        for l in batch_levels_unique:\n",
    "            mlb = mel_loss_batch[torch.where(batch_levels == l)[0]].mean()\n",
    "            self.log(\n",
    "                f\"MelLoss/train/speaker{l.item()}\", self.global_step, scalar=mlb,\n",
    "            )\n",
    "            if self.location_specific_attention:\n",
    "                glb = gate_loss_batch[torch.where(batch_levels == l)[0]].mean()\n",
    "                self.log(\n",
    "                    f\"GateLoss/train/speaker{l.item()}\", self.global_step, scalar=glb,\n",
    "                )\n",
    "                self.log(\n",
    "                    f\"Loss/train/speaker{l.item()}\", self.global_step, scalar=mlb + glb,\n",
    "                )\n",
    "\n",
    "        if self.global_step % self.steps_per_sample == 0:\n",
    "\n",
    "            mel_out_postnet = y_pred.mel_outputs_postnet\n",
    "            \n",
    "            sample_idx = randint(0, mel_out_postnet.size(0) - 1)\n",
    "            audio = self.sample(mel=mel_out_postnet[sample_idx])\n",
    "            input_length = X.input_lengths[sample_idx].item()\n",
    "            output_length = X.output_lengths[sample_idx].item()\n",
    "            gate_pred = y_pred.gate_pred\n",
    "            alignments = y_pred.alignments\n",
    "            mel_target = y.mel_padded\n",
    "            gate_target = y.gate_target\n",
    "            if self.location_specific_attention:\n",
    "                alignment_metrics = get_alignment_metrics(alignments)\n",
    "                alignment_diagonalness = alignment_metrics[\"diagonalness\"]\n",
    "                alignment_max = alignment_metrics[\"max\"]\n",
    "                self.log(\n",
    "                    \"AlignmentDiagonalness/train\",\n",
    "                    self.global_step,\n",
    "                    scalar=alignment_diagonalness,\n",
    "                )\n",
    "                self.log(\"AlignmentMax/train\", self.global_step, scalar=alignment_max)\n",
    "                self.log(\"AudioSample/train\", self.global_step, audio=audio)\n",
    "                self.log(\n",
    "                    \"Attention/train\",\n",
    "                    self.global_step,\n",
    "                    image=save_figure_to_numpy(\n",
    "                        plot_attention(\n",
    "                            alignments[sample_idx].data.cpu().transpose(0, 1),\n",
    "                            encoder_length=input_length,\n",
    "                            decoder_length=output_length,\n",
    "                        )\n",
    "                    ),\n",
    "                )\n",
    "                self.log(\n",
    "                    \"Gate/train\",\n",
    "                    self.global_step,\n",
    "                    image=save_figure_to_numpy(\n",
    "                        plot_gate_outputs(\n",
    "                            gate_targets=gate_target[sample_idx].data.cpu(),\n",
    "                            gate_outputs=gate_pred[sample_idx].data.cpu(),\n",
    "                        )\n",
    "                    ),\n",
    "                )\n",
    "            alignment_max = alignment_metrics[\"max\"]\n",
    "            audio = self.sample(mel=mel_out_postnet[sample_idx])\n",
    "\n",
    "            self.log(\n",
    "                \"MelPredicted/train\",\n",
    "                self.global_step,\n",
    "                image=save_figure_to_numpy(\n",
    "                    plot_spectrogram(mel_out_postnet[sample_idx].data.cpu())\n",
    "                ),\n",
    "            )\n",
    "            self.log(\n",
    "                \"MelTarget/train\",\n",
    "                self.global_step,\n",
    "                image=save_figure_to_numpy(\n",
    "                    plot_spectrogram(mel_target[sample_idx].data.cpu())\n",
    "                ),\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "            for speaker_id in self.sample_inference_speaker_ids:\n",
    "                if self.distributed_run:\n",
    "                    self.sample_inference(\n",
    "                        model.module, self.sample_inference_text, speaker_id,\n",
    "                    )\n",
    "                else:\n",
    "                    self.sample_inference(\n",
    "                        model, self.sample_inference_text, speaker_id,\n",
    "                    )\n",
    "\n",
    "    def sample_inference(self, model, transcription=None, speaker_id=None):\n",
    "        if self.rank is not None and self.rank != 0:\n",
    "            return\n",
    "        # Generate an audio sample\n",
    "        with torch.no_grad():\n",
    "            if transcription is None:\n",
    "                transcription = random_utterance()\n",
    "\n",
    "            if self.compute_gst:\n",
    "                gst_embedding = self.compute_gst([transcription])\n",
    "                gst_embedding = torch.FloatTensor(gst_embedding).cuda()\n",
    "            else:\n",
    "                gst_embedding = None\n",
    "\n",
    "            utterance = torch.LongTensor(\n",
    "                text_to_sequence(\n",
    "                    transcription,\n",
    "                    self.text_cleaners,\n",
    "                    p_arpabet=self.p_arpabet,\n",
    "                    symbol_set=self.symbol_set,\n",
    "                )\n",
    "            )[None].cuda()\n",
    "\n",
    "            input_lengths = torch.LongTensor([utterance.shape[1]]).cuda()\n",
    "#             input_ = [\n",
    "#                 utterance,\n",
    "#                 input_lengths,\n",
    "#                 torch.LongTensor([speaker_id]).cuda(),\n",
    "#                 gst_embedding,\n",
    "#             ]\n",
    "            \n",
    "            input_ = Batch(text = utterance,\n",
    "                          input_lengths = input_lengths,\n",
    "                          speaker_ids = torch.LongTensor([speaker_id]).cuda(),\n",
    "                          gst = gst_embedding)\n",
    "        \n",
    "            model.eval()\n",
    "\n",
    "            output = model.inference(input_)\n",
    "\n",
    "            model.train()\n",
    "            try:\n",
    "                audio = self.sample(output.mel_outputs_postnet[0])\n",
    "                self.log(f\"SampleInference/{speaker_id}\", self.global_step, audio=audio)\n",
    "            except Exception as e:\n",
    "                print(f\"Exception raised while doing sample inference: {e}\")\n",
    "                print(\"Mel shape: \", output.mel_outputs_postnet[0].shape)\n",
    "            self.log(\n",
    "                f\"Attention/{speaker_id}/sample_inference\",\n",
    "                self.global_step,\n",
    "                image=save_figure_to_numpy(\n",
    "                    plot_attention(output.alignments[0].data.cpu().transpose(0, 1))\n",
    "                ),\n",
    "            )\n",
    "            self.log(\n",
    "                f\"MelPredicted/{speaker_id}/sample_inference\",\n",
    "                self.global_step,\n",
    "                image=save_figure_to_numpy(plot_spectrogram(output.mel_outputs_postnet[0].data.cpu())),\n",
    "            )\n",
    "            self.log(\n",
    "                f\"Gate/{speaker_id}/sample_inference\",\n",
    "                self.global_step,\n",
    "                image=save_figure_to_numpy(\n",
    "                    plot_gate_outputs(gate_outputs=output.gate_pred[0].data.cpu())\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    def log_validation(\n",
    "        self,\n",
    "        X,\n",
    "        y_pred,\n",
    "        y,\n",
    "        mean_loss,\n",
    "        mean_mel_loss,\n",
    "        mean_gate_loss,\n",
    "        batch_mel_loss,\n",
    "        batch_gate_loss,\n",
    "        speakers_val,\n",
    "    ):\n",
    "\n",
    "        self.log(\"Loss/val\", self.global_step, scalar=mean_loss)\n",
    "        self.log(\"MelLoss/val\", self.global_step, scalar=mean_mel_loss)\n",
    "        self.log(\"GateLoss/val\", self.global_step, scalar=mean_gate_loss)\n",
    "\n",
    "        val_levels = speakers_val\n",
    "        val_levels_unique = torch.unique(val_levels)\n",
    "        for lev in val_levels_unique:\n",
    "            mlv = (\n",
    "                batch_mel_loss[torch.where(val_levels == lev)[0]]\n",
    "                .mean()\n",
    "                .cpu()\n",
    "                .detach()\n",
    "                .numpy()\n",
    "            )\n",
    "            self.log(\n",
    "                f\"MelLoss/val/speaker{lev.item()}\", self.global_step, scalar=mlv,\n",
    "            )\n",
    "            if self.location_specific_attention:\n",
    "                glv = (\n",
    "                    batch_gate_loss[torch.where(val_levels == lev)[0]]\n",
    "                    .mean()\n",
    "                    .cpu()\n",
    "                    .detach()\n",
    "                    .numpy()\n",
    "                )\n",
    "                self.log(\n",
    "                    f\"GateLoss/val/speaker{lev.item()}\", self.global_step, scalar=glv,\n",
    "                )\n",
    "                lv = mlv + glv\n",
    "            # NOTE (Sam): add duration loss\n",
    "            if self.non_attentive:\n",
    "                lv = mlv\n",
    "            self.log(\n",
    "                f\"Loss/val/speaker{lev.item()}\", self.global_step, scalar=lv,\n",
    "            )\n",
    "        # Generate the sample from a random item from the last y_pred batch.\n",
    "        mel_target = y.mel_padded\n",
    "        gate_target = y.gate_target\n",
    "        alignments = y_pred.alignments\n",
    "        gate_pred = y_pred.gate_pred\n",
    "        mel_out_postnet = y_pred.mel_outputs_postnet\n",
    "        sample_idx = randint(0, mel_out_postnet.size(0) - 1)\n",
    "\n",
    "        if self.location_specific_attention:\n",
    "            alignment_metrics = get_alignment_metrics(alignments)\n",
    "            alignment_diagonalness = alignment_metrics[\"diagonalness\"]\n",
    "            alignment_max = alignment_metrics[\"max\"]\n",
    "            self.log(\n",
    "                \"AlignmentDiagonalness/val\",\n",
    "                self.global_step,\n",
    "                scalar=alignment_diagonalness,\n",
    "            )\n",
    "            self.log(\"AlignmentMax/val\", self.global_step, scalar=alignment_max)\n",
    "            self.log(\n",
    "                \"Gate/val\",\n",
    "                self.global_step,\n",
    "                image=save_figure_to_numpy(\n",
    "                    plot_gate_outputs(\n",
    "                        gate_targets=gate_target[sample_idx].data.cpu(),\n",
    "                        gate_outputs=gate_pred[sample_idx].data.cpu(),\n",
    "                    )\n",
    "                ),\n",
    "            )\n",
    "            input_length = X.input_lengths[sample_idx].item()\n",
    "            output_length = X.output_lengths[sample_idx].item()\n",
    "            self.log(\n",
    "                \"Attention/val\",\n",
    "                self.global_step,\n",
    "                image=save_figure_to_numpy(\n",
    "                    plot_attention(\n",
    "                        alignments[sample_idx].data.cpu().transpose(0, 1),\n",
    "                        encoder_length=input_length,\n",
    "                        decoder_length=output_length,\n",
    "                    )\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        audio = self.sample(mel=mel_out_postnet[sample_idx])\n",
    "\n",
    "        self.log(\"AudioSample/val\", self.global_step, audio=audio)\n",
    "        self.log(\n",
    "            \"MelPredicted/val\",\n",
    "            self.global_step,\n",
    "            image=save_figure_to_numpy(\n",
    "                plot_spectrogram(mel_out_postnet[sample_idx].data.cpu())\n",
    "            ),\n",
    "        )\n",
    "        self.log(\n",
    "            \"MelTarget/val\",\n",
    "            self.global_step,\n",
    "            image=save_figure_to_numpy(\n",
    "                plot_spectrogram(mel_target[sample_idx].data.cpu())\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def initialize_loader(\n",
    "        self,\n",
    "        include_f0: bool = False,\n",
    "        include_durations: bool = False,\n",
    "        n_frames_per_step: int = 1,\n",
    "    ):\n",
    "        train_set = TextMelDataset(\n",
    "            **self.training_dataset_args,\n",
    "            debug=self.debug,\n",
    "            debug_dataset_size=self.batch_size,\n",
    "        )\n",
    "        val_set = TextMelDataset(\n",
    "            **self.val_dataset_args,\n",
    "            debug=self.debug,\n",
    "            debug_dataset_size=self.batch_size,\n",
    "        )\n",
    "        collate_fn = TextMelCollate(\n",
    "            n_frames_per_step=n_frames_per_step,\n",
    "            include_f0=include_f0,\n",
    "            include_durations=include_durations,\n",
    "        )\n",
    "        sampler = None\n",
    "        if self.distributed_run:\n",
    "            self.init_distributed()\n",
    "            sampler = DistributedSampler(train_set, rank=self.rank)\n",
    "        train_loader = DataLoader(\n",
    "            train_set,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=(sampler is None),\n",
    "            sampler=sampler,\n",
    "            collate_fn=collate_fn,\n",
    "        )\n",
    "        return train_set, val_set, train_loader, sampler, collate_fn\n",
    "\n",
    "    def train(self):\n",
    "        train_start_time = time.perf_counter()\n",
    "        print(\"start train\", train_start_time)\n",
    "        print(\"campaign with this game vn1\")\n",
    "        train_set, val_set, train_loader, sampler, collate_fn = self.initialize_loader(\n",
    "            include_f0=self.include_f0, include_durations=self.include_durations\n",
    "        )\n",
    "        print(\"campaign with this game v0\")\n",
    "        criterion = Tacotron2Loss(\n",
    "            pos_weight=self.pos_weight,\n",
    "            non_attentive=self.non_attentive,\n",
    "            location_specific_attention=self.location_specific_attention,\n",
    "            sampling_rate=self.sampling_rate,\n",
    "            hop_length=self.hop_length,\n",
    "        )  # keep higher than 5 to make clips not stretch on\n",
    "        print(\"campaign with this game vp3\")\n",
    "        model = Tacotron2(self.hparams)\n",
    "        print(\"campaign with this game vp7\")\n",
    "        if self.device == \"cuda\":\n",
    "            model = model.cuda()\n",
    "        print(\"campaign with this game v1\")\n",
    "        if self.distributed_run:\n",
    "            #NOTE (Sam) find_unused_parameters reduces unreduced losses that we might have missed\n",
    "            model = DDP(model, device_ids=[self.rank], find_unused_parameters=True)\n",
    "        print(\"campaign with this game v2\")\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay,\n",
    "        )\n",
    "        start_epoch = 0\n",
    "\n",
    "        if self.warm_start_name:\n",
    "            if self.distributed_run:\n",
    "                module, optimizer, start_epoch = self.warm_start(\n",
    "                    model.module, optimizer\n",
    "                )\n",
    "                model.module = module\n",
    "            else:\n",
    "                model, optimizer, start_epoch = self.warm_start(model, optimizer)\n",
    "\n",
    "        if self.fp16_run:\n",
    "            scaler = GradScaler()\n",
    "        print(\"campaign with this game\")\n",
    "        start_time, previous_start_time = time.perf_counter(), time.perf_counter()\n",
    "        for epoch in range(start_epoch, self.epochs):\n",
    "            #             train_loader, sampler, collate_fn = self.adjust_frames_per_step(\n",
    "            #                 model, train_loader, sampler, collate_fn\n",
    "            #             )\n",
    "            if self.distributed_run:\n",
    "                sampler.set_epoch(epoch)\n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                previous_start_time = start_time\n",
    "                start_time = time.perf_counter()\n",
    "                self.global_step += 1\n",
    "                model.zero_grad()\n",
    "                if self.distributed_run:\n",
    "                    X, y = model.module.parse_batch(batch)\n",
    "                else:\n",
    "                    X, y = model.parse_batch(batch)\n",
    "                if self.fp16_run:\n",
    "                    with autocast():\n",
    "                        y_pred = model(X)\n",
    "                        if self.location_specific_attention:\n",
    "                            (\n",
    "                                loss,\n",
    "                                loss_batch,\n",
    "                                mel_loss,\n",
    "                                mel_loss_batch,\n",
    "                                gate_loss,\n",
    "                                gate_loss_batch,\n",
    "                            ) = criterion(y_pred, y)\n",
    "                        if self.non_attentive:\n",
    "                            (\n",
    "                                loss,\n",
    "                                loss_batch,\n",
    "                                mel_loss,\n",
    "                                mel_loss_batch,\n",
    "                                duration_loss,\n",
    "                                duration_loss_batch,\n",
    "                            ) = criterion(y_pred, y)\n",
    "                else:\n",
    "                    y_pred = model(X)\n",
    "                    if self.location_specific_attention:\n",
    "                        (\n",
    "                            loss,\n",
    "                            loss_batch,\n",
    "                            mel_loss,\n",
    "                            mel_loss_batch,\n",
    "                            gate_loss,\n",
    "                            gate_loss_batch,\n",
    "                        ) = criterion(y_pred, y)\n",
    "                    if self.non_attentive:\n",
    "                        (\n",
    "                            loss,\n",
    "                            loss_batch,\n",
    "                            mel_loss,\n",
    "                            mel_loss_batch,\n",
    "                            duration_loss,\n",
    "                            duration_loss_batch,\n",
    "                        ) = criterion(y_pred, y)\n",
    "                if self.distributed_run:\n",
    "                    reduced_mel_loss = reduce_tensor(mel_loss, self.world_size).item()\n",
    "                    reduced_mel_loss_batch = reduce_tensor(\n",
    "                        mel_loss_batch, self.world_size\n",
    "                    )\n",
    "                    if self.location_specific_attention:\n",
    "                        reduced_gate_loss = reduce_tensor(\n",
    "                            gate_loss, self.world_size\n",
    "                        ).item()\n",
    "                        reduced_gate_loss_batch = reduce_tensor(\n",
    "                            gate_loss_batch, self.world_size\n",
    "                        )\n",
    "                        reduced_loss = reduced_mel_loss + reduced_gate_loss\n",
    "                    if self.non_attentive:\n",
    "                        reduced_gate_loss = None\n",
    "                        reduced_gate_loss_batch = None\n",
    "                        reduced_duration_loss = reduce_tensor(\n",
    "                            duration_loss, self.world_size\n",
    "                        ).item()\n",
    "                        reduced_duration_loss_batch = reduce_tensor(\n",
    "                            duration_loss_batch, self.world_size\n",
    "                        ).item()\n",
    "                        reduced_loss = reduced_mel_loss + reduced_duration_loss\n",
    "                else:\n",
    "                    reduced_mel_loss = mel_loss  # .item()\n",
    "                    reduced_mel_loss_batch = mel_loss_batch.detach()\n",
    "                    if self.location_specific_attention:\n",
    "                        reduced_gate_loss = gate_loss.item()\n",
    "                        reduced_gate_loss_batch = gate_loss_batch.detach()\n",
    "                        reduced_loss = reduced_mel_loss + reduced_gate_loss\n",
    "                    if self.non_attentive:\n",
    "                        reduced_gate_loss = None\n",
    "                        reduced_gate_loss_batch = None\n",
    "                        reduced_duration_loss = duration_loss.item()\n",
    "                        reduced_duration_loss_batch = duration_loss_batch.detach()\n",
    "                        reduced_loss = reduced_mel_loss + reduced_duration_loss\n",
    "\n",
    "                if self.fp16_run:\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    grad_norm = torch.nn.utils.clip_grad_norm(\n",
    "                        model.parameters(), self.grad_clip_thresh\n",
    "                    )\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                    grad_norm = torch.nn.utils.clip_grad_norm(\n",
    "                        model.parameters(), self.grad_clip_thresh\n",
    "                    )\n",
    "                    optimizer.step()\n",
    "                step_duration_seconds = time.perf_counter() - start_time\n",
    "                log_start = time.time()\n",
    "                self.log_training(\n",
    "                    model,\n",
    "                    X,\n",
    "                    y_pred,\n",
    "                    y,\n",
    "                    reduced_loss,\n",
    "                    reduced_mel_loss,\n",
    "                    reduced_gate_loss,\n",
    "                    reduced_mel_loss_batch,\n",
    "                    reduced_gate_loss_batch,\n",
    "                    grad_norm,\n",
    "                    step_duration_seconds,\n",
    "                )\n",
    "                log_stop = time.time()\n",
    "                if self.location_specific_attention:\n",
    "                    log_str = f\"epoch: {epoch}/{self.epochs} | batch: {batch_idx}/{len(train_loader)} | loss: {reduced_loss:.2f} | mel: {reduced_mel_loss:.2f} | gate: {reduced_gate_loss:.3f} | t: {start_time - previous_start_time:.2f}s | w: {(time.perf_counter() - train_start_time)/(60*60):.2f}h\"\n",
    "                if self.non_attentive:\n",
    "                    log_str = f\"epoch: {epoch}/{self.epochs} | batch: {batch_idx}/{len(train_loader)} | loss: {reduced_loss:.2f} | mel: {reduced_mel_loss:.2f} | t: {start_time - previous_start_time:.2f}s | w: {(time.perf_counter() - train_start_time)/(60*60):.2f}h\"\n",
    "                if self.distributed_run:\n",
    "                    log_str += f\" | rank: {self.rank}\"\n",
    "                print(log_str)\n",
    "            if epoch % self.epochs_per_checkpoint == 0:\n",
    "                self.save_checkpoint(\n",
    "                    f\"tacotron2_{epoch}\",\n",
    "                    model=model,\n",
    "                    optimizer=optimizer,\n",
    "                    iteration=epoch,\n",
    "                    learning_rate=self.learning_rate,\n",
    "                    global_step=self.global_step,\n",
    "                )\n",
    "\n",
    "            # There's no need to validate in debug mode since we're not really training.\n",
    "            if self.debug:\n",
    "                continue\n",
    "            self.validate(\n",
    "                model=model,\n",
    "                val_set=val_set,\n",
    "                collate_fn=collate_fn,\n",
    "                criterion=criterion,\n",
    "            )\n",
    "\n",
    "    def validate(self, **kwargs):\n",
    "        val_start_time = time.perf_counter()\n",
    "\n",
    "        model = kwargs[\"model\"]\n",
    "        val_set = kwargs[\"val_set\"]\n",
    "        collate_fn = kwargs[\"collate_fn\"]\n",
    "        criterion = kwargs[\"criterion\"]\n",
    "        sampler = DistributedSampler(val_set) if self.distributed_run else None\n",
    "        (total_loss, total_mel_loss, total_gate_loss, total_duration_loss) = (\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "        )\n",
    "        total_steps = 0\n",
    "        model.eval()\n",
    "        speakers_val = []\n",
    "        batch_mel_loss = []\n",
    "        batch_gate_loss = []\n",
    "        batch_duration_loss = []\n",
    "        with torch.no_grad():\n",
    "            val_loader = DataLoader(\n",
    "                val_set,\n",
    "                sampler=sampler,\n",
    "                shuffle=False,\n",
    "                batch_size=self.batch_size,\n",
    "                collate_fn=collate_fn,\n",
    "            )\n",
    "            for batch in val_loader:\n",
    "                total_steps += 1\n",
    "                if self.distributed_run:\n",
    "                    X, y = model.module.parse_batch(batch)\n",
    "                    speakers_val.append(X.speaker_ids)\n",
    "                else:\n",
    "                    X, y = model.parse_batch(batch)\n",
    "                    speakers_val.append(X.speaker_ids)\n",
    "                y_pred = model(X)\n",
    "                if self.location_specific_attention:\n",
    "                    (\n",
    "                        loss,\n",
    "                        loss_batch,\n",
    "                        mel_loss,\n",
    "                        mel_loss_batch,\n",
    "                        gate_loss,\n",
    "                        gate_loss_batch,\n",
    "                    ) = criterion(y_pred, y)\n",
    "                if self.non_attentive:\n",
    "                    (\n",
    "                        loss,\n",
    "                        loss_batch,\n",
    "                        mel_loss,\n",
    "                        mel_loss_batch,\n",
    "                        duration_loss,\n",
    "                        duration_loss_batch,\n",
    "                    ) = criterion(y_pred, y)\n",
    "                if self.distributed_run:\n",
    "                    reduced_mel_loss = reduce_tensor(mel_loss, self.world_size).item()\n",
    "                    reduced_mel_loss_batch = reduce_tensor(\n",
    "                        mel_loss_batch, self.world_size\n",
    "                    )\n",
    "                    if self.location_specific_attention:\n",
    "                        reduced_gate_loss = reduce_tensor(\n",
    "                            gate_loss, self.world_size\n",
    "                        ).item()\n",
    "                        reduced_gate_loss_batch = reduce_tensor(\n",
    "                            gate_loss_batch, self.world_size\n",
    "                        ).detach()\n",
    "                        reduced_loss = reduced_mel_loss + reduced_gate_loss\n",
    "\n",
    "                    if self.non_attentive:\n",
    "                        reduced_duration_loss = reduce_tensor(\n",
    "                            duration_loss, self.world_size\n",
    "                        ).item()\n",
    "                        reduced_loss = reduced_mel_loss + reduced_duration_loss\n",
    "\n",
    "                else:\n",
    "                    reduced_mel_loss = mel_loss  # .item()\n",
    "                    reduced_mel_loss_batch = mel_loss_batch.detach()\n",
    "                    if self.location_specific_attention:\n",
    "                        reduced_gate_loss = gate_loss.item()\n",
    "                        reduced_gate_loss_batch = gate_loss_batch.detach()\n",
    "                    if self.non_attentive:\n",
    "                        reduced_duration_loss = duration_loss.item()\n",
    "                        reduced_duration_loss_batch = duration_loss_batch.detach()\n",
    "\n",
    "                batch_mel_loss.append(reduced_mel_loss_batch)\n",
    "                total_mel_loss += reduced_mel_loss\n",
    "                if self.location_specific_attention:\n",
    "                    batch_gate_loss.append(reduced_gate_loss_batch)\n",
    "                    reduced_val_loss = reduced_mel_loss + reduced_gate_loss\n",
    "                    total_gate_loss += reduced_gate_loss\n",
    "\n",
    "                if self.non_attentive:\n",
    "                    batch_duration_loss.append(reduced_duration_loss)\n",
    "                    reduced_val_loss = reduced_mel_loss + reduced_duration_loss\n",
    "                    total_duration_loss += reduced_duration_loss\n",
    "\n",
    "                total_loss += reduced_val_loss\n",
    "\n",
    "            mean_mel_loss = total_mel_loss / total_steps\n",
    "            if self.location_specific_attention:\n",
    "                mean_gate_loss = total_gate_loss / total_steps\n",
    "                batch_gate_loss = torch.hstack(batch_gate_loss)\n",
    "            else:\n",
    "                # NOTE (Sam): for convenience... really should have a namedtuple loss class\n",
    "                mean_gate_loss = torch.tensor([0]).float()\n",
    "                batch_gate_loss = torch.zeros((self.batch_size, 1)).float()\n",
    "            mean_loss = total_loss / total_steps\n",
    "            batch_mel_loss = torch.hstack(batch_mel_loss)\n",
    "\n",
    "            speakers_val = torch.hstack(speakers_val)\n",
    "            self.log_validation(\n",
    "                X,\n",
    "                y_pred,\n",
    "                y,\n",
    "                mean_loss,\n",
    "                mean_mel_loss,\n",
    "                mean_gate_loss,\n",
    "                batch_gate_loss,\n",
    "                batch_mel_loss,\n",
    "                speakers_val,\n",
    "            )\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        if self.location_specific_attention:\n",
    "            val_log_str = f\"Validation loss: {mean_loss:.2f} | mel: {mel_loss:.2f} | gate: {mean_gate_loss:.3f} | t: {time.perf_counter() - val_start_time:.2f}s\"\n",
    "        if self.non_attentive:\n",
    "            val_log_str = f\"Validation loss: {mean_loss:.2f} | mel: {mel_loss:.2f} |  t: {time.perf_counter() - val_start_time:.2f}s\"\n",
    "\n",
    "    @property\n",
    "    def val_dataset_args(self):\n",
    "\n",
    "        args = dict(**self.training_dataset_args)\n",
    "        args[\"audiopaths_and_text\"] = self.val_audiopaths_and_text\n",
    "        return args\n",
    "\n",
    "    @property\n",
    "    def training_dataset_args(self):\n",
    "        return {\n",
    "            \"audiopaths_and_text\": self.training_audiopaths_and_text,\n",
    "            \"text_cleaners\": self.text_cleaners,\n",
    "            \"p_arpabet\": self.p_arpabet,\n",
    "            \"n_mel_channels\": self.n_mel_channels,\n",
    "            \"sampling_rate\": self.sampling_rate,\n",
    "            \"mel_fmin\": self.mel_fmin,\n",
    "            \"mel_fmax\": self.mel_fmax,\n",
    "            \"filter_length\": self.filter_length,\n",
    "            \"hop_length\": self.hop_length,\n",
    "            \"win_length\": self.win_length,\n",
    "            \"symbol_set\": self.symbol_set,\n",
    "            \"max_wav_value\": self.max_wav_value,\n",
    "            \"pos_weight\": self.pos_weight,\n",
    "            \"compute_gst\": self.compute_gst,\n",
    "            \"include_durations\": self.include_durations,\n",
    "            \"compute_durations\": self.compute_durations,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3479476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from uberduck_ml_dev.vendor.tfcompat.hparam import HParams\n",
    "from uberduck_ml_dev.trainer.base import DEFAULTS as TRAINER_DEFAULTS\n",
    "from uberduck_ml_dev.models.tacotron2.tacotron2 import DEFAULTS as TACOTRON2_DEFAULTS\n",
    "\n",
    "config = TRAINER_DEFAULTS.values()\n",
    "config.update(TACOTRON2_DEFAULTS.values())\n",
    "DEFAULTS = HParams(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68222e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank = None\n",
    "# device_count = None\n",
    "# trainer = Tacotron2Trainer(DEFAULTS, rank=rank, world_size=device_count)\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a4fb8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTSTrainer start 73523.630250416\n",
      "Initializing trainer with hparams:\n",
      "{'attention_dim': 128,\n",
      " 'attention_location_kernel_size': 31,\n",
      " 'attention_location_n_filters': 32,\n",
      " 'attention_rnn_dim': 1024,\n",
      " 'batch_size': 16,\n",
      " 'checkpoint_name': None,\n",
      " 'checkpoint_path': 'test/fixtures/results/checkpoints',\n",
      " 'coarse_n_frames_per_step': None,\n",
      " 'compute_durations': False,\n",
      " 'cudnn_enabled': True,\n",
      " 'dataset_path': './dataset',\n",
      " 'debug': False,\n",
      " 'decay_rate': 8000,\n",
      " 'decay_start': 15000,\n",
      " 'decoder_rnn_dim': 1024,\n",
      " 'distributed_run': True,\n",
      " 'encoder_embedding_dim': 512,\n",
      " 'encoder_kernel_size': 5,\n",
      " 'encoder_n_convolutions': 3,\n",
      " 'epochs': 5,\n",
      " 'epochs_per_checkpoint': 4,\n",
      " 'filter_length': 1024,\n",
      " 'fp16_run': False,\n",
      " 'gate_threshold': 0.5,\n",
      " 'grad_clip_thresh': 1.0,\n",
      " 'gst_type': None,\n",
      " 'has_speaker_embedding': False,\n",
      " 'hop_length': 256,\n",
      " 'ignore_layers': ['speaker_embedding.weight'],\n",
      " 'include_durations': False,\n",
      " 'include_f0': False,\n",
      " 'learning_rate': 0.001,\n",
      " 'location_specific_attention': True,\n",
      " 'log_dir': 'test/fixtures/results/logs',\n",
      " 'mask_padding': True,\n",
      " 'max_decoder_steps': 1000,\n",
      " 'max_wav_value': 32768.0,\n",
      " 'mel_fmax': 8000,\n",
      " 'mel_fmin': 0,\n",
      " 'n_frames_per_step_initial': 1,\n",
      " 'n_mel_channels': 80,\n",
      " 'n_speakers': 1,\n",
      " 'n_symbols': 148,\n",
      " 'non_attentive': False,\n",
      " 'num_heads': 8,\n",
      " 'p_arpabet': 1.0,\n",
      " 'p_attention_dropout': 0.1,\n",
      " 'p_decoder_dropout': 0.1,\n",
      " 'p_teacher_forcing': 1.0,\n",
      " 'pos_weight': None,\n",
      " 'postnet_embedding_dim': 512,\n",
      " 'postnet_kernel_size': 5,\n",
      " 'postnet_n_convolutions': 5,\n",
      " 'prenet_dim': 256,\n",
      " 'prenet_f0_dim': 1,\n",
      " 'prenet_f0_kernel_size': 1,\n",
      " 'prenet_f0_n_layers': 1,\n",
      " 'prenet_fms_kernel_size': 1,\n",
      " 'prenet_rms_dim': 0,\n",
      " 'reduction_window_schedule': [{'batch_size': 16,\n",
      "                                'n_frames_per_step': 1,\n",
      "                                'until_step': 10000},\n",
      "                               {'batch_size': 16,\n",
      "                                'n_frames_per_step': 1,\n",
      "                                'until_step': 50000},\n",
      "                               {'batch_size': 16,\n",
      "                                'n_frames_per_step': 1,\n",
      "                                'until_step': 60000},\n",
      "                               {'batch_size': 16,\n",
      "                                'n_frames_per_step': 1,\n",
      "                                'until_step': 70000},\n",
      "                               {'batch_size': 16,\n",
      "                                'n_frames_per_step': 1,\n",
      "                                'until_step': None}],\n",
      " 'ref_enc_filters': [32, 32, 64, 64, 128, 128],\n",
      " 'ref_enc_gru_size': 128,\n",
      " 'ref_enc_pad': [1, 1],\n",
      " 'ref_enc_size': [3, 3],\n",
      " 'ref_enc_strides': [2, 2],\n",
      " 'sample_inference_speaker_ids': [0],\n",
      " 'sample_inference_text': 'That quick beige fox jumped in the air loudly over '\n",
      "                          'the thin dog fence.',\n",
      " 'sampling_rate': 22050,\n",
      " 'seed': 1234,\n",
      " 'speaker_embedding_dim': 128,\n",
      " 'steps_per_sample': 100,\n",
      " 'symbol_set': 'nvidia_taco2',\n",
      " 'symbols_embedding_dim': 512,\n",
      " 'text_cleaners': ['english_cleaners'],\n",
      " 'torchmoji_model_file': None,\n",
      " 'torchmoji_vocabulary_file': None,\n",
      " 'training_audiopaths_and_text': 'test/fixtures/ljtest/list.txt',\n",
      " 'val_audiopaths_and_text': 'test/fixtures/ljtest/list.txt',\n",
      " 'warm_start_name': 'test/fixtures/models/taco2ljdefault',\n",
      " 'weight_decay': 1e-06,\n",
      " 'win_length': 1024,\n",
      " 'with_gst': False}\n"
     ]
    }
   ],
   "source": [
    "# # skip\n",
    "# import json\n",
    "# import torch.distributed as dist\n",
    "\n",
    "# config = DEFAULTS.values()\n",
    "# config[\"batch_size\"] = 4\n",
    "# with open(\"test/fixtures/ljtest/taco2_lj2lj.json\") as f:\n",
    "#     config.update(json.load(f))\n",
    "# config[\"distributed_run\"] = True\n",
    "# hparams = HParams(**config)\n",
    "# rank = 0\n",
    "# device_count = 4\n",
    "# trainer = Tacotron2Trainer(hparams, rank=rank, world_size=device_count)\n",
    "# # debug DDP launching\n",
    "# self = trainer\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cc408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip\n",
    "\n",
    "import torch.distributed as dist\n",
    "\n",
    "dist.init_process_group(\n",
    "    \"nccl\", init_method=\"tcp://localhost:54321\", rank=1, world_size=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e26ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip\n",
    "\n",
    "import json\n",
    "\n",
    "config = DEFAULTS.values()\n",
    "config[\"batch_size\"] = 4\n",
    "with open(\"test/fixtures/ljtest/taco2_lj2lj.json\") as f:\n",
    "    config.update(json.load(f))\n",
    "config[\"distributed_run\"] = False\n",
    "hparams = HParams(**config)\n",
    "rank = 0\n",
    "device_count = 4\n",
    "trainer = Tacotron2Trainer(hparams, rank=rank, world_size=device_count)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b9e2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def initialize_loader(\n",
    "#     self,\n",
    "#     include_f0: bool = False,\n",
    "#     include_durations: bool = False,\n",
    "#     n_frames_per_step: int = 1,\n",
    "# ):\n",
    "#     train_set = TextMelDataset(\n",
    "#         **self.training_dataset_args,\n",
    "#         debug=self.debug,\n",
    "#         debug_dataset_size=self.batch_size,\n",
    "#     )\n",
    "#     val_set = TextMelDataset(\n",
    "#         **self.val_dataset_args, debug=self.debug, debug_dataset_size=self.batch_size,\n",
    "#     )\n",
    "#     collate_fn = TextMelCollate(\n",
    "#         n_frames_per_step=n_frames_per_step,\n",
    "#         include_f0=include_f0,\n",
    "#         include_durations=include_durations,\n",
    "#     )\n",
    "#     sampler = None\n",
    "#     if self.distributed_run:\n",
    "#         self.init_distributed()\n",
    "#         sampler = DistributedSampler(train_set, rank=self.rank)\n",
    "#     train_loader = DataLoader(\n",
    "#         train_set,\n",
    "#         batch_size=self.batch_size,\n",
    "#         shuffle=(sampler is None),\n",
    "#         sampler=sampler,\n",
    "#         collate_fn=collate_fn,\n",
    "#     )\n",
    "#     return train_set, val_set, train_loader, sampler, collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f646235",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uberduck",
   "language": "python",
   "name": "uberduck"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
