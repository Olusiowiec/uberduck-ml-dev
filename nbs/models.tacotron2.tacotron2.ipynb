{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fd67b94",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Example:-partial-teacher-forcing\" data-toc-modified-id=\"Example:-partial-teacher-forcing-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Example: partial teacher forcing</a></span></li><li><span><a href=\"#Example:-attention-guided-rhythm-transfer\" data-toc-modified-id=\"Example:-attention-guided-rhythm-transfer-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Example: attention-guided rhythm transfer</a></span></li><li><span><a href=\"#Example:-has_speaker_embedding\" data-toc-modified-id=\"Example:-has_speaker_embedding-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Example: has_speaker_embedding</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12e7a06",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35fc8726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.tacotron2.tacotron2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df468e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import pdb\n",
    "from torch import nn\n",
    "from uberduck_ml_dev.models.base import TTSModel\n",
    "from uberduck_ml_dev.models.common import Attention, Conv1d, LinearNorm, GST\n",
    "from uberduck_ml_dev.text.symbols import symbols\n",
    "from uberduck_ml_dev.vendor.tfcompat.hparam import HParams\n",
    "from uberduck_ml_dev.utils.utils import to_gpu, get_mask_from_lengths\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.nn import functional as F\n",
    "from uberduck_ml_dev.utils.duration import (\n",
    "    GaussianUpsampling,\n",
    "    RangePredictor,\n",
    "    PositionalEncoding,\n",
    "    DurationPredictor,\n",
    ")\n",
    "from uberduck_ml_dev.models.tacotron2.decoder import Decoder\n",
    "from uberduck_ml_dev.models.tacotron2.encoder import Encoder\n",
    "from uberduck_ml_dev.models.tacotron2.prenet import Prenet\n",
    "from uberduck_ml_dev.models.tacotron2.postnet import Postnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5805fc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from uberduck_ml_dev.data.batch import Batch\n",
    "\n",
    "\n",
    "class Tacotron2(TTSModel):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__(hparams)\n",
    "\n",
    "        self.mask_padding = hparams.mask_padding\n",
    "        self.fp16_run = hparams.fp16_run\n",
    "        self.pos_weight = hparams.pos_weight\n",
    "        self.n_mel_channels = hparams.n_mel_channels\n",
    "        self.n_frames_per_step_initial = hparams.n_frames_per_step_initial\n",
    "        self.n_frames_per_step_current = hparams.n_frames_per_step_initial\n",
    "        self.embedding = nn.Embedding(self.n_symbols, hparams.symbols_embedding_dim)\n",
    "        std = np.sqrt(2.0 / (self.n_symbols + hparams.symbols_embedding_dim))\n",
    "        val = np.sqrt(3.0) * std  # uniform bounds for std\n",
    "        self.embedding.weight.data.uniform_(-val, val)\n",
    "        self.encoder = Encoder(hparams)\n",
    "        self.decoder = Decoder(hparams)\n",
    "        self.postnet = Postnet(hparams)\n",
    "        self.speaker_embedding_dim = hparams.speaker_embedding_dim\n",
    "        self.encoder_embedding_dim = hparams.encoder_embedding_dim\n",
    "        self.has_speaker_embedding = hparams.has_speaker_embedding\n",
    "        self.cudnn_enabled = hparams.cudnn_enabled\n",
    "        self.non_attentive = hparams.non_attentive\n",
    "        self.location_specific_attention = hparams.location_specific_attention\n",
    "        if self.non_attentive:\n",
    "            self.duration_predictor = DurationPredictor(hparams)\n",
    "\n",
    "        if self.n_speakers > 1 and not self.has_speaker_embedding:\n",
    "            raise Exception(\"Speaker embedding is required if n_speakers > 1\")\n",
    "        if hparams.has_speaker_embedding:\n",
    "            self.speaker_embedding = nn.Embedding(\n",
    "                self.n_speakers, hparams.speaker_embedding_dim\n",
    "            )\n",
    "        else:\n",
    "            self.speaker_embedding = None\n",
    "        if self.n_speakers > 1:\n",
    "            self.spkr_lin = nn.Linear(\n",
    "                self.speaker_embedding_dim, self.encoder_embedding_dim\n",
    "            )\n",
    "        else:\n",
    "            self.spkr_lin = lambda a: torch.zeros(\n",
    "                self.encoder_embedding_dim, device=a.device\n",
    "            )\n",
    "\n",
    "        self.gst_init(hparams)\n",
    "\n",
    "    def gst_init(self, hparams):\n",
    "        self.gst_lin = None\n",
    "        self.gst_type = None\n",
    "\n",
    "        if hparams.get(\"gst_type\") == \"torchmoji\":\n",
    "            assert hparams.gst_dim, \"gst_dim must be set\"\n",
    "            self.gst_type = hparams.get(\"gst_type\")\n",
    "            self.gst_lin = nn.Linear(hparams.gst_dim, self.encoder_embedding_dim)\n",
    "            print(\"Initialized Torchmoji GST\")\n",
    "        else:\n",
    "            print(\"Not using any style tokens\")\n",
    "\n",
    "    def parse_batch(self, batch: Batch):\n",
    "\n",
    "        durations_padded = batch.durations_padded\n",
    "        text_int_padded = batch.text_int_padded\n",
    "        input_lengths = batch.input_lengths\n",
    "        mel_padded = batch.mel_padded\n",
    "        gate_target = batch.gate_target\n",
    "        output_lengths = batch.output_lengths\n",
    "        speaker_ids = batch.speaker_ids\n",
    "        embedded_gst = batch.gst\n",
    "        f0_padded = batch.f0_padded\n",
    "        gate_pred = batch.gate_pred\n",
    "\n",
    "        if self.cudnn_enabled:\n",
    "            text_int_padded = to_gpu(text_int_padded).long()\n",
    "            input_lengths = to_gpu(input_lengths).long()\n",
    "            mel_padded = to_gpu(mel_padded).float()\n",
    "            gate_target = to_gpu(gate_target).float()\n",
    "            speaker_ids = to_gpu(speaker_ids).long()\n",
    "            output_lengths = to_gpu(output_lengths).long()\n",
    "            gate_pred = to_gpu(output_lengths).long()\n",
    "            if durations_padded is not None:\n",
    "                durations_padded = to_gpu(durations_padded).long()\n",
    "            if embedded_gst is not None:\n",
    "                embedded_gst = to_gpu(embedded_gst).float()\n",
    "\n",
    "        # max_len = torch.max(input_lengths.data).item()\n",
    "        ret_x = Batch(\n",
    "            text_int_padded=text_int_padded,\n",
    "            input_lengths=input_lengths,\n",
    "            mel_padded=mel_padded,\n",
    "            gate_pred=gate_pred,\n",
    "            output_lengths=output_lengths,\n",
    "            speaker_ids=speaker_ids,\n",
    "            gst=embedded_gst,\n",
    "            durations_padded=durations_padded,\n",
    "            f0_padded=f0_padded,\n",
    "            # max_len=max_len,\n",
    "        )\n",
    "        if self.location_specific_attention:\n",
    "            ret_y = Batch(mel_padded=mel_padded, gate_target=gate_target)\n",
    "        if self.non_attentive:\n",
    "            ret_y = Batch(mel_padded=mel_padded, durations_padded=durations_padded)\n",
    "\n",
    "        return (ret_x, ret_y)\n",
    "\n",
    "    def parse_output(self, output_raw, output_lengths=None):\n",
    "\n",
    "        if self.mask_padding and output_lengths is not None:\n",
    "            mask = ~get_mask_from_lengths(output_lengths)\n",
    "            mask = mask.expand(self.n_mel_channels, mask.size(0), mask.size(1))\n",
    "            mask = F.pad(mask, (0, output_raw.mel_outputs.size(2) - mask.size(2)))\n",
    "            mask = mask.permute(1, 0, 2)\n",
    "\n",
    "            output_raw.mel_outputs.data.masked_fill_(mask, 0.0)\n",
    "            output_raw.mel_outputs_postnet.data.masked_fill_(mask, 0.0)\n",
    "            if self.location_specific_attention:\n",
    "                output_raw.gate_pred.data.masked_fill_(mask[:, 0, :], 1e3)\n",
    "\n",
    "        return output_raw\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_alignment(self, inputs):\n",
    "        (\n",
    "            input_text,\n",
    "            input_lengths,\n",
    "            targets,\n",
    "            max_len,\n",
    "            output_lengths,\n",
    "            speaker_ids,\n",
    "            *_,\n",
    "        ) = inputs\n",
    "\n",
    "        input_lengths, output_lengths = input_lengths.data, output_lengths.data\n",
    "\n",
    "        embedded_inputs = self.embedding(input_text).transpose(1, 2)\n",
    "        embedded_text = self.encoder(embedded_inputs, input_lengths)\n",
    "        encoder_outputs = embedded_text\n",
    "        if self.speaker_embedding:\n",
    "            embedded_speakers = self.speaker_embedding(speaker_ids)[:, None]\n",
    "            encoder_outputs += self.spkr_lin(embedded_speakers)\n",
    "\n",
    "        encoder_outputs = torch.cat((encoder_outputs,), dim=2)\n",
    "\n",
    "        mel_outputs, gate_pred, alignments = self.decoder(\n",
    "            encoder_outputs,\n",
    "            targets,\n",
    "            input_lengths=input_lengths,\n",
    "            encoder_output_lengths=input_lengths,\n",
    "        )\n",
    "        return alignments\n",
    "\n",
    "    def forward(self, inputs: Batch):\n",
    "        input_text = inputs.text_int_padded\n",
    "        input_lengths = inputs.input_lengths\n",
    "        targets = inputs.mel_padded\n",
    "        output_lengths = inputs.output_lengths\n",
    "        speaker_ids = inputs.speaker_ids\n",
    "        embedded_gst = inputs.gst\n",
    "        durations_padded = inputs.durations_padded\n",
    "        # max_len = inputs.max_len\n",
    "\n",
    "        input_lengths, output_lengths = input_lengths.data, output_lengths.data\n",
    "\n",
    "        embedded_inputs = self.embedding(input_text).transpose(1, 2)\n",
    "        embedded_text = self.encoder(embedded_inputs, input_lengths)\n",
    "        encoder_outputs = embedded_text\n",
    "        if self.speaker_embedding:\n",
    "            embedded_speakers = self.speaker_embedding(speaker_ids)[:, None]\n",
    "            encoder_outputs += self.spkr_lin(embedded_speakers)\n",
    "\n",
    "        if self.gst_lin is not None:\n",
    "            assert (\n",
    "                embedded_gst is not None\n",
    "            ), f\"embedded_gst is None but gst_type was set to {self.gst_type}\"\n",
    "            encoder_outputs += self.gst_lin(embedded_gst)\n",
    "        #         encoder_outputs = torch.cat((encoder_outputs,), dim=2)\n",
    "\n",
    "        if self.location_specific_attention:\n",
    "            mel_outputs, gate_pred, alignments = self.decoder(\n",
    "                encoder_outputs=encoder_outputs,\n",
    "                decoder_inputs=targets,\n",
    "                encoder_output_lengths=input_lengths,\n",
    "                # output_lengths=input_lengths,\n",
    "                output_lengths=output_lengths,\n",
    "            )\n",
    "\n",
    "        if self.non_attentive:\n",
    "            predicted_durations = self.decoder.duration_predictor(\n",
    "                encoder_outputs, input_lengths.cpu()\n",
    "            )\n",
    "            mel_outputs, predicted_durations = self.decoder(\n",
    "                encoder_outputs=encoder_outputs,\n",
    "                decoder_inputs=targets,\n",
    "                # output_lengths=input_lengths,\n",
    "                output_lengths=output_lengths,\n",
    "                input_lengths=input_lengths,\n",
    "                durations=durations_padded,\n",
    "            )\n",
    "\n",
    "        mel_outputs_postnet = self.postnet(mel_outputs)\n",
    "        mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
    "\n",
    "        if self.location_specific_attention:\n",
    "            output_raw = Batch(\n",
    "                mel_outputs=mel_outputs,\n",
    "                mel_outputs_postnet=mel_outputs_postnet,\n",
    "                gate_pred=gate_pred,\n",
    "                output_lengths=output_lengths,\n",
    "                alignments=alignments,\n",
    "            )\n",
    "\n",
    "        if self.non_attentive:\n",
    "            output_raw = Batch(\n",
    "                predicted_durations=predicted_durations,\n",
    "                mel_outputs=mel_outputs,\n",
    "                mel_outputs_postnet=mel_outputs_postnet,\n",
    "            )\n",
    "\n",
    "        output = self.parse_output(output_raw, output_lengths)\n",
    "        return output\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def inference(self, inputs):\n",
    "        text = inputs.text\n",
    "        input_lengths = inputs.input_lengths\n",
    "        speaker_ids = inputs.speaker_ids\n",
    "        embedded_gst = inputs.gst\n",
    "        # text, input_lengths, speaker_ids, embedded_gst, *_ = inputs\n",
    "\n",
    "        embedded_inputs = self.embedding(text).transpose(1, 2)\n",
    "        embedded_text = self.encoder.inference(embedded_inputs, input_lengths)\n",
    "        encoder_outputs = embedded_text\n",
    "        if self.speaker_embedding:\n",
    "            embedded_speakers = self.speaker_embedding(speaker_ids)[:, None]\n",
    "            encoder_outputs += self.spkr_lin(embedded_speakers)\n",
    "\n",
    "        if self.gst_lin is not None:\n",
    "            assert (\n",
    "                embedded_gst is not None\n",
    "            ), f\"embedded_gst is None but gst_type was set to {self.gst_type}\"\n",
    "            encoder_outputs += self.gst_lin(embedded_gst)\n",
    "        #         encoder_outputs = torch.cat((encoder_outputs,), dim=2)\n",
    "\n",
    "        mel_outputs, gate_pred, alignments, mel_lengths = self.decoder.inference(\n",
    "            encoder_outputs, input_lengths\n",
    "        )\n",
    "        mel_outputs_postnet = self.postnet(mel_outputs)\n",
    "        mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
    "\n",
    "        output_raw = Batch(\n",
    "            mel_outputs=mel_outputs,\n",
    "            mel_outputs_postnet=mel_outputs_postnet,\n",
    "            gate_pred=gate_pred,\n",
    "            alignments=alignments,\n",
    "            output_lengths=mel_lengths,\n",
    "        )\n",
    "        return self.parse_output(output_raw)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def inference_noattention(self, inputs):\n",
    "        \"\"\"Run inference conditioned on an attention map.\"\"\"\n",
    "        text, input_lengths, speaker_ids, attention_maps = inputs\n",
    "        embedded_inputs = self.embedding(text).transpose(1, 2)\n",
    "        embedded_text = self.encoder.inference(embedded_inputs, input_lengths)\n",
    "\n",
    "        encoder_outputs = torch.cat((embedded_text,), dim=2)\n",
    "\n",
    "        mel_outputs, gate_pred, alignments = self.decoder.inference_noattention(\n",
    "            encoder_outputs, attention_maps\n",
    "        )\n",
    "        mel_outputs_postnet = self.postnet(mel_outputs)\n",
    "        mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
    "\n",
    "        return self.parse_output(\n",
    "            [mel_outputs, mel_outputs_postnet, gate_pred, alignments]\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def inference_partial_tf(\n",
    "        self, inputs, tf_mel, tf_until_idx,\n",
    "    ):\n",
    "        \"\"\"Run inference with partial teacher forcing.\n",
    "\n",
    "        Teacher forcing is done until tf_until_idx in the mel spectrogram.\n",
    "        Make sure you pass the mel index and not the text index!\n",
    "\n",
    "        tf_mel: (B, T, n_mel_channels)\n",
    "        \"\"\"\n",
    "        text, input_lengths, *_ = inputs\n",
    "        embedded_inputs = self.embedding(text).transpose(1, 2)\n",
    "        embedded_text = self.encoder.inference(embedded_inputs, input_lengths)\n",
    "        encoder_outputs = torch.cat((embedded_text,), dim=2)\n",
    "\n",
    "        mel_outputs, gate_pred, alignments = self.decoder.inference_partial_tf(\n",
    "            encoder_outputs, tf_mel, tf_until_idx,\n",
    "        )\n",
    "\n",
    "        mel_outputs_postnet = self.postnet(mel_outputs)\n",
    "        mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
    "\n",
    "        return self.parse_output(\n",
    "            [mel_outputs, mel_outputs_postnet, gate_pred, alignments]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "177770ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from uberduck_ml_dev.vendor.tfcompat.hparam import HParams\n",
    "from uberduck_ml_dev.models.base import DEFAULTS as MODEL_DEFAULTS\n",
    "\n",
    "DEFAULTS = HParams(\n",
    "    symbols_embedding_dim=512,\n",
    "    fp16_run=False,\n",
    "    mask_padding=True,\n",
    "    n_mel_channels=80,\n",
    "    # encoder parameters\n",
    "    encoder_kernel_size=5,\n",
    "    encoder_n_convolutions=3,\n",
    "    encoder_embedding_dim=512,\n",
    "    # decoder parameters\n",
    "    coarse_n_frames_per_step=None,\n",
    "    decoder_rnn_dim=1024,\n",
    "    prenet_dim=256,\n",
    "    prenet_f0_n_layers=1,\n",
    "    prenet_f0_dim=1,\n",
    "    prenet_f0_kernel_size=1,\n",
    "    prenet_rms_dim=0,\n",
    "    prenet_fms_kernel_size=1,\n",
    "    max_decoder_steps=1000,\n",
    "    gate_threshold=0.5,\n",
    "    p_attention_dropout=0.1,\n",
    "    p_decoder_dropout=0.1,\n",
    "    p_teacher_forcing=1.0,\n",
    "    pos_weight=None,\n",
    "    # attention parameters\n",
    "    attention_rnn_dim=1024,\n",
    "    attention_dim=128,\n",
    "    # location layer parameters\n",
    "    attention_location_n_filters=32,\n",
    "    attention_location_kernel_size=31,\n",
    "    # mel post-processing network parameters\n",
    "    postnet_embedding_dim=512,\n",
    "    postnet_kernel_size=5,\n",
    "    postnet_n_convolutions=5,\n",
    "    n_speakers=1,\n",
    "    speaker_embedding_dim=128,\n",
    "    # reference encoder\n",
    "    with_gst=False,\n",
    "    ref_enc_filters=[32, 32, 64, 64, 128, 128],\n",
    "    ref_enc_size=[3, 3],\n",
    "    ref_enc_strides=[2, 2],\n",
    "    ref_enc_pad=[1, 1],\n",
    "    filter_length=1024,\n",
    "    hop_length=256,\n",
    "    include_f0=False,\n",
    "    ref_enc_gru_size=128,\n",
    "    symbol_set=\"nvidia_taco2\",\n",
    "    num_heads=8,\n",
    "    text_cleaners=[\"english_cleaners\"],\n",
    "    sampling_rate=22050,\n",
    "    checkpoint_name=None,\n",
    "    max_wav_value=32768.0,\n",
    "    mel_fmax=8000,\n",
    "    mel_fmin=0,\n",
    "    n_frames_per_step_initial=1,\n",
    "    win_length=1024,\n",
    "    has_speaker_embedding=False,\n",
    "    gst_type=None,\n",
    "    torchmoji_model_file=None,\n",
    "    torchmoji_vocabulary_file=None,\n",
    "    sample_inference_speaker_ids=None,\n",
    "    sample_inference_text=\"That quick beige fox jumped in the air loudly over the thin dog fence.\",\n",
    "    distributed_run=False,\n",
    "    cudnn_enabled=False,\n",
    "    # compute_durations=False,\n",
    "    non_attentive=False,\n",
    "    location_specific_attention=True,\n",
    "    include_durations=False,\n",
    "    compute_durations=False,\n",
    ")\n",
    "\n",
    "config = DEFAULTS.values()\n",
    "config.update(MODEL_DEFAULTS.values())\n",
    "DEFAULTS = HParams(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d9eca11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from uberduck_ml_dev.vendor.tfcompat.hparam import HParams\n",
    "from uberduck_ml_dev.models.base import DEFAULTS as MODEL_DEFAULTS\n",
    "\n",
    "config = DEFAULTS.values()\n",
    "config.update(\n",
    "    dict(\n",
    "        # compute_durations=True,\n",
    "        non_attentive=True,\n",
    "        positional_embedding_dim=32,\n",
    "        range_lstm_dim=1024,\n",
    "        duration_lstm_dim=1024,\n",
    "        location_specific_attention=False,\n",
    "        cudnn_enabled=True,\n",
    "        include_durations=True,\n",
    "        compute_durations=True,\n",
    "        decoder_rnn_dim_nlayers=2,\n",
    "    )\n",
    ")\n",
    "config.update(MODEL_DEFAULTS.values())\n",
    "NON_ATTENTIVE_DEFAULTS = HParams(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3d4316b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from uberduck_ml_dev.trainer.tacotron2 import Tacotron2Trainer\n",
    "import json\n",
    "from uberduck_ml_dev.vendor.tfcompat.hparam import HParams\n",
    "\n",
    "config = NON_ATTENTIVE_DEFAULTS.values()\n",
    "with open(\"test/fixtures/ljtest/taco2_lj2lj.json\") as f:\n",
    "    config.update(json.load(f))\n",
    "config[\"speaker_embedding_dim\"] = 1\n",
    "config[\"decoder_rnn_dim_nlayers\"] = 2\n",
    "config[\"ignore_layers\"] = [\n",
    "    \"speaker_embedding.weight\",\n",
    "    \"decoder.attention_rnn.weight_ih\",\n",
    "    \"decoder.attention_rnn.weight_hh\",\n",
    "    \"decoder.attention_rnn.bias_ih\",\n",
    "    \"decoder.attention_rnn.bias_hh\",\n",
    "    \"decoder.attention_layer.query_layer.linear_layer.weight\",\n",
    "    \"decoder.attention_layer.memory_layer.linear_layer.weight\",\n",
    "    \"decoder.attention_layer.v.linear_layer.weight\",\n",
    "    \"decoder.attention_layer.location_layer.location_conv.conv.weight\",\n",
    "    \"decoder.attention_layer.location_layer.location_dense.linear_layer.weight\",\n",
    "    \"decoder.decoder_rnn.weight_ih\",\n",
    "    \"decoder.decoder_rnn.weight_hh\",\n",
    "    \"decoder.decoder_rnn.bias_ih\",\n",
    "    \"decoder.decoder_rnn.bias_hh\",\n",
    "    \"decoder.linear_projection.linear_layer.weight\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b758f657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['speaker_embedding.weight',\n",
       " 'decoder.attention_rnn.weight_ih',\n",
       " 'decoder.attention_rnn.weight_hh',\n",
       " 'decoder.attention_rnn.bias_ih',\n",
       " 'decoder.attention_rnn.bias_hh',\n",
       " 'decoder.attention_layer.query_layer.linear_layer.weight',\n",
       " 'decoder.attention_layer.memory_layer.linear_layer.weight',\n",
       " 'decoder.attention_layer.v.linear_layer.weight',\n",
       " 'decoder.attention_layer.location_layer.location_conv.conv.weight',\n",
       " 'decoder.attention_layer.location_layer.location_dense.linear_layer.weight',\n",
       " 'decoder.decoder_rnn.weight_ih',\n",
       " 'decoder.decoder_rnn.weight_hh',\n",
       " 'decoder.decoder_rnn.bias_ih',\n",
       " 'decoder.decoder_rnn.bias_hh',\n",
       " 'decoder.linear_projection.linear_layer.weight']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams = HParams(**config)\n",
    "hparams.ignore_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e53499c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using any style tokens\n",
      "WARNING! Attempting to load a model with out the decoder.range_predictor.lstm.weight_ih_l0 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.range_predictor.lstm.weight_hh_l0 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.range_predictor.lstm.bias_ih_l0 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.range_predictor.lstm.bias_hh_l0 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.range_predictor.lstm.weight_ih_l0_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.range_predictor.lstm.weight_hh_l0_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.range_predictor.lstm.bias_ih_l0_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.range_predictor.lstm.bias_hh_l0_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.range_predictor.lstm.weight_ih_l1 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.range_predictor.lstm.weight_hh_l1 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.range_predictor.lstm.bias_ih_l1 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.range_predictor.lstm.bias_hh_l1 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.range_predictor.lstm.weight_ih_l1_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.range_predictor.lstm.weight_hh_l1_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.range_predictor.lstm.bias_ih_l1_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.range_predictor.lstm.bias_hh_l1_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.range_predictor.proj.linear_layer.weight layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.range_predictor.proj.linear_layer.bias layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.duration_predictor.lstm.weight_ih_l0 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.duration_predictor.lstm.weight_hh_l0 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.duration_predictor.lstm.bias_ih_l0 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.duration_predictor.lstm.bias_hh_l0 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.duration_predictor.lstm.weight_ih_l0_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.duration_predictor.lstm.weight_hh_l0_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.duration_predictor.lstm.bias_ih_l0_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.duration_predictor.lstm.bias_hh_l0_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.duration_predictor.lstm.weight_ih_l1 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.duration_predictor.lstm.weight_hh_l1 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.duration_predictor.lstm.bias_ih_l1 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.duration_predictor.lstm.bias_hh_l1 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.duration_predictor.lstm.weight_ih_l1_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.duration_predictor.lstm.weight_hh_l1_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.duration_predictor.lstm.bias_ih_l1_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.duration_predictor.lstm.bias_hh_l1_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.duration_predictor.proj.linear_layer.weight layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.duration_predictor.proj.linear_layer.bias layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.linear_projection.linear_layer.weight layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.positional_embedding.pe layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.decoder_rnn.weight_ih_l0 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.decoder_rnn.weight_hh_l0 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.decoder_rnn.bias_ih_l0 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.decoder_rnn.bias_hh_l0 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.decoder_rnn.weight_ih_l1 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.decoder_rnn.weight_hh_l1 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.decoder_rnn.bias_ih_l1 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the decoder.decoder_rnn.bias_hh_l1 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the duration_predictor.lstm.weight_ih_l0 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the duration_predictor.lstm.weight_hh_l0 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the duration_predictor.lstm.bias_ih_l0 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the duration_predictor.lstm.bias_hh_l0 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the duration_predictor.lstm.weight_ih_l0_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the duration_predictor.lstm.weight_hh_l0_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the duration_predictor.lstm.bias_ih_l0_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the duration_predictor.lstm.bias_hh_l0_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the duration_predictor.lstm.weight_ih_l1 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the duration_predictor.lstm.weight_hh_l1 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the duration_predictor.lstm.bias_ih_l1 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the duration_predictor.lstm.bias_hh_l1 layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the duration_predictor.lstm.weight_ih_l1_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the duration_predictor.lstm.weight_hh_l1_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the duration_predictor.lstm.bias_ih_l1_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the duration_predictor.lstm.bias_hh_l1_reverse layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the duration_predictor.proj.linear_layer.weight layer. This could lead to unexpected results during evaluation.\n",
      "WARNING! Attempting to load a model with out the duration_predictor.proj.linear_layer.bias layer. This could lead to unexpected results during evaluation.\n"
     ]
    }
   ],
   "source": [
    "tt2 = Tacotron2(hparams)\n",
    "tt2.from_pretrained(\n",
    "    warm_start_path=hparams.warm_start_name, ignore_layers=hparams.ignore_layers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1922130d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using any style tokens\n",
      "TTSTrainer start 26983.672709655\n",
      "Initializing trainer with hparams:\n",
      "{'attention_dim': 128,\n",
      " 'attention_location_kernel_size': 31,\n",
      " 'attention_location_n_filters': 32,\n",
      " 'attention_rnn_dim': 1024,\n",
      " 'batch_size': 16,\n",
      " 'checkpoint_name': None,\n",
      " 'checkpoint_path': 'test/fixtures/results/checkpoints',\n",
      " 'coarse_n_frames_per_step': None,\n",
      " 'compute_durations': True,\n",
      " 'cudnn_enabled': True,\n",
      " 'dataset_path': './dataset',\n",
      " 'debug': False,\n",
      " 'decoder_rnn_dim': 1024,\n",
      " 'decoder_rnn_dim_nlayers': 2,\n",
      " 'distributed_run': False,\n",
      " 'duration_lstm_dim': 1024,\n",
      " 'encoder_embedding_dim': 512,\n",
      " 'encoder_kernel_size': 5,\n",
      " 'encoder_n_convolutions': 3,\n",
      " 'epochs': 5,\n",
      " 'epochs_per_checkpoint': 4,\n",
      " 'filter_length': 1024,\n",
      " 'fp16_run': False,\n",
      " 'gate_threshold': 0.5,\n",
      " 'grad_clip_thresh': 1.0,\n",
      " 'gst_type': None,\n",
      " 'has_speaker_embedding': False,\n",
      " 'hop_length': 256,\n",
      " 'ignore_layers': ['speaker_embedding.weight',\n",
      "                   'decoder.attention_rnn.weight_ih',\n",
      "                   'decoder.attention_rnn.weight_hh',\n",
      "                   'decoder.attention_rnn.bias_ih',\n",
      "                   'decoder.attention_rnn.bias_hh',\n",
      "                   'decoder.attention_layer.query_layer.linear_layer.weight',\n",
      "                   'decoder.attention_layer.memory_layer.linear_layer.weight',\n",
      "                   'decoder.attention_layer.v.linear_layer.weight',\n",
      "                   'decoder.attention_layer.location_layer.location_conv.conv.weight',\n",
      "                   'decoder.attention_layer.location_layer.location_dense.linear_layer.weight',\n",
      "                   'decoder.decoder_rnn.weight_ih',\n",
      "                   'decoder.decoder_rnn.weight_hh',\n",
      "                   'decoder.decoder_rnn.bias_ih',\n",
      "                   'decoder.decoder_rnn.bias_hh',\n",
      "                   'decoder.linear_projection.linear_layer.weight'],\n",
      " 'include_durations': True,\n",
      " 'include_f0': False,\n",
      " 'learning_rate': 0.001,\n",
      " 'location_specific_attention': False,\n",
      " 'log_dir': 'test/fixtures/results/logs',\n",
      " 'mask_padding': True,\n",
      " 'max_decoder_steps': 1000,\n",
      " 'max_wav_value': 32768.0,\n",
      " 'mel_fmax': 8000,\n",
      " 'mel_fmin': 0,\n",
      " 'n_frames_per_step_initial': 1,\n",
      " 'n_mel_channels': 80,\n",
      " 'n_speakers': 1,\n",
      " 'n_symbols': 148,\n",
      " 'non_attentive': True,\n",
      " 'num_heads': 8,\n",
      " 'p_arpabet': 1.0,\n",
      " 'p_attention_dropout': 0.1,\n",
      " 'p_decoder_dropout': 0.1,\n",
      " 'p_teacher_forcing': 1.0,\n",
      " 'pos_weight': None,\n",
      " 'positional_embedding_dim': 32,\n",
      " 'postnet_embedding_dim': 512,\n",
      " 'postnet_kernel_size': 5,\n",
      " 'postnet_n_convolutions': 5,\n",
      " 'prenet_dim': 256,\n",
      " 'prenet_f0_dim': 1,\n",
      " 'prenet_f0_kernel_size': 1,\n",
      " 'prenet_f0_n_layers': 1,\n",
      " 'prenet_fms_kernel_size': 1,\n",
      " 'prenet_rms_dim': 0,\n",
      " 'range_lstm_dim': 1024,\n",
      " 'reduction_window_schedule': [{'batch_size': 16,\n",
      "                                'n_frames_per_step': 1,\n",
      "                                'until_step': 10000},\n",
      "                               {'batch_size': 16,\n",
      "                                'n_frames_per_step': 1,\n",
      "                                'until_step': 50000},\n",
      "                               {'batch_size': 16,\n",
      "                                'n_frames_per_step': 1,\n",
      "                                'until_step': 60000},\n",
      "                               {'batch_size': 16,\n",
      "                                'n_frames_per_step': 1,\n",
      "                                'until_step': 70000},\n",
      "                               {'batch_size': 16,\n",
      "                                'n_frames_per_step': 1,\n",
      "                                'until_step': None}],\n",
      " 'ref_enc_filters': [32, 32, 64, 64, 128, 128],\n",
      " 'ref_enc_gru_size': 128,\n",
      " 'ref_enc_pad': [1, 1],\n",
      " 'ref_enc_size': [3, 3],\n",
      " 'ref_enc_strides': [2, 2],\n",
      " 'sample_inference_speaker_ids': [0],\n",
      " 'sample_inference_text': 'That quick beige fox jumped in the air loudly over '\n",
      "                          'the thin dog fence.',\n",
      " 'sampling_rate': 22050,\n",
      " 'seed': 1234,\n",
      " 'speaker_embedding_dim': 1,\n",
      " 'steps_per_sample': 100,\n",
      " 'symbol_set': 'nvidia_taco2',\n",
      " 'symbols_embedding_dim': 512,\n",
      " 'text_cleaners': ['english_cleaners'],\n",
      " 'torchmoji_model_file': None,\n",
      " 'torchmoji_vocabulary_file': None,\n",
      " 'training_audiopaths_and_text': 'test/fixtures/ljtest/list.txt',\n",
      " 'val_audiopaths_and_text': 'test/fixtures/ljtest/list.txt',\n",
      " 'warm_start_name': 'test/fixtures/models/taco2ljdefault',\n",
      " 'weight_decay': 1e-06,\n",
      " 'win_length': 1024,\n",
      " 'with_gst': False}\n",
      "[NeMo I 2022-02-22 06:42:13 cloud:56] Found existing object /home/s_uberduck_ai/.cache/torch/NeMo/NeMo_1.7.0rc0/qn5x5_libri_tts_phonemes/656c7439dd3a0d614978529371be498b/qn5x5_libri_tts_phonemes.nemo.\n",
      "[NeMo I 2022-02-22 06:42:13 cloud:62] Re-using file from: /home/s_uberduck_ai/.cache/torch/NeMo/NeMo_1.7.0rc0/qn5x5_libri_tts_phonemes/656c7439dd3a0d614978529371be498b/qn5x5_libri_tts_phonemes.nemo\n",
      "[NeMo I 2022-02-22 06:42:13 common:704] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-02-22 06:42:13 features:232] Using torch_stft is deprecated and will be removed in 1.1.0. Please set stft_conv and stft_exact_pad to False for FilterbankFeatures and AudioToMelSpectrogramPreprocessor. Please set exact_pad to True as needed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-02-22 06:42:13 features:255] PADDING: 1\n",
      "[NeMo I 2022-02-22 06:42:13 features:265] STFT using conv\n",
      "[NeMo I 2022-02-22 06:42:14 save_restore_connector:157] Model EncDecCTCModel was successfully restored from /home/s_uberduck_ai/.cache/torch/NeMo/NeMo_1.7.0rc0/qn5x5_libri_tts_phonemes/656c7439dd3a0d614978529371be498b/qn5x5_libri_tts_phonemes.nemo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo E 2022-02-22 06:42:14 vocabs:324] Torch distributed needs to be initialized before you initialized <nemo.collections.common.data.vocabs.Phonemes object at 0x7fc2289b19d0>. This class is prone to data access race conditions. Now downloading corpora from global rank 0. If other ranks pass this before rank 0, errors might result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-02-22 06:42:14 cloud:56] Found existing object /home/s_uberduck_ai/.cache/torch/NeMo/NeMo_1.7.0rc0/qn5x5_libri_tts_phonemes/656c7439dd3a0d614978529371be498b/qn5x5_libri_tts_phonemes.nemo.\n",
      "[NeMo I 2022-02-22 06:42:14 cloud:62] Re-using file from: /home/s_uberduck_ai/.cache/torch/NeMo/NeMo_1.7.0rc0/qn5x5_libri_tts_phonemes/656c7439dd3a0d614978529371be498b/qn5x5_libri_tts_phonemes.nemo\n",
      "[NeMo I 2022-02-22 06:42:14 common:704] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-02-22 06:42:15 features:232] Using torch_stft is deprecated and will be removed in 1.1.0. Please set stft_conv and stft_exact_pad to False for FilterbankFeatures and AudioToMelSpectrogramPreprocessor. Please set exact_pad to True as needed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-02-22 06:42:15 features:255] PADDING: 1\n",
      "[NeMo I 2022-02-22 06:42:15 features:265] STFT using conv\n",
      "[NeMo I 2022-02-22 06:42:15 save_restore_connector:157] Model EncDecCTCModel was successfully restored from /home/s_uberduck_ai/.cache/torch/NeMo/NeMo_1.7.0rc0/qn5x5_libri_tts_phonemes/656c7439dd3a0d614978529371be498b/qn5x5_libri_tts_phonemes.nemo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo E 2022-02-22 06:42:15 vocabs:324] Torch distributed needs to be initialized before you initialized <nemo.collections.common.data.vocabs.Phonemes object at 0x7fc204077b20>. This class is prone to data access race conditions. Now downloading corpora from global rank 0. If other ranks pass this before rank 0, errors might result.\n"
     ]
    }
   ],
   "source": [
    "model = Tacotron2(hparams)\n",
    "if torch.cuda.is_available() and hparams.cudnn_enabled:\n",
    "    model.cuda()\n",
    "trainer = Tacotron2Trainer(hparams, rank=0, world_size=0)\n",
    "train_set, val_set, train_loader, sampler, collate_fn = trainer.initialize_loader(\n",
    "    include_durations=hparams.include_durations\n",
    ")\n",
    "batch = next(enumerate(train_loader))[1]\n",
    "X, y = model.parse_batch(batch)\n",
    "forward_output = model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b08d0fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using any style tokens\n",
      "TTSTrainer start 27050.619226975\n",
      "Initializing trainer with hparams:\n",
      "{'attention_dim': 128,\n",
      " 'attention_location_kernel_size': 31,\n",
      " 'attention_location_n_filters': 32,\n",
      " 'attention_rnn_dim': 1024,\n",
      " 'batch_size': 16,\n",
      " 'checkpoint_name': None,\n",
      " 'checkpoint_path': 'test/fixtures/results/checkpoints',\n",
      " 'coarse_n_frames_per_step': None,\n",
      " 'compute_durations': True,\n",
      " 'cudnn_enabled': True,\n",
      " 'dataset_path': './dataset',\n",
      " 'debug': False,\n",
      " 'decoder_rnn_dim': 1024,\n",
      " 'decoder_rnn_dim_nlayers': 2,\n",
      " 'distributed_run': True,\n",
      " 'duration_lstm_dim': 1024,\n",
      " 'encoder_embedding_dim': 512,\n",
      " 'encoder_kernel_size': 5,\n",
      " 'encoder_n_convolutions': 3,\n",
      " 'epochs': 5,\n",
      " 'epochs_per_checkpoint': 4,\n",
      " 'filter_length': 1024,\n",
      " 'fp16_run': False,\n",
      " 'gate_threshold': 0.5,\n",
      " 'grad_clip_thresh': 1.0,\n",
      " 'gst_type': None,\n",
      " 'has_speaker_embedding': False,\n",
      " 'hop_length': 256,\n",
      " 'ignore_layers': ['speaker_embedding.weight',\n",
      "                   'decoder.attention_rnn.weight_ih',\n",
      "                   'decoder.attention_rnn.weight_hh',\n",
      "                   'decoder.attention_rnn.bias_ih',\n",
      "                   'decoder.attention_rnn.bias_hh',\n",
      "                   'decoder.attention_layer.query_layer.linear_layer.weight',\n",
      "                   'decoder.attention_layer.memory_layer.linear_layer.weight',\n",
      "                   'decoder.attention_layer.v.linear_layer.weight',\n",
      "                   'decoder.attention_layer.location_layer.location_conv.conv.weight',\n",
      "                   'decoder.attention_layer.location_layer.location_dense.linear_layer.weight',\n",
      "                   'decoder.decoder_rnn.weight_ih',\n",
      "                   'decoder.decoder_rnn.weight_hh',\n",
      "                   'decoder.decoder_rnn.bias_ih',\n",
      "                   'decoder.decoder_rnn.bias_hh',\n",
      "                   'decoder.linear_projection.linear_layer.weight'],\n",
      " 'include_durations': True,\n",
      " 'include_f0': False,\n",
      " 'learning_rate': 0.001,\n",
      " 'location_specific_attention': False,\n",
      " 'log_dir': 'test/fixtures/results/logs',\n",
      " 'mask_padding': True,\n",
      " 'max_decoder_steps': 1000,\n",
      " 'max_wav_value': 32768.0,\n",
      " 'mel_fmax': 8000,\n",
      " 'mel_fmin': 0,\n",
      " 'n_frames_per_step_initial': 1,\n",
      " 'n_mel_channels': 80,\n",
      " 'n_speakers': 1,\n",
      " 'n_symbols': 148,\n",
      " 'non_attentive': True,\n",
      " 'num_heads': 8,\n",
      " 'p_arpabet': 1.0,\n",
      " 'p_attention_dropout': 0.1,\n",
      " 'p_decoder_dropout': 0.1,\n",
      " 'p_teacher_forcing': 1.0,\n",
      " 'pos_weight': None,\n",
      " 'positional_embedding_dim': 32,\n",
      " 'postnet_embedding_dim': 512,\n",
      " 'postnet_kernel_size': 5,\n",
      " 'postnet_n_convolutions': 5,\n",
      " 'prenet_dim': 256,\n",
      " 'prenet_f0_dim': 1,\n",
      " 'prenet_f0_kernel_size': 1,\n",
      " 'prenet_f0_n_layers': 1,\n",
      " 'prenet_fms_kernel_size': 1,\n",
      " 'prenet_rms_dim': 0,\n",
      " 'range_lstm_dim': 1024,\n",
      " 'reduction_window_schedule': [{'batch_size': 16,\n",
      "                                'n_frames_per_step': 1,\n",
      "                                'until_step': 10000},\n",
      "                               {'batch_size': 16,\n",
      "                                'n_frames_per_step': 1,\n",
      "                                'until_step': 50000},\n",
      "                               {'batch_size': 16,\n",
      "                                'n_frames_per_step': 1,\n",
      "                                'until_step': 60000},\n",
      "                               {'batch_size': 16,\n",
      "                                'n_frames_per_step': 1,\n",
      "                                'until_step': 70000},\n",
      "                               {'batch_size': 16,\n",
      "                                'n_frames_per_step': 1,\n",
      "                                'until_step': None}],\n",
      " 'ref_enc_filters': [32, 32, 64, 64, 128, 128],\n",
      " 'ref_enc_gru_size': 128,\n",
      " 'ref_enc_pad': [1, 1],\n",
      " 'ref_enc_size': [3, 3],\n",
      " 'ref_enc_strides': [2, 2],\n",
      " 'sample_inference_speaker_ids': [0],\n",
      " 'sample_inference_text': 'That quick beige fox jumped in the air loudly over '\n",
      "                          'the thin dog fence.',\n",
      " 'sampling_rate': 22050,\n",
      " 'seed': 1234,\n",
      " 'speaker_embedding_dim': 1,\n",
      " 'steps_per_sample': 100,\n",
      " 'symbol_set': 'nvidia_taco2',\n",
      " 'symbols_embedding_dim': 512,\n",
      " 'text_cleaners': ['english_cleaners'],\n",
      " 'torchmoji_model_file': None,\n",
      " 'torchmoji_vocabulary_file': None,\n",
      " 'training_audiopaths_and_text': 'test/fixtures/ljtest/list.txt',\n",
      " 'val_audiopaths_and_text': 'test/fixtures/ljtest/list.txt',\n",
      " 'warm_start_name': 'test/fixtures/models/taco2ljdefault',\n",
      " 'weight_decay': 1e-06,\n",
      " 'win_length': 1024,\n",
      " 'with_gst': False}\n",
      "[NeMo I 2022-02-22 06:43:20 cloud:56] Found existing object /home/s_uberduck_ai/.cache/torch/NeMo/NeMo_1.7.0rc0/qn5x5_libri_tts_phonemes/656c7439dd3a0d614978529371be498b/qn5x5_libri_tts_phonemes.nemo.\n",
      "[NeMo I 2022-02-22 06:43:20 cloud:62] Re-using file from: /home/s_uberduck_ai/.cache/torch/NeMo/NeMo_1.7.0rc0/qn5x5_libri_tts_phonemes/656c7439dd3a0d614978529371be498b/qn5x5_libri_tts_phonemes.nemo\n",
      "[NeMo I 2022-02-22 06:43:20 common:704] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-02-22 06:43:20 features:232] Using torch_stft is deprecated and will be removed in 1.1.0. Please set stft_conv and stft_exact_pad to False for FilterbankFeatures and AudioToMelSpectrogramPreprocessor. Please set exact_pad to True as needed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-02-22 06:43:20 features:255] PADDING: 1\n",
      "[NeMo I 2022-02-22 06:43:20 features:265] STFT using conv\n",
      "[NeMo I 2022-02-22 06:43:21 save_restore_connector:157] Model EncDecCTCModel was successfully restored from /home/s_uberduck_ai/.cache/torch/NeMo/NeMo_1.7.0rc0/qn5x5_libri_tts_phonemes/656c7439dd3a0d614978529371be498b/qn5x5_libri_tts_phonemes.nemo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo E 2022-02-22 06:43:21 vocabs:324] Torch distributed needs to be initialized before you initialized <nemo.collections.common.data.vocabs.Phonemes object at 0x7fc204077e20>. This class is prone to data access race conditions. Now downloading corpora from global rank 0. If other ranks pass this before rank 0, errors might result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-02-22 06:43:21 cloud:56] Found existing object /home/s_uberduck_ai/.cache/torch/NeMo/NeMo_1.7.0rc0/qn5x5_libri_tts_phonemes/656c7439dd3a0d614978529371be498b/qn5x5_libri_tts_phonemes.nemo.\n",
      "[NeMo I 2022-02-22 06:43:21 cloud:62] Re-using file from: /home/s_uberduck_ai/.cache/torch/NeMo/NeMo_1.7.0rc0/qn5x5_libri_tts_phonemes/656c7439dd3a0d614978529371be498b/qn5x5_libri_tts_phonemes.nemo\n",
      "[NeMo I 2022-02-22 06:43:21 common:704] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-02-22 06:43:21 features:232] Using torch_stft is deprecated and will be removed in 1.1.0. Please set stft_conv and stft_exact_pad to False for FilterbankFeatures and AudioToMelSpectrogramPreprocessor. Please set exact_pad to True as needed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-02-22 06:43:21 features:255] PADDING: 1\n",
      "[NeMo I 2022-02-22 06:43:21 features:265] STFT using conv\n",
      "[NeMo I 2022-02-22 06:43:22 save_restore_connector:157] Model EncDecCTCModel was successfully restored from /home/s_uberduck_ai/.cache/torch/NeMo/NeMo_1.7.0rc0/qn5x5_libri_tts_phonemes/656c7439dd3a0d614978529371be498b/qn5x5_libri_tts_phonemes.nemo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo E 2022-02-22 06:43:22 vocabs:324] Torch distributed needs to be initialized before you initialized <nemo.collections.common.data.vocabs.Phonemes object at 0x7fc17ceb7520>. This class is prone to data access race conditions. Now downloading corpora from global rank 0. If other ranks pass this before rank 0, errors might result.\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config[\"distributed_run\"] = True\n",
    "hparams = HParams(**config)\n",
    "hparams.ignore_layers\n",
    "model = Tacotron2(hparams)\n",
    "if torch.cuda.is_available() and hparams.cudnn_enabled:\n",
    "    model.cuda()\n",
    "trainer = Tacotron2Trainer(hparams, rank=0, world_size=0)\n",
    "train_set, val_set, train_loader, sampler, collate_fn = trainer.initialize_loader(\n",
    "    include_durations=hparams.include_durations\n",
    ")\n",
    "batch = next(enumerate(train_loader))[1]\n",
    "X, y = model.parse_batch(batch)\n",
    "forward_output = model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86c01969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip\n",
    "config = NON_ATTENTIVE_DEFAULTS.values()\n",
    "with open(\"test/fixtures/ljtest/taco2_lj2lj.json\") as f:\n",
    "    config.update(json.load(f))\n",
    "hparams = HParams(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d33273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85b93cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using any style tokens\n",
      "TTSTrainer start 18068.914510187\n",
      "Initializing trainer with hparams:\n",
      "{'attention_dim': 128,\n",
      " 'attention_location_kernel_size': 31,\n",
      " 'attention_location_n_filters': 32,\n",
      " 'attention_rnn_dim': 1024,\n",
      " 'batch_size': 16,\n",
      " 'checkpoint_name': None,\n",
      " 'checkpoint_path': 'test/fixtures/results/checkpoints',\n",
      " 'coarse_n_frames_per_step': None,\n",
      " 'compute_durations': False,\n",
      " 'cudnn_enabled': True,\n",
      " 'dataset_path': './dataset',\n",
      " 'debug': False,\n",
      " 'decoder_rnn_dim': 1024,\n",
      " 'distributed_run': False,\n",
      " 'encoder_embedding_dim': 512,\n",
      " 'encoder_kernel_size': 5,\n",
      " 'encoder_n_convolutions': 3,\n",
      " 'epochs': 5,\n",
      " 'epochs_per_checkpoint': 4,\n",
      " 'filter_length': 1024,\n",
      " 'fp16_run': False,\n",
      " 'gate_threshold': 0.5,\n",
      " 'grad_clip_thresh': 1.0,\n",
      " 'gst_type': None,\n",
      " 'has_speaker_embedding': False,\n",
      " 'hop_length': 256,\n",
      " 'ignore_layers': ['speaker_embedding.weight'],\n",
      " 'include_durations': False,\n",
      " 'include_f0': False,\n",
      " 'learning_rate': 0.001,\n",
      " 'location_specific_attention': True,\n",
      " 'log_dir': 'test/fixtures/results/logs',\n",
      " 'mask_padding': True,\n",
      " 'max_decoder_steps': 1000,\n",
      " 'max_wav_value': 32768.0,\n",
      " 'mel_fmax': 8000,\n",
      " 'mel_fmin': 0,\n",
      " 'n_frames_per_step_initial': 1,\n",
      " 'n_mel_channels': 80,\n",
      " 'n_speakers': 1,\n",
      " 'n_symbols': 148,\n",
      " 'non_attentive': False,\n",
      " 'num_heads': 8,\n",
      " 'p_arpabet': 1.0,\n",
      " 'p_attention_dropout': 0.1,\n",
      " 'p_decoder_dropout': 0.1,\n",
      " 'p_teacher_forcing': 1.0,\n",
      " 'pos_weight': None,\n",
      " 'postnet_embedding_dim': 512,\n",
      " 'postnet_kernel_size': 5,\n",
      " 'postnet_n_convolutions': 5,\n",
      " 'prenet_dim': 256,\n",
      " 'prenet_f0_dim': 1,\n",
      " 'prenet_f0_kernel_size': 1,\n",
      " 'prenet_f0_n_layers': 1,\n",
      " 'prenet_fms_kernel_size': 1,\n",
      " 'prenet_rms_dim': 0,\n",
      " 'reduction_window_schedule': [{'batch_size': 16,\n",
      "                                'n_frames_per_step': 1,\n",
      "                                'until_step': 10000},\n",
      "                               {'batch_size': 16,\n",
      "                                'n_frames_per_step': 1,\n",
      "                                'until_step': 50000},\n",
      "                               {'batch_size': 16,\n",
      "                                'n_frames_per_step': 1,\n",
      "                                'until_step': 60000},\n",
      "                               {'batch_size': 16,\n",
      "                                'n_frames_per_step': 1,\n",
      "                                'until_step': 70000},\n",
      "                               {'batch_size': 16,\n",
      "                                'n_frames_per_step': 1,\n",
      "                                'until_step': None}],\n",
      " 'ref_enc_filters': [32, 32, 64, 64, 128, 128],\n",
      " 'ref_enc_gru_size': 128,\n",
      " 'ref_enc_pad': [1, 1],\n",
      " 'ref_enc_size': [3, 3],\n",
      " 'ref_enc_strides': [2, 2],\n",
      " 'sample_inference_speaker_ids': [0],\n",
      " 'sample_inference_text': 'That quick beige fox jumped in the air loudly over '\n",
      "                          'the thin dog fence.',\n",
      " 'sampling_rate': 22050,\n",
      " 'seed': 1234,\n",
      " 'speaker_embedding_dim': 1,\n",
      " 'steps_per_sample': 100,\n",
      " 'symbol_set': 'nvidia_taco2',\n",
      " 'symbols_embedding_dim': 512,\n",
      " 'text_cleaners': ['english_cleaners'],\n",
      " 'torchmoji_model_file': None,\n",
      " 'torchmoji_vocabulary_file': None,\n",
      " 'training_audiopaths_and_text': 'test/fixtures/ljtest/list.txt',\n",
      " 'val_audiopaths_and_text': 'test/fixtures/ljtest/list.txt',\n",
      " 'warm_start_name': 'test/fixtures/models/taco2ljdefault',\n",
      " 'weight_decay': 1e-06,\n",
      " 'win_length': 1024,\n",
      " 'with_gst': False}\n",
      "[NeMo I 2022-02-22 04:13:38 cloud:56] Found existing object /home/s_uberduck_ai/.cache/torch/NeMo/NeMo_1.7.0rc0/qn5x5_libri_tts_phonemes/656c7439dd3a0d614978529371be498b/qn5x5_libri_tts_phonemes.nemo.\n",
      "[NeMo I 2022-02-22 04:13:38 cloud:62] Re-using file from: /home/s_uberduck_ai/.cache/torch/NeMo/NeMo_1.7.0rc0/qn5x5_libri_tts_phonemes/656c7439dd3a0d614978529371be498b/qn5x5_libri_tts_phonemes.nemo\n",
      "[NeMo I 2022-02-22 04:13:38 common:704] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-02-22 04:13:39 features:232] Using torch_stft is deprecated and will be removed in 1.1.0. Please set stft_conv and stft_exact_pad to False for FilterbankFeatures and AudioToMelSpectrogramPreprocessor. Please set exact_pad to True as needed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-02-22 04:13:39 features:255] PADDING: 1\n",
      "[NeMo I 2022-02-22 04:13:39 features:265] STFT using conv\n",
      "[NeMo I 2022-02-22 04:13:39 save_restore_connector:157] Model EncDecCTCModel was successfully restored from /home/s_uberduck_ai/.cache/torch/NeMo/NeMo_1.7.0rc0/qn5x5_libri_tts_phonemes/656c7439dd3a0d614978529371be498b/qn5x5_libri_tts_phonemes.nemo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo E 2022-02-22 04:13:39 vocabs:324] Torch distributed needs to be initialized before you initialized <nemo.collections.common.data.vocabs.Phonemes object at 0x7f9e9dba4c70>. This class is prone to data access race conditions. Now downloading corpora from global rank 0. If other ranks pass this before rank 0, errors might result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-02-22 04:13:40 cloud:56] Found existing object /home/s_uberduck_ai/.cache/torch/NeMo/NeMo_1.7.0rc0/qn5x5_libri_tts_phonemes/656c7439dd3a0d614978529371be498b/qn5x5_libri_tts_phonemes.nemo.\n",
      "[NeMo I 2022-02-22 04:13:40 cloud:62] Re-using file from: /home/s_uberduck_ai/.cache/torch/NeMo/NeMo_1.7.0rc0/qn5x5_libri_tts_phonemes/656c7439dd3a0d614978529371be498b/qn5x5_libri_tts_phonemes.nemo\n",
      "[NeMo I 2022-02-22 04:13:40 common:704] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-02-22 04:13:40 features:232] Using torch_stft is deprecated and will be removed in 1.1.0. Please set stft_conv and stft_exact_pad to False for FilterbankFeatures and AudioToMelSpectrogramPreprocessor. Please set exact_pad to True as needed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-02-22 04:13:40 features:255] PADDING: 1\n",
      "[NeMo I 2022-02-22 04:13:40 features:265] STFT using conv\n",
      "[NeMo I 2022-02-22 04:13:41 save_restore_connector:157] Model EncDecCTCModel was successfully restored from /home/s_uberduck_ai/.cache/torch/NeMo/NeMo_1.7.0rc0/qn5x5_libri_tts_phonemes/656c7439dd3a0d614978529371be498b/qn5x5_libri_tts_phonemes.nemo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo E 2022-02-22 04:13:41 vocabs:324] Torch distributed needs to be initialized before you initialized <nemo.collections.common.data.vocabs.Phonemes object at 0x7f9df3d34f40>. This class is prone to data access race conditions. Now downloading corpora from global rank 0. If other ranks pass this before rank 0, errors might result.\n"
     ]
    }
   ],
   "source": [
    "from uberduck_ml_dev.trainer.tacotron2 import Tacotron2Trainer\n",
    "import json\n",
    "from uberduck_ml_dev.vendor.tfcompat.hparam import HParams\n",
    "\n",
    "config = DEFAULTS.values()\n",
    "with open(\"test/fixtures/ljtest/taco2_lj2lj.json\") as f:\n",
    "    config.update(json.load(f))\n",
    "hparams = HParams(**config)\n",
    "hparams.speaker_embedding_dim = 1\n",
    "model = Tacotron2(hparams)\n",
    "if torch.cuda.is_available() and hparams.cudnn_enabled:\n",
    "    model.cuda()\n",
    "trainer = Tacotron2Trainer(hparams, rank=0, world_size=0)\n",
    "train_set, val_set, train_loader, sampler, collate_fn = trainer.initialize_loader()\n",
    "batch = next(enumerate(train_loader))[1]\n",
    "\n",
    "X, y = model.parse_batch(batch)\n",
    "forward_output = model(X)\n",
    "# assert len(forward_output) == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2be102c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 80, 857])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_output.mel_outputs_postnet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26c29d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batch(text_int_padded=tensor([[ 91,  73,  11,  ...,   0,   0,   0],\n",
       "        [ 73, 131, 129,  ...,   0,   0,   0],\n",
       "        [129, 130, 109,  ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [129, 130, 109,  ...,   0,   0,   0],\n",
       "        [ 91,  73,  11,  ...,   0,   0,   0],\n",
       "        [ 91,  73,  11,  ...,   0,   0,   0]]), input_lengths=tensor([100,  92,  91, 129,  20, 103,  27, 142,  67,  73,  63,  37, 138, 136,\n",
       "         66, 126]), mel_padded=tensor([[[ -6.9099,  -6.5893,  -6.2174,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         [ -6.5998,  -5.6521,  -5.3905,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         [ -7.1840,  -5.1640,  -4.4443,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         ...,\n",
       "         [ -6.1202,  -5.0931,  -4.1450,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         [ -6.1241,  -4.7102,  -3.8889,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         [ -5.8752,  -5.1208,  -4.4587,  ...,   0.0000,   0.0000,   0.0000]],\n",
       "\n",
       "        [[ -8.2416,  -7.3275,  -6.2134,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         [ -8.0507,  -7.8750,  -6.0124,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         [ -6.8606,  -6.0836,  -5.3746,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         ...,\n",
       "         [-10.0927,  -9.6251,  -9.0523,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         [ -9.8306,  -8.7253,  -7.3471,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         [ -7.5351,  -6.2055,  -4.9453,  ...,   0.0000,   0.0000,   0.0000]],\n",
       "\n",
       "        [[ -6.3914,  -5.8803,  -5.5581,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         [ -6.3051,  -5.2801,  -4.7190,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         [ -6.2031,  -4.9425,  -4.4880,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         ...,\n",
       "         [ -7.6847,  -6.3393,  -5.5033,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         [ -7.0109,  -6.1494,  -6.0679,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         [ -7.2781,  -6.4489,  -6.1833,  ...,   0.0000,   0.0000,   0.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ -9.9454,  -8.2868,  -6.8776,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         [ -8.3458,  -7.8255,  -6.3032,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         [ -7.6163,  -6.5003,  -5.6330,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         ...,\n",
       "         [ -9.9448,  -5.9017,  -5.3131,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         [ -9.9037,  -4.8066,  -4.0681,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         [ -9.9471,  -5.5974,  -5.0066,  ...,   0.0000,   0.0000,   0.0000]],\n",
       "\n",
       "        [[ -6.3835,  -6.4445,  -5.7985,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         [ -6.4192,  -6.3240,  -5.4149,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         [ -7.4190,  -5.5051,  -4.8472,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         ...,\n",
       "         [ -6.8628,  -6.6728,  -5.8960,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         [ -6.5624,  -6.2555,  -5.4936,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         [ -7.2713,  -6.5981,  -5.5069,  ...,   0.0000,   0.0000,   0.0000]],\n",
       "\n",
       "        [[ -9.1835,  -6.4719,  -6.0837,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         [ -6.7287,  -6.1699,  -6.1468,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         [ -5.8363,  -5.4046,  -5.2813,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         ...,\n",
       "         [ -6.1044,  -5.9024,  -5.3051,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         [ -5.6057,  -5.5987,  -5.1356,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         [ -7.6790,  -7.2099,  -5.6514,  ...,   0.0000,   0.0000,   0.0000]]]), gate_pred=None, output_lengths=tensor([723, 710, 651, 833, 154, 760, 164, 857, 490, 443, 389, 223, 796, 832,\n",
       "        454, 699]), speaker_ids=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), f0_padded=None, gst=None, durations_padded=None, max_len=None, mel_outputs=None, mel_outputs_postnet=None, gate_target=tensor([[0., 0., 0.,  ..., 1., 1., 1.],\n",
       "        [0., 0., 0.,  ..., 1., 1., 1.],\n",
       "        [0., 0., 0.,  ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 1., 1., 1.],\n",
       "        [0., 0., 0.,  ..., 1., 1., 1.],\n",
       "        [0., 0., 0.,  ..., 1., 1., 1.]]), alignments=None, predicted_durations=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(enumerate(train_loader))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312ef875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using any style tokens\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../models/tacotron2-eminem-arpabet-400-2021-12-14.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17879/3446367272.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTacotron2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEFAULTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mloaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../models/tacotron2-eminem-arpabet-400-2021-12-14.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mhg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHiFiGanGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../models/config_v1.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../models/g_02590000_8spk\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../models/tacotron2-eminem-arpabet-400-2021-12-14.pt'"
     ]
    }
   ],
   "source": [
    "# skip\n",
    "from uberduck_ml_dev.data_loader import MelSTFT\n",
    "from uberduck_ml_dev.text.symbols import NVIDIA_TACO2_SYMBOLS\n",
    "from uberduck_ml_dev.text.util import text_to_sequence\n",
    "from uberduck_ml_dev.utils.plot import plot_attention, plot_attention_phonemes\n",
    "from uberduck_ml_dev.vocoders.hifigan import HiFiGanGenerator\n",
    "\n",
    "from IPython.display import display, Audio\n",
    "\n",
    "model = Tacotron2(DEFAULTS)\n",
    "loaded = torch.load(\"../models/tacotron2-eminem-arpabet-400-2021-12-14.pt\")\n",
    "model.load_state_dict(loaded)\n",
    "hg = HiFiGanGenerator(\"../models/config_v1.json\", \"../models/g_02590000_8spk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3629122",
   "metadata": {},
   "source": [
    "## Example: partial teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46730c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip\n",
    "%matplotlib inline\n",
    "model.eval()\n",
    "sequence = torch.LongTensor(\n",
    "    text_to_sequence(\n",
    "        \"Let's hope this is a high variance utterance.\",\n",
    "        [\"english_cleaners\"],\n",
    "        p_arpabet=1,\n",
    "        symbol_set=NVIDIA_TACO2_SYMBOLS,\n",
    "    )\n",
    ")\n",
    "mel_out, mel_out_postnet, gate_out, attn, *_ = model.inference(\n",
    "    (sequence[None], torch.LongTensor([len(sequence)]), [0])\n",
    ")\n",
    "audio = hg.infer(mel_out_postnet)\n",
    "display(Audio(audio, rate=22050))\n",
    "plot_attention_phonemes(\n",
    "    sequence, attn[0].transpose(0, 1), symbol_set=NVIDIA_TACO2_SYMBOLS\n",
    ")\n",
    "new_sequence = torch.LongTensor(\n",
    "    text_to_sequence(\n",
    "        \"Let's hope this is a highlight of your life.\",\n",
    "        [\"english_cleaners\"],\n",
    "        p_arpabet=1,\n",
    "        symbol_set=NVIDIA_TACO2_SYMBOLS,\n",
    "    )\n",
    ")\n",
    "_mel_out, _mel_out_postnet, _gate_out, _attn = model.inference_partial_tf(\n",
    "    (new_sequence[None], torch.LongTensor([len(new_sequence)])), mel_out_postnet, 90,\n",
    ")\n",
    "audio = hg.infer(_mel_out_postnet)\n",
    "display(Audio(audio, rate=22050))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a95e52",
   "metadata": {},
   "source": [
    "## Example: attention-guided rhythm transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0c1ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip\n",
    "model.eval()\n",
    "sequence = torch.LongTensor(\n",
    "    text_to_sequence(\n",
    "        \"Let's hope this is a high variance utterance.\",\n",
    "        [\"english_cleaners\"],\n",
    "        p_arpabet=1,\n",
    "        symbol_set=NVIDIA_TACO2_SYMBOLS,\n",
    "    )\n",
    ")\n",
    "mel_out, mel_out_postnet, gate_out, prev_attn, *_ = model.inference(\n",
    "    (sequence[None], torch.LongTensor([len(sequence)]), [0])\n",
    ")\n",
    "shortened_attn = torch.empty(\n",
    "    prev_attn.shape[1] // 2, prev_attn.shape[0], prev_attn.shape[2]\n",
    ")\n",
    "shortened_attn.shape\n",
    "for idx in range(len(shortened_attn)):\n",
    "    shortened_attn[idx, :, :] = (\n",
    "        prev_attn[:, 2 * idx, :] + prev_attn[:, 2 * idx + 1, :]\n",
    "    ) / 2\n",
    "\n",
    "plot_attention(shortened_attn.transpose(0, 1)[0])\n",
    "\n",
    "transcription = \"Well you know as you know the web's a pretty miraculous thing and it was a very simple paradigm that was invented which was.\"\n",
    "mel = torch.load(\"./test/fixtures/stevejobs-1.pt\")\n",
    "\n",
    "input_text = text_to_sequence(\n",
    "    transcription,\n",
    "    p_arpabet=1,\n",
    "    symbol_set=NVIDIA_TACO2_SYMBOLS,\n",
    "    cleaner_names=[\"english_cleaners\"],\n",
    ")\n",
    "input_lengths = torch.LongTensor([len(input_text)])\n",
    "input_text = torch.LongTensor(input_text)[None]\n",
    "print(input_text.shape)\n",
    "targets = mel[None]\n",
    "print(targets.shape)\n",
    "max_len = targets.size(2)\n",
    "output_lengths = torch.LongTensor([targets.size(2)])\n",
    "speaker_ids = torch.LongTensor([0])\n",
    "input_ = (input_text, input_lengths, targets, max_len, output_lengths, speaker_ids)\n",
    "\n",
    "model_out = model.forward(input_)\n",
    "print(len(model_out))\n",
    "mel_out, mel_out_postnet, gate_out, attn = model_out\n",
    "print(input_text.shape, input_lengths.shape, speaker_ids.shape, attn.shape)\n",
    "\n",
    "plot_attention_phonemes(input_text[0], attn[0].transpose(0, 1), NVIDIA_TACO2_SYMBOLS)\n",
    "\n",
    "mel, mel_postnet, gate, attn = model.inference_noattention(\n",
    "    [input_text, input_lengths, speaker_ids, attn.transpose(0, 1)]\n",
    ")\n",
    "\n",
    "input_text.shape, attn.shape\n",
    "\n",
    "audio = hg.infer(mel_postnet)\n",
    "display(Audio(audio, rate=22050))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f157e0",
   "metadata": {},
   "source": [
    "## Example: has_speaker_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2e6a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip\n",
    "import IPython.display as ipd\n",
    "\n",
    "from uberduck_ml_dev.text.symbols import NVIDIA_TACO2_SYMBOLS\n",
    "from uberduck_ml_dev.text.util import text_to_sequence\n",
    "from uberduck_ml_dev.utils.audio import mel_to_audio\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "tt = Tacotron2(DEFAULTS)\n",
    "t1_count = count_parameters(tt)\n",
    "config = dict(DEFAULTS.values())\n",
    "config[\"has_speaker_embedding\"] = True\n",
    "tt2 = Tacotron2(HParams(**config))\n",
    "t2_count = count_parameters(tt2)\n",
    "assert t1_count < t2_count\n",
    "tt2.from_pretrained(\n",
    "    model_dict=torch.load(\"../models/tacotron2-eminem-arpabet-400-2021-12-14.pt\")\n",
    ")\n",
    "tt.from_pretrained(\n",
    "    model_dict=torch.load(\"../models/tacotron2-eminem-arpabet-400-2021-12-14.pt\")\n",
    ")\n",
    "seq = text_to_sequence(\n",
    "    \"The quick brown fox jumped over the lazy dog.\",\n",
    "    [\"english_cleaners\"],\n",
    "    p_arpabet=1.0,\n",
    "    symbol_set=NVIDIA_TACO2_SYMBOLS,\n",
    ")\n",
    "seq = torch.IntTensor(seq)[None]\n",
    "print(seq.shape)\n",
    "mel, mel_postnet, _, _, _ = tt.inference(\n",
    "    (seq, torch.LongTensor([seq.size(1)]), torch.LongTensor([0]))\n",
    ")\n",
    "audio = mel_to_audio(mel_postnet[0])\n",
    "\n",
    "ipd.display(ipd.Audio(audio.data.numpy(), rate=22050))\n",
    "mel, mel_postnet, *_ = tt2.inference(\n",
    "    (seq, torch.LongTensor([seq.size(1)]), torch.LongTensor([0]))\n",
    ")\n",
    "audio = mel_to_audio(mel_postnet[0])\n",
    "ipd.display(ipd.Audio(audio.data.numpy(), rate=22050))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uberduck",
   "language": "python",
   "name": "uberduck"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
