{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535085d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp trainer.gradtts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653d4f95",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47500540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.distributed as dist\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "\n",
    "from uberduck_ml_dev.models.common import MelSTFT\n",
    "from uberduck_ml_dev.utils.plot import (\n",
    "    plot_attention,\n",
    "    plot_gate_outputs,\n",
    "    plot_spectrogram,\n",
    "    plot_tensor,\n",
    ")\n",
    "from uberduck_ml_dev.text.util import text_to_sequence, random_utterance\n",
    "from uberduck_ml_dev.text.symbols import symbols_with_ipa\n",
    "from uberduck_ml_dev.trainer.base import TTSTrainer\n",
    "\n",
    "from uberduck_ml_dev.data_loader import (\n",
    "    TextAudioSpeakerLoader,\n",
    "    TextMelCollate,\n",
    "    DistributedBucketSampler,\n",
    "    TextMelDataset,\n",
    ")\n",
    "from uberduck_ml_dev.vendor.tfcompat.hparam import HParams\n",
    "from uberduck_ml_dev.utils.plot import save_figure_to_numpy, plot_spectrogram\n",
    "from uberduck_ml_dev.utils.utils import slice_segments, clip_grad_value_\n",
    "from uberduck_ml_dev.text.symbols import SYMBOL_SETS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b34fa71",
   "metadata": {},
   "source": [
    "# Grad TTS Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e61d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from tqdm import tqdm\n",
    "from uberduck_ml_dev.text.util import text_to_sequence, random_utterance\n",
    "from uberduck_ml_dev.models.gradtts import (\n",
    "    GradTTS,\n",
    ")\n",
    "from uberduck_ml_dev.utils.utils import intersperse\n",
    "\n",
    "\n",
    "class GradTTSTrainer(TTSTrainer):\n",
    "    REQUIRED_HPARAMS = [\n",
    "        \"training_audiopaths_and_text\",\n",
    "        \"test_audiopaths_and_text\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        for param in self.REQUIRED_HPARAMS:\n",
    "            if not hasattr(self, param):\n",
    "                raise Exception(f\"GradTTSTrainer missing a required param: {param}\")\n",
    "        self.sampling_rate = self.hparams.sampling_rate\n",
    "        self.checkpoint_path = self.hparams.log_dir\n",
    "\n",
    "    def sample_inference(self, model, timesteps=10, spk=None):\n",
    "        with torch.no_grad():\n",
    "            sequence = text_to_sequence(\n",
    "                random_utterance(),\n",
    "                self.text_cleaners,\n",
    "                1.0,\n",
    "                symbol_set=self.hparams.symbol_set,\n",
    "            )\n",
    "            if self.hparams.intersperse_text:\n",
    "                sequence = intersperse(\n",
    "                    sequence, (len(SYMBOL_SETS[self.hparams.symbol_set]))\n",
    "                )\n",
    "            x = torch.LongTensor(sequence).cuda()[None]\n",
    "            x_lengths = torch.LongTensor([x.shape[-1]]).cuda()\n",
    "            y_enc, y_dec, attn = model(\n",
    "                x,\n",
    "                x_lengths,\n",
    "                n_timesteps=50,\n",
    "                temperature=1.5,\n",
    "                stoc=False,\n",
    "                spk=spk,\n",
    "                length_scale=0.91,\n",
    "            )\n",
    "            audio = self.sample(\n",
    "                y_dec,\n",
    "                algorithm=\"hifigan\",\n",
    "                hifigan_config=\"/home/w_uberduck_ai/Speech-Backbones/Grad-TTS/checkpts/hifigan-config.json\",\n",
    "                hifigan_checkpoint=\"/home/w_uberduck_ai/Speech-Backbones/Grad-TTS/checkpts/gen_02640000_studio\",\n",
    "                max_wav_value=32768,\n",
    "                cudnn_enabled=self.hparams.cudnn_enabled,\n",
    "            )\n",
    "            return audio\n",
    "\n",
    "    def train(self, checkpoint=None):\n",
    "        if self.distributed_run:\n",
    "            self.init_distributed()\n",
    "\n",
    "        train_dataset = TextMelDataset(\n",
    "            self.hparams.training_audiopaths_and_text,\n",
    "            self.hparams.text_cleaners,\n",
    "            1.0,\n",
    "            self.hparams.n_feats,\n",
    "            self.hparams.sampling_rate,\n",
    "            self.hparams.mel_fmin,\n",
    "            self.hparams.mel_fmax,\n",
    "            self.hparams.filter_length,\n",
    "            self.hparams.hop_length,\n",
    "            (self.hparams.filter_length - self.hparams.hop_length) // 2,\n",
    "            self.hparams.win_length,\n",
    "            intersperse_text=self.hparams.intersperse_text,\n",
    "            intersperse_token=(len(SYMBOL_SETS[self.hparams.symbol_set])),\n",
    "            symbol_set=self.hparams.symbol_set\n",
    "            #             debug=self.debug,\n",
    "            #             debug_dataset_size=self.debug_dataset_size,\n",
    "        )\n",
    "        collate_fn = TextMelCollate()\n",
    "\n",
    "        loader = DataLoader(\n",
    "            dataset=train_dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            collate_fn=collate_fn,\n",
    "            drop_last=True,\n",
    "            num_workers=0,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "        test_dataset = TextMelDataset(\n",
    "            self.hparams.test_audiopaths_and_text,\n",
    "            self.hparams.text_cleaners,\n",
    "            1.0,\n",
    "            self.hparams.n_feats,\n",
    "            self.hparams.sampling_rate,\n",
    "            self.hparams.mel_fmin,\n",
    "            self.hparams.mel_fmax,\n",
    "            self.hparams.filter_length,\n",
    "            self.hparams.hop_length,\n",
    "            (self.hparams.filter_length - self.hparams.hop_length) // 2,\n",
    "            self.hparams.win_length,\n",
    "            intersperse_text=self.hparams.intersperse_text,\n",
    "            intersperse_token=(len(SYMBOL_SETS[self.hparams.symbol_set])),\n",
    "            symbol_set=self.hparams.symbol_set,\n",
    "            #             debug=self.debug,\n",
    "            #             debug_dataset_size=self.debug_dataset_size,\n",
    "        )\n",
    "\n",
    "        model = GradTTS(self.hparams)\n",
    "\n",
    "        if self.hparams.checkpoint:\n",
    "            model.load_state_dict(torch.load(self.hparams.checkpoint))\n",
    "        model = model.cuda()\n",
    "\n",
    "        print(\n",
    "            \"Number of encoder + duration predictor parameters: %.2fm\"\n",
    "            % (model.encoder.nparams / 1e6)\n",
    "        )\n",
    "        print(\"Number of decoder parameters: %.2fm\" % (model.decoder.nparams / 1e6))\n",
    "        print(\"Total parameters: %.2fm\" % (model.nparams / 1e6))\n",
    "\n",
    "        print(\"Initializing optimizer...\")\n",
    "        optimizer = torch.optim.Adam(\n",
    "            params=model.parameters(), lr=self.hparams.learning_rate\n",
    "        )\n",
    "        test_batch = test_dataset.sample_test_batch(size=self.hparams.test_size)\n",
    "        for i, item in enumerate(test_batch):\n",
    "            text, mel, spk = item\n",
    "            self.log(\n",
    "                f\"image_{i}/ground_truth\",\n",
    "                0,\n",
    "                image=plot_tensor(mel.squeeze()),\n",
    "            )\n",
    "        iteration = 0\n",
    "        last_time = time.time()\n",
    "        for epoch in range(1, self.hparams.n_epochs + 1):\n",
    "            model.train()\n",
    "            dur_losses = []\n",
    "            prior_losses = []\n",
    "            diff_losses = []\n",
    "            for batch_idx, batch in enumerate(loader):\n",
    "                model.zero_grad()\n",
    "                x, x_lengths, y, _, y_lengths, speaker_ids = batch\n",
    "\n",
    "                dur_loss, prior_loss, diff_loss = model.compute_loss(\n",
    "                    x, x_lengths, y, y_lengths, out_size=self.hparams.out_size\n",
    "                )\n",
    "                loss = sum([dur_loss, prior_loss, diff_loss])\n",
    "                loss.backward()\n",
    "\n",
    "                enc_grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                    model.encoder.parameters(), max_norm=1\n",
    "                )\n",
    "                dec_grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                    model.decoder.parameters(), max_norm=1\n",
    "                )\n",
    "                optimizer.step()\n",
    "\n",
    "                self.log(\"training/duration_loss\", iteration, dur_loss.item())\n",
    "                self.log(\"training/prior_loss\", iteration, prior_loss.item())\n",
    "                self.log(\"training/diffusion_loss\", iteration, diff_loss.item())\n",
    "                self.log(\"training/encoder_grad_norm\", iteration, enc_grad_norm)\n",
    "                self.log(\"training/decoder_grad_norm\", iteration, dec_grad_norm)\n",
    "\n",
    "                dur_losses.append(dur_loss.item())\n",
    "                prior_losses.append(prior_loss.item())\n",
    "                diff_losses.append(diff_loss.item())\n",
    "\n",
    "                iteration += 1\n",
    "\n",
    "            log_msg = f\"Epoch {epoch}, iter: {iteration}: dur_loss: {np.mean(dur_losses):.4f} | prior_loss: {np.mean(prior_losses):.4f} | diff_loss: {np.mean(diff_losses):.4f} | time: {time.time()-last_time:.2f}s\"\n",
    "            last_time = time.time()\n",
    "            with open(f\"{self.hparams.log_dir}/train.log\", \"a\") as f:\n",
    "                f.write(log_msg + \"\\n\")\n",
    "                print(log_msg)\n",
    "\n",
    "            if epoch % self.log_interval == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for i, item in enumerate(test_batch):\n",
    "                        x, _y, _speaker_id = item\n",
    "                        x = x.to(torch.long).unsqueeze(0)\n",
    "                        x_lengths = torch.LongTensor([x.shape[-1]])\n",
    "                        y_enc, y_dec, attn = model(x, x_lengths, n_timesteps=50)\n",
    "                        self.log(\n",
    "                            f\"image_{i}/generated_enc\",\n",
    "                            iteration,\n",
    "                            image=plot_tensor(y_enc.squeeze().cpu()),\n",
    "                        )\n",
    "                        self.log(\n",
    "                            f\"image_{i}/generated_dec\",\n",
    "                            iteration,\n",
    "                            image=plot_tensor(y_dec.squeeze().cpu()),\n",
    "                        )\n",
    "                        self.log(\n",
    "                            f\"image_{i}/alignment\",\n",
    "                            iteration,\n",
    "                            image=plot_tensor(attn.squeeze().cpu()),\n",
    "                        )\n",
    "                        self.log(\n",
    "                            f\"audio/inference_{i}\",\n",
    "                            iteration,\n",
    "                            audio=self.sample_inference(model),\n",
    "                        )\n",
    "\n",
    "            if epoch % self.save_every == 0:\n",
    "                torch.save(\n",
    "                    model.state_dict(), f=f\"{self.hparams.log_dir}/grad_{epoch}.pt\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb6e9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTSTrainer start 2261702.775188925\n",
      "Initializing trainer with hparams:\n",
      "{'batch_size': 1,\n",
      " 'beta_max': 20.0,\n",
      " 'beta_min': 0.05,\n",
      " 'checkpoint': None,\n",
      " 'cudnn_enabled': True,\n",
      " 'dec_dim': 64,\n",
      " 'distributed_run': False,\n",
      " 'enc_dropout': 0.1,\n",
      " 'enc_kernel': 3,\n",
      " 'filter_channels': 768,\n",
      " 'filter_channels_dp': 256,\n",
      " 'filter_length': 1024,\n",
      " 'hop_length': 256,\n",
      " 'intersperse_text': True,\n",
      " 'learning_rate': 0.0001,\n",
      " 'log_dir': 'output',\n",
      " 'log_interval': 100,\n",
      " 'max_wav_value': 32768.0,\n",
      " 'mel_fmax': 8000,\n",
      " 'mel_fmin': 0.0,\n",
      " 'n_enc_channels': 192,\n",
      " 'n_enc_layers': 6,\n",
      " 'n_epochs': 10000,\n",
      " 'n_feats': 80,\n",
      " 'n_heads': 2,\n",
      " 'n_spks': 1,\n",
      " 'out_size': 172,\n",
      " 'oversample_weights': None,\n",
      " 'pe_scale': 1000,\n",
      " 'rank': 0,\n",
      " 'sampling_rate': 22050,\n",
      " 'save_every': 1000,\n",
      " 'seed': 37,\n",
      " 'spk_emb_dim': 64,\n",
      " 'symbol_set': 'gradtts',\n",
      " 'test_audiopaths_and_text': 'val.txt',\n",
      " 'test_size': 2,\n",
      " 'text_cleaners': ['english_cleaners'],\n",
      " 'training_audiopaths_and_text': 'train.txt',\n",
      " 'win_length': 1024,\n",
      " 'window_size': 4}\n"
     ]
    }
   ],
   "source": [
    "DEFAULTS = HParams(\n",
    "    training_audiopaths_and_text=\"train.txt\",\n",
    "    test_audiopaths_and_text=\"val.txt\",\n",
    "    cudnn_enabled=True,\n",
    "    log_dir=\"output\",\n",
    "    symbol_set=\"gradtts\",\n",
    "    intersperse_text=True,\n",
    "    n_spks=1,\n",
    "    spk_emb_dim=64,\n",
    "    sampling_rate=22050,\n",
    "    hop_length=256,\n",
    "    win_length=1024,\n",
    "    n_enc_channels=192,\n",
    "    filter_channels=768,\n",
    "    filter_channels_dp=256,\n",
    "    n_enc_layers=6,\n",
    "    enc_kernel=3,\n",
    "    enc_dropout=0.1,\n",
    "    n_heads=2,\n",
    "    window_size=4,\n",
    "    dec_dim=64,\n",
    "    beta_min=0.05,\n",
    "    beta_max=20.0,\n",
    "    pe_scale=1000,\n",
    "    test_size=2,\n",
    "    n_epochs=10000,\n",
    "    batch_size=1,\n",
    "    learning_rate=1e-4,\n",
    "    seed=37,\n",
    "    out_size=2 * 22050 // 256,\n",
    "    filter_length=1024,\n",
    "    rank=0,\n",
    "    distributed_run=False,\n",
    "    oversample_weights=None,\n",
    "    text_cleaners=[\"english_cleaners\"],\n",
    "    max_wav_value=32768.0,\n",
    "    n_feats=80,\n",
    "    mel_fmax=8000,\n",
    "    mel_fmin=0.0,\n",
    "    checkpoint=None,\n",
    "    log_interval=100,\n",
    "    save_every=1000,\n",
    ")\n",
    "trainer = GradTTSTrainer(DEFAULTS, rank=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ca56c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of encoder + duration predictor parameters: 7.20m\n",
      "Number of decoder parameters: 7.63m\n",
      "Total parameters: 14.84m\n",
      "Initializing optimizer...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17936/4032920361.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_17936/4231993808.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, checkpoint)\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeaker_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                 dur_loss, prior_loss, diff_loss = model.compute_loss(\n\u001b[0m\u001b[1;32m    147\u001b[0m                     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                 )\n",
      "\u001b[0;32m~/uberduck-ml-dev/uberduck_ml_dev/models/gradtts.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, x, x_lengths, y, y_lengths, spk, out_size)\u001b[0m\n\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m         \u001b[0;31m# Get encoder_outputs `mu_x` and log-scaled token durations `logw`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m         \u001b[0mmu_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m         \u001b[0my_max_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/uberduck-ml-dev/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/uberduck-ml-dev/uberduck_ml_dev/models/gradtts.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, x_lengths, spk)\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m         \u001b[0mx_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/uberduck-ml-dev/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/uberduck-ml-dev/lib/python3.8/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[0;32m/opt/conda/envs/uberduck-ml-dev/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63e750f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec332df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b836c787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2efcf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
