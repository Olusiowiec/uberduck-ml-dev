{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "224180cb",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Debugging\" data-toc-modified-id=\"Debugging-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Debugging</a></span></li><li><span><a href=\"#Trainer\" data-toc-modified-id=\"Trainer-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Trainer</a></span></li><li><span><a href=\"#Losses\" data-toc-modified-id=\"Losses-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Losses</a></span></li><li><span><a href=\"#VITS-Trainer\" data-toc-modified-id=\"VITS-Trainer-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>VITS Trainer</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df74d1c",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535085d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp trainer.gradtts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d789a053",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41357cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "653d4f95",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47500540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.distributed as dist\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "\n",
    "from uberduck_ml_dev.models.common import MelSTFT\n",
    "from uberduck_ml_dev.utils.plot import (\n",
    "    plot_attention,\n",
    "    plot_gate_outputs,\n",
    "    plot_spectrogram,\n",
    "    plot_tensor,\n",
    ")\n",
    "from uberduck_ml_dev.text.util import text_to_sequence, random_utterance\n",
    "from uberduck_ml_dev.text.symbols import symbols_with_ipa\n",
    "from uberduck_ml_dev.trainer.base import TTSTrainer\n",
    "\n",
    "from uberduck_ml_dev.data_loader import (\n",
    "    TextAudioSpeakerLoader,\n",
    "    TextMelCollate,\n",
    "    DistributedBucketSampler,\n",
    "    TextMelDataset,\n",
    ")\n",
    "from uberduck_ml_dev.vendor.tfcompat.hparam import HParams\n",
    "from uberduck_ml_dev.utils.plot import save_figure_to_numpy, plot_spectrogram\n",
    "from uberduck_ml_dev.utils.utils import slice_segments, clip_grad_value_\n",
    "from uberduck_ml_dev.text.symbols import SYMBOL_SETS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b34fa71",
   "metadata": {},
   "source": [
    "# Grad TTS Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e61d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from tqdm import tqdm\n",
    "from uberduck_ml_dev.text.util import text_to_sequence, random_utterance\n",
    "from uberduck_ml_dev.models.gradtts import (\n",
    "    GradTTS,\n",
    ")\n",
    "from uberduck_ml_dev.utils.utils import intersperse\n",
    "\n",
    "\n",
    "class GradTTSTrainer(TTSTrainer):\n",
    "    REQUIRED_HPARAMS = [\n",
    "        \"training_audiopaths_and_text\",\n",
    "        \"test_audiopaths_and_text\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        for param in self.REQUIRED_HPARAMS:\n",
    "            if not hasattr(self, param):\n",
    "                raise Exception(f\"GradTTSTrainer missing a required param: {param}\")\n",
    "        self.sampling_rate = self.hparams.sampling_rate\n",
    "        self.checkpoint_path = self.hparams.log_dir\n",
    "\n",
    "    def sample_inference(self, model, timesteps=10, spk=None):\n",
    "        with torch.no_grad():\n",
    "            sequence = text_to_sequence(\n",
    "                random_utterance(),\n",
    "                self.text_cleaners,\n",
    "                1.0,\n",
    "                symbol_set=self.hparams.symbol_set,\n",
    "            )\n",
    "            if self.hparams.intersperse_text:\n",
    "                sequence = intersperse(\n",
    "                    sequence, (len(SYMBOL_SETS[self.hparams.symbol_set]))\n",
    "                )\n",
    "            x = torch.LongTensor(sequence).cuda()[None]\n",
    "            x_lengths = torch.LongTensor([x.shape[-1]]).cuda()\n",
    "            y_enc, y_dec, attn = model(\n",
    "                x,\n",
    "                x_lengths,\n",
    "                n_timesteps=50,\n",
    "                temperature=1.5,\n",
    "                stoc=False,\n",
    "                spk=spk,\n",
    "                length_scale=0.91,\n",
    "            )\n",
    "            audio = self.sample(\n",
    "                y_dec,\n",
    "                algorithm=\"hifigan\",\n",
    "                hifigan_config=\"/home/w_uberduck_ai/Speech-Backbones/Grad-TTS/checkpts/hifigan-config.json\",\n",
    "                hifigan_checkpoint=\"/home/w_uberduck_ai/Speech-Backbones/Grad-TTS/checkpts/gen_02640000_studio\",\n",
    "                max_wav_value=32768,\n",
    "                cudnn_enabled=self.hparams.cudnn_enabled,\n",
    "            )\n",
    "            return audio\n",
    "\n",
    "    def train(self, checkpoint=None):\n",
    "        if self.distributed_run:\n",
    "            self.init_distributed()\n",
    "\n",
    "        train_dataset = TextMelDataset(\n",
    "            self.hparams.training_audiopaths_and_text,\n",
    "            self.hparams.text_cleaners,\n",
    "            1.0,\n",
    "            self.hparams.n_feats,\n",
    "            self.hparams.sampling_rate,\n",
    "            self.hparams.mel_fmin,\n",
    "            self.hparams.mel_fmax,\n",
    "            self.hparams.filter_length,\n",
    "            self.hparams.hop_length,\n",
    "            (self.hparams.filter_length - self.hparams.hop_length) // 2,\n",
    "            self.hparams.win_length,\n",
    "            intersperse_text=self.hparams.intersperse_text,\n",
    "            intersperse_token=(len(SYMBOL_SETS[self.hparams.symbol_set])),\n",
    "            symbol_set=self.hparams.symbol_set\n",
    "            #             debug=self.debug,\n",
    "            #             debug_dataset_size=self.debug_dataset_size,\n",
    "        )\n",
    "        collate_fn = TextMelCollate()\n",
    "\n",
    "        loader = DataLoader(\n",
    "            dataset=train_dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            collate_fn=collate_fn,\n",
    "            drop_last=True,\n",
    "            num_workers=0,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "        test_dataset = TextMelDataset(\n",
    "            self.hparams.test_audiopaths_and_text,\n",
    "            self.hparams.text_cleaners,\n",
    "            1.0,\n",
    "            self.hparams.n_feats,\n",
    "            self.hparams.sampling_rate,\n",
    "            self.hparams.mel_fmin,\n",
    "            self.hparams.mel_fmax,\n",
    "            self.hparams.filter_length,\n",
    "            self.hparams.hop_length,\n",
    "            (self.hparams.filter_length - self.hparams.hop_length) // 2,\n",
    "            self.hparams.win_length,\n",
    "            intersperse_text=self.hparams.intersperse_text,\n",
    "            intersperse_token=(len(SYMBOL_SETS[self.hparams.symbol_set])),\n",
    "            symbol_set=self.hparams.symbol_set,\n",
    "            #             debug=self.debug,\n",
    "            #             debug_dataset_size=self.debug_dataset_size,\n",
    "        )\n",
    "\n",
    "        model = GradTTS(self.hparams)\n",
    "\n",
    "        if self.hparams.checkpoint:\n",
    "            model.load_state_dict(torch.load(self.hparams.checkpoint))\n",
    "        model = model.cuda()\n",
    "\n",
    "        print(\n",
    "            \"Number of encoder + duration predictor parameters: %.2fm\"\n",
    "            % (model.encoder.nparams / 1e6)\n",
    "        )\n",
    "        print(\"Number of decoder parameters: %.2fm\" % (model.decoder.nparams / 1e6))\n",
    "        print(\"Total parameters: %.2fm\" % (model.nparams / 1e6))\n",
    "\n",
    "        print(\"Initializing optimizer...\")\n",
    "        optimizer = torch.optim.Adam(\n",
    "            params=model.parameters(), lr=self.hparams.learning_rate\n",
    "        )\n",
    "        test_batch = test_dataset.sample_test_batch(size=self.hparams.test_size)\n",
    "        for i, item in enumerate(test_batch):\n",
    "            text, mel, spk = item\n",
    "            self.log(\n",
    "                f\"image_{i}/ground_truth\",\n",
    "                0,\n",
    "                image=plot_tensor(mel.squeeze()),\n",
    "            )\n",
    "\n",
    "        iteration = 0\n",
    "        last_time = time.time()\n",
    "        for epoch in range(1, self.hparams.n_epochs + 1):\n",
    "            model.train()\n",
    "            dur_losses = []\n",
    "            prior_losses = []\n",
    "            diff_losses = []\n",
    "            for batch_idx, batch in enumerate(loader):\n",
    "                model.zero_grad()\n",
    "                x, x_lengths, y, _, y_lengths, speaker_ids = batch\n",
    "\n",
    "                dur_loss, prior_loss, diff_loss = model.compute_loss(\n",
    "                    x, x_lengths, y, y_lengths, out_size=self.hparams.out_size\n",
    "                )\n",
    "                loss = sum([dur_loss, prior_loss, diff_loss])\n",
    "                loss.backward()\n",
    "\n",
    "                enc_grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                    model.encoder.parameters(), max_norm=1\n",
    "                )\n",
    "                dec_grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                    model.decoder.parameters(), max_norm=1\n",
    "                )\n",
    "                optimizer.step()\n",
    "\n",
    "                self.log(\"training/duration_loss\", iteration, dur_loss.item())\n",
    "                self.log(\"training/prior_loss\", iteration, prior_loss.item())\n",
    "                self.log(\"training/diffusion_loss\", iteration, diff_loss.item())\n",
    "                self.log(\"training/encoder_grad_norm\", iteration, enc_grad_norm)\n",
    "                self.log(\"training/decoder_grad_norm\", iteration, dec_grad_norm)\n",
    "\n",
    "                dur_losses.append(dur_loss.item())\n",
    "                prior_losses.append(prior_loss.item())\n",
    "                diff_losses.append(diff_loss.item())\n",
    "\n",
    "                iteration += 1\n",
    "\n",
    "            log_msg = f\"Epoch {epoch}, iter: {iteration}: dur_loss: {np.mean(dur_losses):.4f} | prior_loss: {np.mean(prior_losses):.4f} | diff_loss: {np.mean(diff_losses):.4f} | time: {time.time()-last_time:.2f}s\"\n",
    "            last_time = time.time()\n",
    "            with open(f\"{self.hparams.log_dir}/train.log\", \"a\") as f:\n",
    "                f.write(log_msg + \"\\n\")\n",
    "                print(log_msg)\n",
    "\n",
    "            if epoch % self.log_interval == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for i, item in enumerate(test_batch):\n",
    "                        x, _y, _speaker_id = item\n",
    "                        x = x.to(torch.long).unsqueeze(0)\n",
    "                        x_lengths = torch.LongTensor([x.shape[-1]])\n",
    "                        y_enc, y_dec, attn = model(x, x_lengths, n_timesteps=50)\n",
    "                        self.log(\n",
    "                            f\"image_{i}/generated_enc\",\n",
    "                            iteration,\n",
    "                            image=plot_tensor(y_enc.squeeze().cpu()),\n",
    "                        )\n",
    "                        self.log(\n",
    "                            f\"image_{i}/generated_dec\",\n",
    "                            iteration,\n",
    "                            image=plot_tensor(y_dec.squeeze().cpu()),\n",
    "                        )\n",
    "                        self.log(\n",
    "                            f\"image_{i}/alignment\",\n",
    "                            iteration,\n",
    "                            image=plot_tensor(attn.squeeze().cpu()),\n",
    "                        )\n",
    "                        self.log(\n",
    "                            f\"audio/inference_{i}\",\n",
    "                            iteration,\n",
    "                            audio=self.sample_inference(model),\n",
    "                        )\n",
    "\n",
    "            if epoch % self.save_every == 0:\n",
    "                torch.save(\n",
    "                    model.state_dict(), f=f\"{self.hparams.log_dir}/grad_{epoch}.pt\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb6e9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTSTrainer start 2012310.788795182\n",
      "Initializing trainer with hparams:\n",
      "{'batch_size': 1,\n",
      " 'beta_max': 20.0,\n",
      " 'beta_min': 0.05,\n",
      " 'checkpoint': None,\n",
      " 'cudnn_enabled': True,\n",
      " 'dec_dim': 64,\n",
      " 'distributed_run': False,\n",
      " 'enc_dropout': 0.1,\n",
      " 'enc_kernel': 3,\n",
      " 'filter_channels': 768,\n",
      " 'filter_channels_dp': 256,\n",
      " 'filter_length': 1024,\n",
      " 'hop_length': 256,\n",
      " 'intersperse_text': True,\n",
      " 'learning_rate': 0.0001,\n",
      " 'log_dir': 'output',\n",
      " 'log_interval': 100,\n",
      " 'max_wav_value': 32768.0,\n",
      " 'mel_fmax': 8000,\n",
      " 'mel_fmin': 0.0,\n",
      " 'n_enc_channels': 192,\n",
      " 'n_enc_layers': 6,\n",
      " 'n_epochs': 10000,\n",
      " 'n_feats': 80,\n",
      " 'n_heads': 2,\n",
      " 'n_spks': 1,\n",
      " 'out_size': 172,\n",
      " 'oversample_weights': None,\n",
      " 'pe_scale': 1000,\n",
      " 'rank': 0,\n",
      " 'sampling_rate': 22050,\n",
      " 'save_every': 1000,\n",
      " 'seed': 37,\n",
      " 'spk_emb_dim': 64,\n",
      " 'symbol_set': 'gradtts',\n",
      " 'test_audiopaths_and_text': '/home/w_uberduck_ai/uberduck-rappers/lachow_sanity.txt',\n",
      " 'test_size': 2,\n",
      " 'text_cleaners': ['english_cleaners'],\n",
      " 'training_audiopaths_and_text': '/home/w_uberduck_ai/uberduck-rappers/lachow_sanity.txt',\n",
      " 'win_length': 1024,\n",
      " 'window_size': 4}\n"
     ]
    }
   ],
   "source": [
    "DEFAULTS = HParams(\n",
    "    training_audiopaths_and_text=\"/home/w_uberduck_ai/uberduck-rappers/lachow_sanity.txt\",\n",
    "    test_audiopaths_and_text=\"/home/w_uberduck_ai/uberduck-rappers/lachow_sanity.txt\",\n",
    "    cudnn_enabled=True,\n",
    "    log_dir=\"output\",\n",
    "    symbol_set=\"gradtts\",\n",
    "    intersperse_text=True,\n",
    "    n_spks=1,\n",
    "    spk_emb_dim=64,\n",
    "    sampling_rate=22050,\n",
    "    hop_length=256,\n",
    "    win_length=1024,\n",
    "    n_enc_channels=192,\n",
    "    filter_channels=768,\n",
    "    filter_channels_dp=256,\n",
    "    n_enc_layers=6,\n",
    "    enc_kernel=3,\n",
    "    enc_dropout=0.1,\n",
    "    n_heads=2,\n",
    "    window_size=4,\n",
    "    dec_dim=64,\n",
    "    beta_min=0.05,\n",
    "    beta_max=20.0,\n",
    "    pe_scale=1000,\n",
    "    test_size=2,\n",
    "    n_epochs=10000,\n",
    "    batch_size=1,\n",
    "    learning_rate=1e-4,\n",
    "    seed=37,\n",
    "    out_size=2 * 22050 // 256,\n",
    "    filter_length=1024,\n",
    "    rank=0,\n",
    "    distributed_run=False,\n",
    "    oversample_weights=None,\n",
    "    text_cleaners=[\"english_cleaners\"],\n",
    "    max_wav_value=32768.0,\n",
    "    n_feats=80,\n",
    "    mel_fmax=8000,\n",
    "    mel_fmin=0.0,\n",
    "    checkpoint=None,\n",
    "    log_interval=100,\n",
    "    save_every=1000,\n",
    ")\n",
    "trainer = GradTTSTrainer(DEFAULTS, rank=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ca56c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of encoder + duration predictor parameters: 7.20m\n",
      "Number of decoder parameters: 7.63m\n",
      "Total parameters: 14.84m\n",
      "Initializing optimizer...\n",
      "Epoch 1, iter: 5: dur_loss: 0.6656 | prior_loss: 15.6823 | diff_loss: 0.9930 | time: 0.63s\n",
      "Epoch 2, iter: 10: dur_loss: 0.6328 | prior_loss: 14.8335 | diff_loss: 0.7924 | time: 0.53s\n",
      "Epoch 3, iter: 15: dur_loss: 0.6274 | prior_loss: 13.5213 | diff_loss: 0.7031 | time: 0.51s\n",
      "Epoch 4, iter: 20: dur_loss: 0.4993 | prior_loss: 14.7112 | diff_loss: 0.5350 | time: 0.48s\n",
      "Epoch 5, iter: 25: dur_loss: 0.5745 | prior_loss: 12.6364 | diff_loss: 0.4708 | time: 0.55s\n",
      "Epoch 6, iter: 30: dur_loss: 0.5692 | prior_loss: 12.2626 | diff_loss: 0.3381 | time: 0.49s\n",
      "Epoch 7, iter: 35: dur_loss: 0.5588 | prior_loss: 12.1942 | diff_loss: 0.2707 | time: 0.54s\n",
      "Epoch 8, iter: 40: dur_loss: 0.5193 | prior_loss: 12.2359 | diff_loss: 0.2822 | time: 0.53s\n",
      "Epoch 9, iter: 45: dur_loss: 0.4628 | prior_loss: 11.9533 | diff_loss: 0.2984 | time: 0.49s\n",
      "Epoch 10, iter: 50: dur_loss: 0.4910 | prior_loss: 10.5854 | diff_loss: 0.2333 | time: 0.52s\n",
      "Epoch 11, iter: 55: dur_loss: 0.4991 | prior_loss: 10.8791 | diff_loss: 0.4839 | time: 0.48s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4634/4032920361.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_4634/1649527949.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, checkpoint)\u001b[0m\n\u001b[1;32m    149\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m                 )\n\u001b[0;32m--> 151\u001b[0;31m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training/duration_loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdur_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/uberduck-ml-dev/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/uberduck-ml-dev/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/uberduck-ml-dev/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    134\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/uberduck-ml-dev/lib/python3.8/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63e750f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec332df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b836c787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2efcf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
