{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48dc6837",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057a0436",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535085d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp trainer.base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47500540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class TTSTrainer:\n",
    "    def __init__(self, hparams):\n",
    "        self.hparams = hparams\n",
    "        for k, v in hparams.values().items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "        if not hasattr(self, \"debug\"):\n",
    "            self.debug = False\n",
    "        if self.debug:\n",
    "            print(\"Running in debug mode with hparams:\")\n",
    "            pprint(hparams.values())\n",
    "\n",
    "    def save_checkpoint(self, checkpoint_name, **kwargs):\n",
    "        checkpoint = {}\n",
    "        for k, v in kwargs.items():\n",
    "            if hasattr(v, \"state_dict\"):\n",
    "                checkpoint[k] = v.state_dict()\n",
    "            else:\n",
    "                checkpoint[k] = v\n",
    "        torch.save(\n",
    "            checkpoint, os.path.join(self.checkpoint_path, f\"{checkpoint_name}.pt\")\n",
    "        )\n",
    "\n",
    "    def load_checkpoint(self, checkpoint_name):\n",
    "        return torch.load(os.path.join(self.checkpoint_path, checkpoint_name))\n",
    "\n",
    "    def train():\n",
    "        raise NotImplemented\n",
    "        # for batch in enumerate(data):\n",
    "        #    #fill in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b19d950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uberduck_ml_dev.vendor.tfcompat.hparam import HParams\n",
    "hp = HParams(foo=\"bar\", baz=123)\n",
    "trainer = TTSTrainer(hp)\n",
    "assert trainer.hparams == hp\n",
    "assert trainer.foo == \"bar\"\n",
    "assert trainer.baz == 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46fc18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from typing import List\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from uberduck_ml_dev.data_loader import TextMelDataset, TextMelCollate\n",
    "from uberduck_ml_dev.models.mellotron import Tacotron2\n",
    "\n",
    "\n",
    "class Tacotron2Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, model_output: List, target: List):\n",
    "        mel_target, gate_target = target[0], target[1]\n",
    "        mel_target.requires_grad = False\n",
    "        gate_target.requires_grad = False\n",
    "        gate_target = gate_target.view(-1, 1)\n",
    "        mel_out, mel_out_postnet, gate_out, _ = model_output\n",
    "        gate_out = gate_out.view(-1, 1)\n",
    "        mel_loss = nn.MSELoss()(mel_out, mel_target) + nn.MSELoss()(\n",
    "            mel_out_postnet, mel_target\n",
    "        )\n",
    "        gate_loss = nn.BCEWithLogitsLoss()(gate_out, gate_target)\n",
    "        return mel_loss + gate_loss\n",
    "\n",
    "\n",
    "class MellotronTrainer(TTSTrainer):\n",
    "    REQUIRED_HPARAMS = [\n",
    "        \"audiopaths_and_text\",\n",
    "        \"checkpoint_path\",\n",
    "        \"dataset_path\",\n",
    "        \"epochs\",\n",
    "        \"mel_fmax\",\n",
    "        \"mel_fmin\",\n",
    "        \"n_mel_channels\",\n",
    "        \"text_cleaners\",\n",
    "    ]\n",
    "\n",
    "    def validate(self, **kwargs):\n",
    "        model = kwargs[\"model\"]\n",
    "        val_set = kwargs[\"val_set\"]\n",
    "        collate_fn = kwargs[\"collate_fn\"]\n",
    "        criterion = kwargs[\"criterion\"]\n",
    "        if self.distributed_run:\n",
    "            raise NotImplemented\n",
    "        total_loss = 0\n",
    "        total_steps = 0\n",
    "        with torch.no_grad():\n",
    "            val_loader = DataLoader(\n",
    "                val_set,\n",
    "                shuffle=False,\n",
    "                batch_size=self.batch_size,\n",
    "                collate_fn=collate_fn,\n",
    "            )\n",
    "            for batch in val_loader:\n",
    "                total_steps += 1\n",
    "                X, y = model.parse_batch(batch)\n",
    "                y_pred = model(X)\n",
    "                loss = criterion(y_pred, y)\n",
    "                total_loss += loss.item()\n",
    "            mean_loss = total_loss / total_steps\n",
    "            print(f\"Average loss: {mean_loss}\")\n",
    "\n",
    "    @property\n",
    "    def training_dataset_args(self):\n",
    "        return [\n",
    "            self.dataset_path,\n",
    "            self.training_audiopaths_and_text,\n",
    "            self.text_cleaners,\n",
    "            # audio params\n",
    "            self.n_mel_channels,\n",
    "            self.sample_rate,\n",
    "            self.mel_fmin,\n",
    "            self.mel_fmax,\n",
    "            self.filter_length,\n",
    "            self.hop_length,\n",
    "            self.win_length,\n",
    "            self.max_wav_value,\n",
    "            self.include_f0,\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def val_dataset_args(self):\n",
    "        val_args = [a for a in self.training_dataset_args]\n",
    "        val_args[1] = self.val_audiopaths_and_text\n",
    "        return val_args\n",
    "\n",
    "    def train(self):\n",
    "        train_set = TextMelDataset(\n",
    "            *self.training_dataset_args,\n",
    "            debug=self.debug,\n",
    "            debug_dataset_size=self.batch_size,\n",
    "        )\n",
    "        val_set = TextMelDataset(\n",
    "            *self.val_dataset_args, debug=self.debug, debug_dataset_size=self.batch_size\n",
    "        )\n",
    "        collate_fn = TextMelCollate(n_frames_per_step=1, include_f0=self.include_f0)\n",
    "        train_loader = DataLoader(\n",
    "            train_set, batch_size=self.batch_size, shuffle=True, collate_fn=collate_fn\n",
    "        )\n",
    "        criterion = Tacotron2Loss()\n",
    "        \n",
    "        model = Tacotron2(self.hparams)\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=self.learning_rate,\n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "        start_epoch = 0\n",
    "        if self.checkpoint_name:\n",
    "            checkpoint = self.load_checkpoint(self.checkpoint_name)\n",
    "            model.load_state_dict(checkpoint[\"model\"])\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "            start_epoch = checkpoint[\"iteration\"]\n",
    "        # main training loop\n",
    "        for epoch in range(start_epoch, self.epochs):\n",
    "            for batch in train_loader:\n",
    "                model.zero_grad()\n",
    "                X, y = model.parse_batch(batch)\n",
    "                y_pred = model(X)\n",
    "                loss = criterion(y_pred, y)\n",
    "                # TODO: fix for distributed run\n",
    "                if self.distributed_run:\n",
    "                    raise NotImplemented\n",
    "                else:\n",
    "                    reduced_loss = loss.item()\n",
    "                loss.backward()\n",
    "                if self.fp16_run:\n",
    "                    raise NotImplemented\n",
    "                else:\n",
    "                    grad_norm = torch.nn.utils.clip_grad_norm(\n",
    "                        model.parameters(), self.grad_clip_thresh\n",
    "                    )\n",
    "                optimizer.step()\n",
    "                print(f\"Loss: {reduced_loss}\")\n",
    "            if epoch % self.epochs_per_checkpoint == 0:\n",
    "                self.save_checkpoint(\n",
    "                    f\"mellotron_{epoch}\",\n",
    "                    model=model,\n",
    "                    optimizer=optimizer,\n",
    "                    iteration=epoch,\n",
    "                    learning_rate=self.learning_rate,\n",
    "                )\n",
    "\n",
    "            # There's no need to validate in debug mode since we're not really training.\n",
    "            if self.debug:\n",
    "                continue\n",
    "            self.validate(\n",
    "                model=model,\n",
    "                val_set=val_set,\n",
    "                collate_fn=collate_fn,\n",
    "                criterion=criterion,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617ac91b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef86d55e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
