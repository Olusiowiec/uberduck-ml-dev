# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/trainer.base.ipynb (unless otherwise specified).

__all__ = ['TTSTrainer', 'Tacotron2Loss', 'MellotronTrainer']

# Cell

class TTSTrainer():

    def __init__(self, hparams):
        self.hparams = hparams
        for k, v in hparams.values().items():
            setattr(self, k, v)


    def train():
        raise NotImplemented
        #for batch in enumerate(data):
        #    #fill in


# Cell
from typing import List
import torch
from torch import nn
from torch.utils.data import DataLoader
from ..data_loader import TextMelDataset, TextMelCollate
from ..models.mellotron import Tacotron2


class Tacotron2Loss(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, model_output: List, target: List):
        mel_target, gate_target = target[0], target[1]
        mel_target.requires_grad = False
        gate_target.requires_grad = False
        gate_target.view(-1, 1)
        mel_out, mel_out_postnet, gate_out, _ = model_ouput
        gate_out = gate_out.view(-1, 1)
        mel_loss = nn.MSELoss()(mel_out, mel_target) + nn.MSELoss()(
            mel_out_postnet, mel_target
        )
        gate_loss = nn.BCEWithLogitsLoss()(gate_out, gate_target)
        return mel_loss + gate_loss


class MellotronTrainer(TTSTrainer):
    REQUIRED_HPARAMS = [
        "audiopaths_and_text",
        "dataset_path",
        "epochs",
        "mel_fmax",
        "mel_fmin",
        "n_mel_channels",
        "text_cleaners",
    ]

    def validate(self, **kwargs):
        model = kwargs["model"]
        val_set = kwargs["val_set"]
        collate_fn = kwargs["collate_fn"]
        criterion = kwargs["criterion"]
        if self.distributed_run:
            raise NotImplemented
        with torch.no_grad():
            val_loader = DataLoader(
                valset,
                shuffle=False,
                batch_size=self.batch_size,
                collate_fn=collate_fn,
            )
            for batch in val_loader:
                X, y = model.parse_batch(batch)
                y_pred = model(y)
                loss = criterion(y_pred, y)

    @property
    def training_dataset_args(self):
        return [
            self.dataset_path,
            self.training_audiopaths_and_text,
            self.text_cleaners,
            # audio params
            self.n_mel_channels,
            self.sample_rate,
            self.mel_fmin,
            self.mel_fmax,
            self.filter_length,
            self.hop_length,
            self.win_length,
            self.max_wav_value,
            self.include_f0,
        ]

    @property
    def val_dataset_args(self):
        val_args = [a for a in self.training_dataset_args]
        val_args[1] = self.val_audiopaths_and_text
        return val_args


    def train(self):
        # load dataset
        train_set = TextMelDataset(*self.training_dataset_args)
        val_set = TextMelDataset(*val_dataset_args)
        collate_fn = TextMelCollate(n_frames_per_step=1, include_f0=self.include_f0)
        train_loader = DataLoader(
            dataset, batch_size=self.batch_size, shuffle=True, collate_fn=collate_fn
        )
        # create loss criterion
        criterion = Tacotron2Loss()
        # create model
        model = Tacotron2(self.hparams)
        # load optimizer
        optimizer = torch.optim.Adam(
            model.parameters(),
            lr=self.learning_rate,
            weight_decay=self.weight_decay,
        )
        # main training loop
        start_epoch = 0
        for epoch in range(start_epoch, self.epochs):
            for batch in train_loader:
                model.zero_grad()
                X, y = model.parse_batch(batch)
                y_pred = model(X)
                loss = criterion(y_pred, y)
                # TODO: fix for distributed run
                if self.distributed_run:
                    raise NotImplemented
                else:
                    reduced_loss = loss.item()
                loss.backward()
                if self.fp16_run:
                    raise NotImplemented
                else:
                    grad_norm = torch.nn.utils.clip_grad_norm(
                        model.parameters(), self.grad_clip_threshold
                    )
                optimizer.step()
                print(f"Loss: {reduced_loss}")
            self.validate(
                model=model,
                val_set=val_set,
            )