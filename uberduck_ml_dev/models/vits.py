# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/models.vits.ipynb (unless otherwise specified).

__all__ = ['VITSEncoder', 'VITSModel']

# Cell
from torch import nn

from .base import TTSModel


class VITSEncoder(nn.Module):
    def __init__(
        self,
        n_vocab,
        out_channels,
        hidden_channels,
        filter_channels,
        n_heads,
        n_layers,
        kernel_size,
        p_dropout,
    ):
        super().__init__()
        self.n_vocab = n_vocab
        self.out_channels = out_channels
        self.hidden_channels = hidden_channels
        self.filter_channels = filter_channels
        self.n_heads = n_heads
        self.n_layers = n_layers
        self.kernel_size = kernel_size
        self.p_dropout = p_dropout

        self.embedding = nn.Embedding(n_vocab, hidden_channels)
        nn.init.normal_(self.emb.weight, 0.0, hidden_channels**-0.5)

        self.encoder

class VITSModel(TTSModel):
    def __init__(
        self,
        n_vocab,
        n_spectrogram_channels,
        segment_size,
        inter_channels,
        hidden_channels,
        filter_channels,
        n_heads,
        n_layers,
        kernel_size,
        p_dropout,
        resblock,
        resblock_kernel_sizes,
        resblock_dilation_sizes,
        upsample_rates,
        upsample_initial_channel,
        upsample_kernel_sizes,
        n_speakers=0,
        gin_channels=0,
        use_sdp=True,
    ):
        super().__init__()

        if n_speakers > 1:
            self.emb_g = nn.Embedding(n_speakers, gin_channels)