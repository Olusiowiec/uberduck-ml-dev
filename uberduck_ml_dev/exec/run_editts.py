# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/exec.run_editts.ipynb (unless otherwise specified).

__all__ = ['parse_args', 'run']

# Cell
import argparse
import sys
import librosa  # NOTE(zach): importing torch before librosa causes LLVM issues for some unknown reason.
import torch
import csv
import re
import json

import matplotlib.pyplot as plt
import soundfile as sf
from scipy.io.wavfile import write, read
import numpy as np

%matplotlib inline

from ..vendor.tfcompat.hparam import HParams
from ..models.common import MelSTFT
from ..models.gradtts import GradTTS, DEFAULTS as GRADTTS_DEFAULTS
from ..vocoders.hifigan import HiFiGanGenerator
from ..utils.plot import plot_spectrogram
from ..utils.audio import (
    overlay_stereo,
    stereo_to_mono,
    mono_to_stereo,
    to_int16,
    resample,
)


def parse_args(args):
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", help="Path to JSON config")
    args = parser.parse_args(args)
    return args

# Cell
def run(hparams):
    # Create model
    model = GradTTS(hparams)
    model.load_state_dict(torch.load(hparams.checkpoint))
    model = model.cuda()

    stft = MelSTFT(
        filter_length=hparams.filter_length,
        hop_length=hparams.hop_length,
        win_length=hparams.win_length,
        n_mel_channels=hparams.n_feats,
        sampling_rate=hparams.sampling_rate,
        mel_fmin=hparams.mel_fmin,
        mel_fmax=hparams.mel_fmax,
        padding=(hparams.filter_length - hparams.hop_length) // 2,
    )

    hifigan = HiFiGanGenerator(
        config=hparams.hifigan_config,
        checkpoint=hparams.hifigan_checkpoint,
        cudnn_enabled=True,
    )

    # Convert vocals to 22kHz stereo audio for EdiTTS
    vocal_data, vocal_sample_rate = librosa.load(
        hparams.reference_vocals, sr=22050, mono=True
    )

    # We want to read in the beats as original 44kHz stereo audio
    beats_data, beats_sample_rate = librosa.load(
        hparams.reference_beats, sr=44100, mono=False
    )
    vocal_data = vocal_data[: 5 * 22050]
    beats_data = beats_data[:, : 5 * 44100]

    vocal_data = to_int16(vocal_data)

    audio_norm = torch.FloatTensor(vocal_data) / hparams.max_wav_value
    audio_norm = audio_norm.unsqueeze(0)
    melspec_original = stft.mel_spectrogram(audio_norm).cuda()

    with open(hparams.customizations, "r") as csvfile:
        datareader = csv.reader(csvfile)
        for i, row in enumerate(datareader):
            regex = r"\|(.*?)\|"
            substitution = f"| {row[0]} |"
            new_transcription = re.sub(regex, substitution, hparams.transcription)
            print(new_transcription)
            y_dec1, y_dec2, y_dec_edit, y_dec_cat = model.infer_editts_edit_content(
                hparams.transcription,
                new_transcription,
                n_timesteps=10,
                symbol_set="gradtts",
                mel1=melspec_original.cuda(),
                i1=int(
                    CUSTOMIZATIONS["first_name"]["start_time"]
                    * hparams.sampling_rate
                    / hparams.hop_length
                ),
                j1=int(
                    CUSTOMIZATIONS["first_name"]["end_time"]
                    * hparams.sampling_rate
                    / hparams.hop_length
                ),
                desired_time=CUSTOMIZATIONS["first_name"]["end_time"]
                - CUSTOMIZATIONS["first_name"]["start_time"],
            )

            personalized_vocals = hifigan.infer(y_dec_edit)

            #   VOLUME MASK AND MULTIPLIER
            volume_multiplier = np.ones(personalized_vocals.shape[-1], dtype=np.int16)
            volume_boundary_0 = int(
                CUSTOMIZATIONS["first_name"]["start_time"] * hparams.sampling_rate
            )
            volume_boundary_1 = int(
                CUSTOMIZATIONS["first_name"]["end_time"] * hparams.sampling_rate
            )
            #             max_volume = personalized_vocals[volume_boundary_0:volume_boundary_1].max()
            #             print(audio_norm.squeeze().shape)
            #             print(personalized_vocals.shape)

            target_dbfs = _dbfs(personalized_vocals[:volume_boundary_0])
            current_dbfs = _dbfs(
                personalized_vocals[volume_boundary_0:volume_boundary_1]
            )
            coeff = _db_to_float(target_dbfs - current_dbfs)
            print(f"coeff {coeff}")
            #             personalized_vocals = personalized_vocals * coeff
            # #             print(f"max volume: {max_volume}")
            #             print(f"rms_volume_synthetic: {rms_volume_synthetic}")
            #             print(f"rms_volume_target: {rms_volume_target}")

            # #             volume_scale = hparams.max_wav_value / max_volume
            #             volume_scale = rms_volume_target / rms_volume_synthetic
            #             print(f"volume_scale: {volume_scale}")
            volume_multiplier[volume_boundary_0:volume_boundary_1] = coeff

            personalized_vocals = personalized_vocals * volume_multiplier
            #             print(f"after: {personalized_vocals.shape}")

            personalized_vocals = personalized_vocals / hparams.max_wav_value
            #             personalized_vocals = resample(
            #                 personalized_vocals, hparams.sampling_rate, 44100
            #             )
            #             personalized_vocals = mono_to_stereo(personalized_vocals)

            #             final_audio = overlay_stereo(personalized_vocals, beats_data)
            #             sf.write(f"{hparams.log_dir}/edited_{i}.wav", final_audio.T, 44100)
            sf.write(f"{hparams.log_dir}/edited_{i}.wav", personalized_vocals, 22050)

        plt.show()

# Cell
try:
    from nbdev.imports import IN_NOTEBOOK
except:
    IN_NOTEBOOK = False
if __name__ == "__main__" and not IN_NOTEBOOK:
    args = parse_args(sys.argv[1:])
    config = GRADTTS_DEFAULTS.values()
    if args.config:
        with open(args.config) as f:
            config.update(json.load(f))
    hparams = HParams(**config)
    run(hparams)